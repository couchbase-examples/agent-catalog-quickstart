{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDGGbfZlmTCq",
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Flight Search Agent Tutorial\n",
        "\n",
        "This notebook demonstrates the Agent Catalog flight search agent using LangGraph with Couchbase vector store and Arize evaluation. Uses AI services with standard OpenAI wrappers and Capella (simple & fast).\n",
        "\n",
        "The agent provides comprehensive flight search capabilities including:\n",
        "- Flight lookup and search\n",
        "- Flight booking management\n",
        "- Airline review search\n",
        "- Booking retrieval and management\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XmfZF6U8gH12",
        "outputId": "cd725edc-d22b-4f2c-84cb-bd62d3ce6351"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "print(os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWQCmIIngH12",
        "outputId": "5003cfd3-f69b-46ed-c74f-df090cb18010"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-10-23 15:57:59--  https://raw.githubusercontent.com/couchbase-examples/agent-catalog-quickstart/refs/heads/main/notebooks/flight_search_agent_langraph/prompts/flight_search_assistant.yaml\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9279 (9.1K) [text/plain]\n",
            "Saving to: \u2018prompts/flight_search_assistant.yaml\u2019\n",
            "\n",
            "\r          prompts/f   0%[                    ]       0  --.-KB/s               \rprompts/flight_sear 100%[===================>]   9.06K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2025-10-23 15:57:59 (15.6 MB/s) - \u2018prompts/flight_search_assistant.yaml\u2019 saved [9279/9279]\n",
            "\n",
            "--2025-10-23 15:58:00--  https://raw.githubusercontent.com/couchbase-examples/agent-catalog-quickstart/refs/heads/main/notebooks/flight_search_agent_langraph/tools/lookup_flight_info.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3302 (3.2K) [text/plain]\n",
            "Saving to: \u2018tools/lookup_flight_info.py\u2019\n",
            "\n",
            "tools/lookup_flight 100%[===================>]   3.22K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-10-23 15:58:00 (47.1 MB/s) - \u2018tools/lookup_flight_info.py\u2019 saved [3302/3302]\n",
            "\n",
            "--2025-10-23 15:58:00--  https://raw.githubusercontent.com/couchbase-examples/agent-catalog-quickstart/refs/heads/main/notebooks/flight_search_agent_langraph/tools/retrieve_flight_bookings.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6550 (6.4K) [text/plain]\n",
            "Saving to: \u2018tools/retrieve_flight_bookings.py\u2019\n",
            "\n",
            "tools/retrieve_flig 100%[===================>]   6.40K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-10-23 15:58:00 (66.0 MB/s) - \u2018tools/retrieve_flight_bookings.py\u2019 saved [6550/6550]\n",
            "\n",
            "--2025-10-23 15:58:00--  https://raw.githubusercontent.com/couchbase-examples/agent-catalog-quickstart/refs/heads/main/notebooks/flight_search_agent_langraph/tools/save_flight_booking.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9381 (9.2K) [text/plain]\n",
            "Saving to: \u2018tools/save_flight_booking.py\u2019\n",
            "\n",
            "tools/save_flight_b 100%[===================>]   9.16K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2025-10-23 15:58:00 (14.8 MB/s) - \u2018tools/save_flight_booking.py\u2019 saved [9381/9381]\n",
            "\n",
            "--2025-10-23 15:58:00--  https://raw.githubusercontent.com/couchbase-examples/agent-catalog-quickstart/refs/heads/main/notebooks/flight_search_agent_langraph/tools/search_airline_reviews.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5240 (5.1K) [text/plain]\n",
            "Saving to: \u2018tools/search_airline_reviews.py\u2019\n",
            "\n",
            "tools/search_airlin 100%[===================>]   5.12K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-10-23 15:58:00 (54.0 MB/s) - \u2018tools/search_airline_reviews.py\u2019 saved [5240/5240]\n",
            "\n",
            "--2025-10-23 15:58:00--  https://raw.githubusercontent.com/couchbase-examples/agent-catalog-quickstart/refs/heads/main/notebooks/flight_search_agent_langraph/tools/schemas.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3615 (3.5K) [text/plain]\n",
            "Saving to: \u2018tools/schemas.py\u2019\n",
            "\n",
            "tools/schemas.py    100%[===================>]   3.53K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-10-23 15:58:01 (41.3 MB/s) - \u2018tools/schemas.py\u2019 saved [3615/3615]\n",
            "\n",
            "--2025-10-23 15:58:01--  https://raw.githubusercontent.com/couchbase-examples/agent-catalog-quickstart/refs/heads/main/notebooks/flight_search_agent_langraph/agentcatalog_index.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1956 (1.9K) [text/plain]\n",
            "Saving to: \u2018agentcatalog_index.json\u2019\n",
            "\n",
            "agentcatalog_index. 100%[===================>]   1.91K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-10-23 15:58:01 (24.7 MB/s) - \u2018agentcatalog_index.json\u2019 saved [1956/1956]\n",
            "\n",
            "--2025-10-23 15:58:01--  https://raw.githubusercontent.com/couchbase-examples/agent-catalog-quickstart/refs/heads/main/notebooks/flight_search_agent_langraph/.agentcignore\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 27 [text/plain]\n",
            "Saving to: \u2018.agentcignore\u2019\n",
            "\n",
            ".agentcignore       100%[===================>]      27  --.-KB/s    in 0s      \n",
            "\n",
            "2025-10-23 15:58:01 (778 KB/s) - \u2018.agentcignore\u2019 saved [27/27]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Download required resources for the flight search agent\n",
        "!mkdir -p prompts\n",
        "!wget -O prompts/flight_search_assistant.yaml https://raw.githubusercontent.com/couchbase-examples/agent-catalog-quickstart/refs/heads/main/notebooks/flight_search_agent_langraph/prompts/flight_search_assistant.yaml\n",
        "!mkdir -p tools\n",
        "!wget -O tools/lookup_flight_info.py https://raw.githubusercontent.com/couchbase-examples/agent-catalog-quickstart/refs/heads/main/notebooks/flight_search_agent_langraph/tools/lookup_flight_info.py\n",
        "!wget -O tools/retrieve_flight_bookings.py https://raw.githubusercontent.com/couchbase-examples/agent-catalog-quickstart/refs/heads/main/notebooks/flight_search_agent_langraph/tools/retrieve_flight_bookings.py\n",
        "!wget -O tools/save_flight_booking.py https://raw.githubusercontent.com/couchbase-examples/agent-catalog-quickstart/refs/heads/main/notebooks/flight_search_agent_langraph/tools/save_flight_booking.py\n",
        "!wget -O tools/search_airline_reviews.py https://raw.githubusercontent.com/couchbase-examples/agent-catalog-quickstart/refs/heads/main/notebooks/flight_search_agent_langraph/tools/search_airline_reviews.py\n",
        "!wget -O tools/schemas.py https://raw.githubusercontent.com/couchbase-examples/agent-catalog-quickstart/refs/heads/main/notebooks/flight_search_agent_langraph/tools/schemas.py\n",
        "!wget -O agentcatalog_index.json https://raw.githubusercontent.com/couchbase-examples/agent-catalog-quickstart/refs/heads/main/notebooks/flight_search_agent_langraph/agentcatalog_index.json\n",
        "!wget -O .agentcignore https://raw.githubusercontent.com/couchbase-examples/agent-catalog-quickstart/refs/heads/main/notebooks/flight_search_agent_langraph/.agentcignore\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ld1O6CqSgH13",
        "outputId": "d92bd600-89b7-4353-d917-b46fd7bdf2e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/60.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/76.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m76.0/76.0 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/44.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m143.9/143.9 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m239.6/239.6 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m301.2/301.2 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m134.7/134.7 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m96.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m50.3/50.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m113.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m88.0/88.0 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m132.3/132.3 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m65.9/65.9 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m208.0/208.0 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m331.1/331.1 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m203.4/203.4 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m207.6/207.6 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-adk 1.16.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.38.0 which is incompatible.\n",
            "google-adk 1.16.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\n",
            "google-adk 1.16.0 requires uvicorn<1.0.0,>=0.34.0, but you have uvicorn 0.29.0 which is incompatible.\n",
            "mcp 1.18.0 requires uvicorn>=0.31.1; sys_platform != \"emscripten\", but you have uvicorn 0.29.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "%pip install -q \\\n",
        "    \"pydantic>=2.0.0,<3.0.0\" \\\n",
        "    \"pydantic-settings>=2.10.1,<3.0.0\" \\\n",
        "    \"python-dotenv>=1.0.0,<2.0.0\" \\\n",
        "    \"pandas>=2.0.0,<3.0.0\" \\\n",
        "    \"nest-asyncio>=1.6.0,<2.0.0\" \\\n",
        "    \"uvicorn>=0.29.0,<0.30.0\" \\\n",
        "    \"kagglehub>=0.2.0,<1.0.0\" \\\n",
        "    \"langchain-couchbase>=0.4.0,<0.5.0\" \\\n",
        "    \"langchain-openai>=0.3.11,<0.4.0\" \\\n",
        "    \"langchain-nvidia-ai-endpoints>=0.3.13,<0.4.0\" \\\n",
        "    \"langgraph>=0.5.1,<0.6.0\" \\\n",
        "    \"arize>=7.51.0,<8.0.0\" \\\n",
        "    \"arize-phoenix>=11.37.0,<12.0.0\" \\\n",
        "    \"arize-phoenix-evals>=2.2.0,<3.0.0\" \\\n",
        "    \"openinference-instrumentation-langchain>=0.1.29,<0.2.0\" \\\n",
        "    \"openinference-instrumentation-openai>=0.1.18,<0.2.0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPxLBXu4gH13",
        "outputId": "97e24c9c-8666-4d00-df50-0825b876d967"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/98.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m98.5/98.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m94.7/94.7 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m275.9/275.9 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for gitignore-parser (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m75.5/75.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m315.5/315.5 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m194.2/194.2 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m98.2/98.2 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m54.6/54.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-adk 1.16.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.38.0 which is incompatible.\n",
            "google-adk 1.16.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\n",
            "google-adk 1.16.0 requires uvicorn<1.0.0,>=0.34.0, but you have uvicorn 0.29.0 which is incompatible.\n",
            "mcp 1.18.0 requires uvicorn>=0.31.1; sys_platform != \"emscripten\", but you have uvicorn 0.29.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "%pip install -q https://github.com/couchbaselabs/agent-catalog/releases/download/v0.2.5a3/agentc_core-0.2.5a3-py3-none-any.whl\n",
        "%pip install -q https://github.com/couchbaselabs/agent-catalog/releases/download/v0.2.5a3/agentc_cli-0.2.5a3-py3-none-any.whl\n",
        "%pip install -q https://github.com/couchbaselabs/agent-catalog/releases/download/v0.2.5a3/agentc-0.2.5a3-py3-none-any.whl\n",
        "%pip install -q https://github.com/couchbaselabs/agent-catalog/releases/download/v0.2.5a3/agentc_langchain-0.2.5a3-py3-none-any.whl # Explicitly install agentc_langchain a3\n",
        "%pip install -q https://github.com/couchbaselabs/agent-catalog/releases/download/v0.2.5a3/agentc_langgraph-0.2.5a3-py3-none-any.whl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "94jTF0e6gH13"
      },
      "outputs": [],
      "source": [
        "# Install the couchbase-infrastructure package\n",
        "%pip install -q couchbase-infrastructure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QaglXlSgH13"
      },
      "source": [
        "## \ud83d\ude80 Educational Infrastructure Setup\n",
        "\n",
        "**This cell sets up your Couchbase Capella infrastructure step-by-step using the `couchbase-infrastructure` package.**\n",
        "\n",
        "### What It Does:\n",
        "1. **Prompts for Credentials** - Securely collects your API key (no .env file needed for Colab!)\n",
        "2. **Creates Capella Project & Cluster** - Sets up your cloud database infrastructure\n",
        "3. **Loads travel-sample Data** - Imports the sample dataset for the tutorial\n",
        "4. **Deploys AI Models** - Provisions embedding (Mistral 7B) and LLM (Llama 3 8B) models\n",
        "5. **Configures Network Access** - Sets up CIDR allowlists for connectivity\n",
        "6. **Creates Database User** - Generates credentials with appropriate permissions\n",
        "7. **Sets Environment Variables** - Configures all required variables for the tutorial\n",
        "\n",
        "### You'll Be Prompted For:\n",
        "- `MANAGEMENT_API_KEY` (required) - Get from [Capella Console](https://cloud.couchbase.com) \u2192 Settings \u2192 API Keys\n",
        "- Optional: `ORGANIZATION_ID`, `PROJECT_NAME`, `CLUSTER_NAME` (defaults provided)\n",
        "\n",
        "### Process Time:\n",
        "\u23f3 This will take **10-15 minutes** for cluster and AI model deployment. You'll see step-by-step progress!\n",
        "\n",
        "### After Running:\n",
        "All subsequent cells will automatically use the provisioned infrastructure. No manual configuration needed!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Yhwo9axFgH13",
        "outputId": "2ebed463-1233-40ca-d6e4-6dacddca3356"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "\ud83d\ude80 Couchbase Capella Infrastructure Setup\n",
            "======================================================================\n",
            "\n",
            "This educational setup shows you how to provision Capella infrastructure\n",
            "step-by-step using the couchbase-infrastructure package.\n",
            "\n",
            "\n",
            "\ud83d\udccb Step 1: Collecting Credentials\n",
            "----------------------------------------------------------------------\n",
            "\u2705 Found .env file. Loading configuration...\n",
            "\n",
            "Get your credentials from: https://cloud.couchbase.com \u2192 Settings \u2192 API Keys\n",
            "\n",
            "\u2705 Using MANAGEMENT_API_KEY from environment\n",
            "\u2705 Using ORGANIZATION_ID from environment: 23086345-371f-4650-8dc4-c61733dd27a0\n",
            "Enter PROJECT_NAME (default: 'agent-app'): \n",
            "Enter CLUSTER_NAME (default: 'agent-app-cluster'): \n",
            "Enter DB_USERNAME (default: 'agent_app_user'): \n",
            "Enter BUCKET_NAME (default: 'travel-sample'): \n",
            "Enter EMBEDDING_MODEL (default: 'nvidia/llama-3.2-nv-embedqa-1b-v2'): \n",
            "Enter LLM_MODEL (default: 'meta/llama3-8b-instruct'): \n",
            "\n",
            "\u2705 Configuration collected successfully!\n",
            "\n",
            "\n",
            "\ud83d\udd27 Step 2: Initializing Configuration\n",
            "----------------------------------------------------------------------\n",
            "\u2705 Configuration initialized\n",
            "\n",
            "\n",
            "\ud83d\udd0c Step 3: Initializing Client\n",
            "----------------------------------------------------------------------\n",
            "\u2705 Using Organization ID: 23086345-371f-4650-8dc4-c61733dd27a0\n",
            "\n",
            "\n",
            "\ud83d\udd0d Step 4: Testing API Connection\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\ud83d\udd0d Testing API connection...</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;34m\ud83d\udd0d Testing API connection\u001b[0m\u001b[1;34m...\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   Current IP: <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">35.188.74.140</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "   Current IP: \u001b[1;33m35.188.74.140\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   API Base URL: <span style=\"color: #808000; text-decoration-color: #808000; text-decoration: underline\">https://cloudapi.sbx-29.sandbox.nonprod-project-avengers.com</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "   API Base URL: \u001b[4;33mhttps://cloudapi.sbx-29.sandbox.nonprod-project-avengers.com\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   Organization ID: <span style=\"color: #808000; text-decoration-color: #808000\">23086345-371f-4650-8dc4-c61733dd27a0</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "   Organization ID: \u001b[33m23086345-371f-4650-8dc4-c61733dd27a0\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   API Response Status: <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">200</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "   API Response Status: \u001b[1;33m200\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   <span style=\"color: #008000; text-decoration-color: #008000\">\u2705 Authentication successful</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "   \u001b[32m\u2705 Authentication successful\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u2705 API connection successful\n",
            "\n",
            "\n",
            "\ud83d\udcc1 Step 5: Creating Capella Project\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   Searching for project named <span style=\"color: #008000; text-decoration-color: #008000\">'</span><span style=\"color: #008080; text-decoration-color: #008080\">agent-app</span><span style=\"color: #008000; text-decoration-color: #008000\">'</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "   Searching for project named \u001b[32m'\u001b[0m\u001b[36magent-app\u001b[0m\u001b[32m'\u001b[0m\u001b[33m...\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">\u2705 Found existing project.</span> Project ID: <span style=\"color: #008080; text-decoration-color: #008080\">e04136ef-4809-44fc-b703-0824e01655a4</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[32m\u2705 Found existing project.\u001b[0m Project ID: \u001b[36me04136ef-4809-44fc-b703-0824e01655a4\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u2705 Project ready: agent-app (ID: e04136ef-4809-44fc-b703-0824e01655a4)\n",
            "\n",
            "\n",
            "\u2601\ufe0f Step 6: Creating Developer Pro Cluster with Analytics\n",
            "----------------------------------------------------------------------\n",
            "\u23f3 This will take 10-15 minutes for cluster deployment...\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   Checking if cluster <span style=\"color: #008000; text-decoration-color: #008000\">'</span><span style=\"color: #008080; text-decoration-color: #008080\">agent-app-cluster</span><span style=\"color: #008000; text-decoration-color: #008000\">'</span> already exists<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "   Checking if cluster \u001b[32m'\u001b[0m\u001b[36magent-app-cluster\u001b[0m\u001b[32m'\u001b[0m already exists\u001b[33m...\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   <span style=\"color: #008000; text-decoration-color: #008000\">\u2705 Cluster </span><span style=\"color: #008000; text-decoration-color: #008000\">'agent-app-cluster'</span><span style=\"color: #008000; text-decoration-color: #008000\"> already exists </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">Status: unknown</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">)</span><span style=\"color: #008000; text-decoration-color: #008000\">.</span> Cluster ID: \n",
              "<span style=\"color: #008080; text-decoration-color: #008080\">b82a6f7f-a9b3-470d-824a-c321d30ad5f4</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "   \u001b[32m\u2705 Cluster \u001b[0m\u001b[32m'agent-app-cluster'\u001b[0m\u001b[32m already exists \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mStatus: unknown\u001b[0m\u001b[1;32m)\u001b[0m\u001b[32m.\u001b[0m Cluster ID: \n",
              "\u001b[36mb82a6f7f-a9b3-470d-824a-c321d30ad5f4\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   Waiting for Cluster to become ready<span style=\"color: #808000; text-decoration-color: #808000\">...</span> <span style=\"font-weight: bold\">(</span>no timeout, will wait indefinitely<span style=\"font-weight: bold\">)</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "   Waiting for Cluster to become ready\u001b[33m...\u001b[0m \u001b[1m(\u001b[0mno timeout, will wait indefinitely\u001b[1m)\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   Current status: <span style=\"color: #808000; text-decoration-color: #808000\">healthy</span> <span style=\"font-weight: bold\">(</span>elapsed: 0s<span style=\"font-weight: bold\">)</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "   Current status: \u001b[33mhealthy\u001b[0m \u001b[1m(\u001b[0melapsed: 0s\u001b[1m)\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">\u2705 Cluster is ready!</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[32m\u2705 Cluster is ready!\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u2705 Cluster ready: agent-app-cluster (ID: b82a6f7f-a9b3-470d-824a-c321d30ad5f4)\n",
            "\n",
            "\n",
            "\ud83c\udf10 Step 7: Configuring Network Access\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   Adding allowed CIDR <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0.0.0</span><span style=\"color: #008080; text-decoration-color: #008080\">/</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> to cluster<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "   Adding allowed CIDR \u001b[1;36m0.0.0.0\u001b[0m\u001b[36m/\u001b[0m\u001b[1;36m0\u001b[0m to cluster\u001b[33m...\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   Checking if CIDR already exists<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "   Checking if CIDR already exists\u001b[33m...\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   <span style=\"color: #008000; text-decoration-color: #008000\">\u2705 CIDR </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.0.0.0</span><span style=\"color: #008000; text-decoration-color: #008000\">/</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0</span><span style=\"color: #008000; text-decoration-color: #008000\"> already exists</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "   \u001b[32m\u2705 CIDR \u001b[0m\u001b[1;32m0.0.0.0\u001b[0m\u001b[32m/\u001b[0m\u001b[1;32m0\u001b[0m\u001b[32m already exists\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u2705 Network access configured (0.0.0.0/0 allowed)\n",
            "\n",
            "\n",
            "\ud83d\udce6 Step 8: Loading travel-sample Bucket\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">\u2705 `travel-sample` bucket load command accepted.</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[32m\u2705 `travel-sample` bucket load command accepted.\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">\u2705 `travel-sample` bucket is ready.</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[32m\u2705 `travel-sample` bucket is ready.\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u2705 Sample data loaded: travel-sample\n",
            "\n",
            "\n",
            "\ud83d\udc64 Step 9: Creating Database User\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   Database user <span style=\"color: #008000; text-decoration-color: #008000\">'</span><span style=\"color: #008080; text-decoration-color: #008080\">agent_app_user</span><span style=\"color: #008000; text-decoration-color: #008000\">'</span> already exists. Deleting to recreate with new password<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "   Database user \u001b[32m'\u001b[0m\u001b[36magent_app_user\u001b[0m\u001b[32m'\u001b[0m already exists. Deleting to recreate with new password\u001b[33m...\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   User <span style=\"color: #008000; text-decoration-color: #008000\">'</span><span style=\"color: #008080; text-decoration-color: #008080\">agent_app_user</span><span style=\"color: #008000; text-decoration-color: #008000\">'</span> deleted successfully.\n",
              "</pre>\n"
            ],
            "text/plain": [
              "   User \u001b[32m'\u001b[0m\u001b[36magent_app_user\u001b[0m\u001b[32m'\u001b[0m deleted successfully.\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   Database user <span style=\"color: #008000; text-decoration-color: #008000\">'</span><span style=\"color: #008080; text-decoration-color: #008080\">agent_app_user</span><span style=\"color: #008000; text-decoration-color: #008000\">'</span> created successfully.\n",
              "</pre>\n"
            ],
            "text/plain": [
              "   Database user \u001b[32m'\u001b[0m\u001b[36magent_app_user\u001b[0m\u001b[32m'\u001b[0m created successfully.\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u2705 Database user created: agent_app_user\n",
            "\n",
            "   Auto-generated password: x!8y...Ij-t\n",
            "\n",
            "\n",
            "\ud83e\udd16 Step 10: Deploying AI Models\n",
            "----------------------------------------------------------------------\n",
            "\u23f3 Deploying embedding and LLM models (5-10 minutes)...\n",
            "\n",
            "   Deploying embedding model...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   Checking if model <span style=\"color: #008000; text-decoration-color: #008000\">'</span><span style=\"color: #008080; text-decoration-color: #008080\">agent-hub-embedding-model</span><span style=\"color: #008000; text-decoration-color: #008000\">'</span> already exists<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "   Checking if model \u001b[32m'\u001b[0m\u001b[36magent-hub-embedding-model\u001b[0m\u001b[32m'\u001b[0m already exists\u001b[33m...\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   Found <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> existing <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">model</span><span style=\"font-weight: bold\">(</span>s<span style=\"font-weight: bold\">)</span>.\n",
              "</pre>\n"
            ],
            "text/plain": [
              "   Found \u001b[1;36m2\u001b[0m existing \u001b[1;35mmodel\u001b[0m\u001b[1m(\u001b[0ms\u001b[1m)\u001b[0m.\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   <span style=\"color: #008000; text-decoration-color: #008000\">\u2705 Model </span><span style=\"color: #008000; text-decoration-color: #008000\">'agent-hub-embedding-model'</span><span style=\"color: #008000; text-decoration-color: #008000\"> already exists </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">Status: healthy</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">)</span><span style=\"color: #008000; text-decoration-color: #008000\">.</span> Model ID: \n",
              "<span style=\"color: #008080; text-decoration-color: #008080\">22ffa6d6-87a3-45fd-905d-efb0c995ffa1</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "   \u001b[32m\u2705 Model \u001b[0m\u001b[32m'agent-hub-embedding-model'\u001b[0m\u001b[32m already exists \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mStatus: healthy\u001b[0m\u001b[1;32m)\u001b[0m\u001b[32m.\u001b[0m Model ID: \n",
              "\u001b[36m22ffa6d6-87a3-45fd-905d-efb0c995ffa1\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   \u2705 Model config matches. Reusing existing model.\n",
              "</pre>\n"
            ],
            "text/plain": [
              "   \u2705 Model config matches. Reusing existing model.\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   Waiting for Embedding Model to become ready<span style=\"color: #808000; text-decoration-color: #808000\">...</span> <span style=\"font-weight: bold\">(</span>no timeout, will wait indefinitely<span style=\"font-weight: bold\">)</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "   Waiting for Embedding Model to become ready\u001b[33m...\u001b[0m \u001b[1m(\u001b[0mno timeout, will wait indefinitely\u001b[1m)\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   Current status: <span style=\"color: #808000; text-decoration-color: #808000\">healthy</span> <span style=\"font-weight: bold\">(</span>elapsed: 0s<span style=\"font-weight: bold\">)</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "   Current status: \u001b[33mhealthy\u001b[0m \u001b[1m(\u001b[0melapsed: 0s\u001b[1m)\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">\u2705 Embedding Model is ready!</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[32m\u2705 Embedding Model is ready!\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u2705 Embedding model deployed: nvidia/llama-3.2-nv-embedqa-1b-v2\n",
            "   Endpoint: https://agd6zdjymyanhi9g.ai.sandbox.nonprod-project-avengers.com\n",
            "\n",
            "   Deploying LLM model...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   Checking if model <span style=\"color: #008000; text-decoration-color: #008000\">'</span><span style=\"color: #008080; text-decoration-color: #008080\">agent-hub-llm-model</span><span style=\"color: #008000; text-decoration-color: #008000\">'</span> already exists<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "   Checking if model \u001b[32m'\u001b[0m\u001b[36magent-hub-llm-model\u001b[0m\u001b[32m'\u001b[0m already exists\u001b[33m...\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   Found <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> existing <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">model</span><span style=\"font-weight: bold\">(</span>s<span style=\"font-weight: bold\">)</span>.\n",
              "</pre>\n"
            ],
            "text/plain": [
              "   Found \u001b[1;36m2\u001b[0m existing \u001b[1;35mmodel\u001b[0m\u001b[1m(\u001b[0ms\u001b[1m)\u001b[0m.\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   <span style=\"color: #008000; text-decoration-color: #008000\">\u2705 Model </span><span style=\"color: #008000; text-decoration-color: #008000\">'agent-hub-llm-model'</span><span style=\"color: #008000; text-decoration-color: #008000\"> already exists </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">Status: healthy</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">)</span><span style=\"color: #008000; text-decoration-color: #008000\">.</span> Model ID: <span style=\"color: #008080; text-decoration-color: #008080\">122c445e-636d-48c9-a1b1-b41043b30d17</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "   \u001b[32m\u2705 Model \u001b[0m\u001b[32m'agent-hub-llm-model'\u001b[0m\u001b[32m already exists \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mStatus: healthy\u001b[0m\u001b[1;32m)\u001b[0m\u001b[32m.\u001b[0m Model ID: \u001b[36m122c445e-636d-48c9-a1b1-b41043b30d17\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   \u2705 Model config matches. Reusing existing model.\n",
              "</pre>\n"
            ],
            "text/plain": [
              "   \u2705 Model config matches. Reusing existing model.\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   Waiting for LLM Model to become ready<span style=\"color: #808000; text-decoration-color: #808000\">...</span> <span style=\"font-weight: bold\">(</span>no timeout, will wait indefinitely<span style=\"font-weight: bold\">)</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "   Waiting for LLM Model to become ready\u001b[33m...\u001b[0m \u001b[1m(\u001b[0mno timeout, will wait indefinitely\u001b[1m)\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   Current status: <span style=\"color: #808000; text-decoration-color: #808000\">healthy</span> <span style=\"font-weight: bold\">(</span>elapsed: 0s<span style=\"font-weight: bold\">)</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "   Current status: \u001b[33mhealthy\u001b[0m \u001b[1m(\u001b[0melapsed: 0s\u001b[1m)\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">\u2705 LLM Model is ready!</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[32m\u2705 LLM Model is ready!\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u2705 LLM model deployed: meta/llama3-8b-instruct\n",
            "   Endpoint: https://agd6zdjymyanhi9g.ai.sandbox.nonprod-project-avengers.com\n",
            "\n",
            "\n",
            "\ud83d\udd11 Step 11: Creating API Key for AI Models\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   Creating API key for models in region <span style=\"color: #008080; text-decoration-color: #008080\">us-east-</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "   Creating API key for models in region \u001b[36mus-east-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[33m...\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">\u2705 API key created successfully.</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[32m\u2705 API key created successfully.\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   Key ID: <span style=\"color: #008080; text-decoration-color: #008080\">e21f3a18-cabf-56c9-8576-3c21a01e44df</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "   Key ID: \u001b[36me21f3a18-cabf-56c9-8576-3c21a01e44df\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   Token: <span style=\"color: #008080; text-decoration-color: #008080\">cbsk-v1-MZgnu4Gnr2jo...</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "   Token: \u001b[36mcbsk-v1-MZgnu4Gnr2jo\u001b[0m\u001b[36m...\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u2705 AI API key created\n",
            "\n",
            "\n",
            "\u2699\ufe0f Step 12: Setting Environment Variables\n",
            "----------------------------------------------------------------------\n",
            "\u26a0\ufe0f  Added protocol to connection string: couchbases://cb.maggf6p2qyrvsjib.sandbox.nonprod-project-avengers.com\n",
            "\u2705 Environment variables configured:\n",
            "\n",
            "   CB_CONN_STRING: couchbases://cb.maggf6p2qyrvsjib.sandbox.nonprod-project-avengers.com\n",
            "   CB_USERNAME: agent_app_user\n",
            "   CB_BUCKET: travel-sample\n",
            "   CAPELLA_API_EMBEDDING_ENDPOINT: https://agd6zdjymyanhi9g.ai.sandbox.nonprod-project-avengers.com\n",
            "   CAPELLA_API_LLM_ENDPOINT: https://agd6zdjymyanhi9g.ai.sandbox.nonprod-project-avengers.com\n",
            "   CAPELLA_API_EMBEDDING_MODEL: nvidia/llama-3.2-nv-embedqa-1b-v2\n",
            "   CAPELLA_API_LLM_MODEL: meta/llama3-8b-instruct\n",
            "\n",
            "======================================================================\n",
            "\u2705 Infrastructure Setup Complete!\n",
            "======================================================================\n",
            "\n",
            "You can now run the flight search agent cells below.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"\ud83d\ude80 Couchbase Capella Infrastructure Setup\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nThis educational setup shows you how to provision Capella infrastructure\")\n",
        "print(\"step-by-step using the couchbase-infrastructure package.\\n\")\n",
        "\n",
        "# Import the infrastructure package\n",
        "from couchbase_infrastructure import CapellaConfig, CapellaClient\n",
        "from couchbase_infrastructure.resources import (\n",
        "    create_project,\n",
        "    create_developer_pro_cluster,\n",
        "    add_allowed_cidr,\n",
        "    load_sample_data,\n",
        "    create_database_user,\n",
        "    deploy_ai_model,\n",
        "    create_ai_api_key,\n",
        ")\n",
        "\n",
        "# Step 1: Load from .env file if available, then collect any missing credentials\n",
        "print(\"\\n\ud83d\udccb Step 1: Collecting Credentials\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "# Try to load .env file\n",
        "env_file = Path('.env')\n",
        "if env_file.exists():\n",
        "    print(\"\u2705 Found .env file. Loading configuration...\\n\")\n",
        "    from dotenv import load_dotenv\n",
        "    load_dotenv('.env')\n",
        "else:\n",
        "    print(\"\u2139\ufe0f  No .env file found. Will prompt for credentials.\\n\")\n",
        "\n",
        "print(\"Get your credentials from: https://cloud.couchbase.com \u2192 Settings \u2192 API Keys\\n\")\n",
        "\n",
        "# Required: MANAGEMENT_API_KEY\n",
        "management_api_key = os.getenv('MANAGEMENT_API_KEY')\n",
        "if management_api_key:\n",
        "    print(\"\u2705 Using MANAGEMENT_API_KEY from environment\")\n",
        "else:\n",
        "    management_api_key = getpass(\"Enter your MANAGEMENT_API_KEY (hidden): \")\n",
        "    if not management_api_key:\n",
        "        raise ValueError(\"MANAGEMENT_API_KEY is required!\")\n",
        "\n",
        "# Required: ORGANIZATION_ID\n",
        "organization_id = os.getenv('ORGANIZATION_ID')\n",
        "if organization_id:\n",
        "    print(f\"\u2705 Using ORGANIZATION_ID from environment: {organization_id}\")\n",
        "else:\n",
        "    organization_id = input(\"Enter your ORGANIZATION_ID (required): \").strip()\n",
        "    if not organization_id:\n",
        "        raise ValueError(\"ORGANIZATION_ID is required! Find it in Capella Console under Settings.\")\n",
        "\n",
        "# Optional configuration (use env vars if available, otherwise prompt with defaults)\n",
        "api_base_url = os.getenv('API_BASE_URL') or input(\"Enter API_BASE_URL (default: 'cloudapi.cloud.couchbase.com'): \").strip() or \"cloudapi.cloud.couchbase.com\"\n",
        "project_name = os.getenv('PROJECT_NAME') or input(\"Enter PROJECT_NAME (default: 'agent-app'): \").strip() or \"agent-app\"\n",
        "cluster_name = os.getenv('CLUSTER_NAME') or input(\"Enter CLUSTER_NAME (default: 'agent-app-cluster'): \").strip() or \"agent-app-cluster\"\n",
        "db_username = os.getenv('DB_USERNAME') or input(\"Enter DB_USERNAME (default: 'agent_app_user'): \").strip() or \"agent_app_user\"\n",
        "sample_bucket = os.getenv('SAMPLE_BUCKET') or input(\"Enter BUCKET_NAME (default: 'travel-sample'): \").strip() or \"travel-sample\"\n",
        "embedding_model = os.getenv('EMBEDDING_MODEL_NAME') or input(\"Enter EMBEDDING_MODEL (default: 'nvidia/llama-3.2-nv-embedqa-1b-v2'): \").strip() or \"nvidia/llama-3.2-nv-embedqa-1b-v2\"\n",
        "llm_model = os.getenv('LLM_MODEL_NAME') or input(\"Enter LLM_MODEL (default: 'meta/llama3-8b-instruct'): \").strip() or \"meta/llama3-8b-instruct\"\n",
        "\n",
        "print(\"\\n\u2705 Configuration collected successfully!\\n\")\n",
        "\n",
        "# Step 2: Initialize configuration\n",
        "print(\"\\n\ud83d\udd27 Step 2: Initializing Configuration\")\n",
        "print(\"-\"*70)\n",
        "config = CapellaConfig(\n",
        "    management_api_key=management_api_key,\n",
        "    organization_id=organization_id,\n",
        "    api_base_url=api_base_url,\n",
        "    project_name=project_name,\n",
        "    cluster_name=cluster_name,\n",
        "    db_username=db_username,\n",
        "    sample_bucket=sample_bucket,\n",
        "    embedding_model_name=embedding_model,\n",
        "    llm_model_name=llm_model,\n",
        ")\n",
        "print(\"\u2705 Configuration initialized\\n\")\n",
        "\n",
        "# Step 3: Initialize client and get organization ID\n",
        "print(\"\\n\ud83d\udd0c Step 3: Initializing Client\")\n",
        "print(\"-\"*70)\n",
        "client = CapellaClient(config)\n",
        "org_id = client.get_organization_id()\n",
        "print(f\"\u2705 Using Organization ID: {org_id}\\n\")\n",
        "\n",
        "# Step 4: Test API connection\n",
        "print(\"\\n\ud83d\udd0d Step 4: Testing API Connection\")\n",
        "print(\"-\"*70)\n",
        "if not client.test_connection(org_id):\n",
        "    raise ConnectionError(\"Failed to connect to Capella API\")\n",
        "print(\"\u2705 API connection successful\\n\")\n",
        "\n",
        "# Step 5: Create Capella Project\n",
        "print(\"\\n\ud83d\udcc1 Step 5: Creating Capella Project\")\n",
        "print(\"-\"*70)\n",
        "project_id = create_project(client, org_id, config.project_name)\n",
        "print(f\"\u2705 Project ready: {config.project_name} (ID: {project_id})\\n\")\n",
        "\n",
        "# Step 6: Create Developer Pro cluster with Analytics\n",
        "print(\"\\n\u2601\ufe0f Step 6: Creating Developer Pro Cluster with Analytics\")\n",
        "print(\"-\"*70)\n",
        "print(\"\u23f3 This will take 10-15 minutes for cluster deployment...\\n\")\n",
        "cluster_id = create_developer_pro_cluster(\n",
        "    client, org_id, project_id, config.cluster_name,\n",
        "    cloud_provider=config.cluster_cloud_provider,\n",
        "    region=config.cluster_region,\n",
        "    cidr=config.cluster_cidr\n",
        ")\n",
        "# Wait for cluster to be ready\n",
        "cluster_check_url = f\"/v4/organizations/{org_id}/projects/{project_id}/clusters/{cluster_id}\"\n",
        "cluster_details = client.wait_for_resource(cluster_check_url, \"Cluster\", None)\n",
        "cluster_conn_string = cluster_details.get(\"connectionString\")\n",
        "print(f\"\u2705 Cluster ready: {config.cluster_name} (ID: {cluster_id})\\n\")\n",
        "\n",
        "# Step 7: Configure network access\n",
        "print(\"\\n\ud83c\udf10 Step 7: Configuring Network Access\")\n",
        "print(\"-\"*70)\n",
        "add_allowed_cidr(client, org_id, project_id, cluster_id, config.allowed_cidr)\n",
        "print(\"\u2705 Network access configured (0.0.0.0/0 allowed)\\n\")\n",
        "\n",
        "# Step 8: Load travel-sample bucket\n",
        "print(\"\\n\ud83d\udce6 Step 8: Loading travel-sample Bucket\")\n",
        "print(\"-\"*70)\n",
        "load_sample_data(client, org_id, project_id, cluster_id, config.sample_bucket)\n",
        "print(f\"\u2705 Sample data loaded: {config.sample_bucket}\\n\")\n",
        "\n",
        "# Step 9: Create database user (password auto-generated)\n",
        "print(\"\\n\ud83d\udc64 Step 9: Creating Database User\")\n",
        "print(\"-\"*70)\n",
        "db_password = create_database_user(\n",
        "    client,\n",
        "    org_id,\n",
        "    project_id,\n",
        "    cluster_id,\n",
        "    config.db_username,\n",
        "    config.sample_bucket,\n",
        "    recreate_if_exists=True,  # Delete and recreate if exists to get fresh password\n",
        ")\n",
        "print(f\"\u2705 Database user created: {config.db_username}\\n\")\n",
        "if db_password and db_password != \"existing_user_password_not_retrievable\":\n",
        "    print(f\"   Auto-generated password: {db_password[:4]}...{db_password[-4:]}\\n\")\n",
        "\n",
        "# Step 10: Deploy AI models\n",
        "print(\"\\n\ud83e\udd16 Step 10: Deploying AI Models\")\n",
        "print(\"-\"*70)\n",
        "print(\"\u23f3 Deploying embedding and LLM models (5-10 minutes)...\\n\")\n",
        "\n",
        "# Deploy Embedding Model\n",
        "print(\"   Deploying embedding model...\")\n",
        "embedding_model_id = deploy_ai_model(\n",
        "    client,\n",
        "    org_id,\n",
        "    config.embedding_model_name,\n",
        "    \"agent-hub-embedding-model\",\n",
        "    \"embedding\",\n",
        "    config,\n",
        ")\n",
        "embedding_check_url = f\"/v4/organizations/{org_id}/aiServices/models/{embedding_model_id}\"\n",
        "embedding_details = client.wait_for_resource(embedding_check_url, \"Embedding Model\", None)\n",
        "\n",
        "# Extract endpoint from nested 'model' object\n",
        "model_info = embedding_details.get(\"model\", {})\n",
        "embedding_endpoint = model_info.get(\"connectionString\", \"\")\n",
        "\n",
        "print(f\"\u2705 Embedding model deployed: {config.embedding_model_name}\")\n",
        "print(f\"   Endpoint: {embedding_endpoint}\\n\")\n",
        "\n",
        "# Deploy LLM Model\n",
        "print(\"   Deploying LLM model...\")\n",
        "llm_model_id = deploy_ai_model(\n",
        "    client,\n",
        "    org_id,\n",
        "    config.llm_model_name,\n",
        "    \"agent-hub-llm-model\",\n",
        "    \"llm\",\n",
        "    config,\n",
        ")\n",
        "llm_check_url = f\"/v4/organizations/{org_id}/aiServices/models/{llm_model_id}\"\n",
        "llm_details = client.wait_for_resource(llm_check_url, \"LLM Model\", None)\n",
        "\n",
        "# Extract endpoint from nested 'model' object\n",
        "model_info = llm_details.get(\"model\", {})\n",
        "llm_endpoint = model_info.get(\"connectionString\", \"\")\n",
        "\n",
        "print(f\"\u2705 LLM model deployed: {config.llm_model_name}\")\n",
        "print(f\"   Endpoint: {llm_endpoint}\\n\")\n",
        "\n",
        "# Step 11: Create API Key for AI models\n",
        "print(\"\\n\ud83d\udd11 Step 11: Creating API Key for AI Models\")\n",
        "print(\"-\"*70)\n",
        "api_key = create_ai_api_key(client, org_id, config.ai_model_region)\n",
        "print(f\"\u2705 AI API key created\\n\")\n",
        "\n",
        "# Step 12: Set environment variables\n",
        "print(\"\\n\u2699\ufe0f Step 12: Setting Environment Variables\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "# Set all environment variables for subsequent cells\n",
        "# Ensure connection string has proper protocol\n",
        "if not cluster_conn_string.startswith(\"couchbase://\") and not cluster_conn_string.startswith(\"couchbases://\"):\n",
        "    cluster_conn_string = f\"couchbases://{cluster_conn_string}\"\n",
        "    print(f\"\u26a0\ufe0f  Added protocol to connection string: {cluster_conn_string}\")\n",
        "\n",
        "os.environ[\"CB_CONN_STRING\"] = cluster_conn_string + \"?tls_verify=none\"\n",
        "os.environ[\"CB_USERNAME\"] = config.db_username\n",
        "os.environ[\"CB_PASSWORD\"] = db_password\n",
        "os.environ[\"CB_BUCKET\"] = config.sample_bucket\n",
        "os.environ[\"CAPELLA_API_ENDPOINT\"] = embedding_endpoint  # Use as base endpoint\n",
        "os.environ[\"CAPELLA_API_EMBEDDING_ENDPOINT\"] = embedding_endpoint\n",
        "os.environ[\"CAPELLA_API_LLM_ENDPOINT\"] = llm_endpoint\n",
        "os.environ[\"CAPELLA_API_EMBEDDINGS_KEY\"] = api_key\n",
        "os.environ[\"CAPELLA_API_LLM_KEY\"] = api_key\n",
        "os.environ[\"CAPELLA_API_EMBEDDING_MODEL\"] = config.embedding_model_name\n",
        "os.environ[\"CAPELLA_API_LLM_MODEL\"] = config.llm_model_name\n",
        "\n",
        "print(\"\u2705 Environment variables configured:\\n\")\n",
        "print(f\"   CB_CONN_STRING: {cluster_conn_string}\")\n",
        "print(f\"   CB_USERNAME: {config.db_username}\")\n",
        "print(f\"   CB_BUCKET: {config.sample_bucket}\")\n",
        "print(f\"   CAPELLA_API_EMBEDDING_ENDPOINT: {embedding_endpoint}\")\n",
        "print(f\"   CAPELLA_API_LLM_ENDPOINT: {llm_endpoint}\")\n",
        "print(f\"   CAPELLA_API_EMBEDDING_MODEL: {config.embedding_model_name}\")\n",
        "print(f\"   CAPELLA_API_LLM_MODEL: {config.llm_model_name}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"\u2705 Infrastructure Setup Complete!\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nYou can now run the flight search agent cells below.\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 789
        },
        "id": "0cNe4nUmgH14",
        "outputId": "4ed858f5-92e1-44c4-ebbd-711890ae910e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u2705 Agent Catalog environment variables set:\n",
            "   AGENT_CATALOG_CONN_STRING: couchbases://cb.maggf6p2qyrvsjib.sandbox.nonprod-project-avengers.com\n",
            "   AGENT_CATALOG_USERNAME: agent_app_user\n",
            "   AGENT_CATALOG_BUCKET: travel-sample\n",
            "\n",
            "======================================================================\n",
            "\ud83d\udcdc Root Certificate Setup\n",
            "======================================================================\n",
            "\n",
            "\u26a0\ufe0f  IMPORTANT: You need to download the root certificate from Capella UI\n",
            "\n",
            "Steps:\n",
            "1. Go to Capella Console: https://cloud.couchbase.com\n",
            "2. Navigate to your cluster \u2192 Connect tab\n",
            "3. Download the 'Root Certificate' file\n",
            "4. Upload it using the file upload below\n",
            "\n",
            "\ud83d\udce4 Please upload your root certificate file:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7cf88fd4-6ae6-48c5-b854-f15dbbc5de5d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-7cf88fd4-6ae6-48c5-b854-f15dbbc5de5d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving agent-app-cluster-root-certificate.txt to agent-app-cluster-root-certificate.txt\n",
            "\n",
            "\u2705 Root certificate uploaded: agent-app-cluster-root-certificate.txt\n",
            "   AGENT_CATALOG_CONN_ROOT_CERTIFICATE: agent-app-cluster-root-certificate.txt\n",
            "\n",
            "======================================================================\n",
            "\u2705 Agent Catalog Configuration Complete\n",
            "======================================================================\n",
            "\n",
            "\u2705 Environment variables written to .env file for agentc commands\n",
            "\n",
            "\ud83d\udd0d Verifying .env file contents:\n",
            "/content\n",
            "-rw-r--r-- 1 root root 1022 Oct 23 16:01 .env\n",
            "\n",
            "First 5 lines of .env (passwords masked):\n",
            "  CB_CONN_STRING=couchbases://cb.maggf6p2qyrvsjib.sandbox.nonprod-project-avengers.com?tls_verify=none\n",
            "  CB_USERNAME=agent_app_user\n",
            "  CB_PASSWORD=***\n",
            "  CB_BUCKET=travel-sample\n",
            "  CAPELLA_API_ENDPOINT=https://agd6zdjymyanhi9g.ai.sandbox.nonprod-project-avengers.com\n"
          ]
        }
      ],
      "source": [
        "# Set Agent Catalog environment variables (required for agentc commands)\n",
        "# These use the same Couchbase connection created above\n",
        "import os\n",
        "\n",
        "# Strip the ?tls_verify=none from the connection string for agentc\n",
        "conn_string = os.environ[\"CB_CONN_STRING\"]\n",
        "if conn_string.endswith(\"?tls_verify=none\"):\n",
        "    conn_string_clean = conn_string[:-len(\"?tls_verify=none\")]\n",
        "else:\n",
        "    conn_string_clean = conn_string\n",
        "\n",
        "# Ensure connection string has proper protocol (agentc requires couchbase:// or couchbases://)\n",
        "if not conn_string_clean.startswith(\"couchbase://\") and not conn_string_clean.startswith(\"couchbases://\"):\n",
        "    # Add couchbases:// protocol for secure connections\n",
        "    conn_string_clean = f\"couchbases://{conn_string_clean}\"\n",
        "    print(f\"\u26a0\ufe0f  Added protocol to connection string: {conn_string_clean}\")\n",
        "\n",
        "os.environ[\"AGENT_CATALOG_CONN_STRING\"] = conn_string_clean\n",
        "\n",
        "os.environ[\"AGENT_CATALOG_USERNAME\"] = os.environ[\"CB_USERNAME\"]\n",
        "os.environ[\"AGENT_CATALOG_PASSWORD\"] = os.environ[\"CB_PASSWORD\"]\n",
        "os.environ[\"AGENT_CATALOG_BUCKET\"] = os.environ[\"CB_BUCKET\"]\n",
        "\n",
        "print(\"\u2705 Agent Catalog environment variables set:\")\n",
        "print(f\"   AGENT_CATALOG_CONN_STRING: {os.environ['AGENT_CATALOG_CONN_STRING']}\")\n",
        "print(f\"   AGENT_CATALOG_USERNAME: {os.environ['AGENT_CATALOG_USERNAME']}\")\n",
        "print(f\"   AGENT_CATALOG_BUCKET: {os.environ['AGENT_CATALOG_BUCKET']}\")\n",
        "\n",
        "# Handle root certificate (required for secure connections)\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"\ud83d\udcdc Root Certificate Setup\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\n\u26a0\ufe0f  IMPORTANT: You need to download the root certificate from Capella UI\")\n",
        "print(\"\\nSteps:\")\n",
        "print(\"1. Go to Capella Console: https://cloud.couchbase.com\")\n",
        "print(\"2. Navigate to your cluster \u2192 Connect tab\")\n",
        "print(\"3. Download the 'Root Certificate' file\")\n",
        "print(\"4. Upload it using the file upload below\\n\")\n",
        "\n",
        "# Try to use Google Colab's file upload, fallback to manual input\n",
        "try:\n",
        "    from google.colab import files\n",
        "    print(\"\ud83d\udce4 Please upload your root certificate file:\")\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    if uploaded:\n",
        "        cert_filename = list(uploaded.keys())[0]\n",
        "        # Validate it's actually a certificate file\n",
        "        if cert_filename.endswith(('.pem', '.crt', '.cer', '.txt')):\n",
        "            os.environ[\"AGENT_CATALOG_CONN_ROOT_CERTIFICATE\"] = cert_filename\n",
        "            print(f\"\\n\u2705 Root certificate uploaded: {cert_filename}\")\n",
        "            print(f\"   AGENT_CATALOG_CONN_ROOT_CERTIFICATE: {cert_filename}\")\n",
        "        else:\n",
        "            print(f\"\\n\u26a0\ufe0f  Uploaded file '{cert_filename}' doesn't appear to be a certificate (.pem, .crt, .cer, .txt)\")\n",
        "            print(\"   Skipping certificate setup. You can configure it later if needed.\")\n",
        "            os.environ[\"AGENT_CATALOG_CONN_ROOT_CERTIFICATE\"] = \"\"\n",
        "    else:\n",
        "        print(\"\\n\u26a0\ufe0f  No file uploaded. You can set it manually later if needed.\")\n",
        "        os.environ[\"AGENT_CATALOG_CONN_ROOT_CERTIFICATE\"] = \"\"\n",
        "except ImportError:\n",
        "    # Not in Colab - ask user to place file and provide filename\n",
        "    print(\"\ud83d\udcdd Not running in Google Colab.\")\n",
        "    print(\"   Please place the root certificate file in the current directory.\\n\")\n",
        "    cert_filename = input(\"Enter the certificate filename (or press Enter to skip): \").strip()\n",
        "\n",
        "    if cert_filename:\n",
        "        os.environ[\"AGENT_CATALOG_CONN_ROOT_CERTIFICATE\"] = cert_filename\n",
        "        print(f\"\\n\u2705 Root certificate set: {cert_filename}\")\n",
        "    else:\n",
        "        print(\"\\n\u26a0\ufe0f  Root certificate not set. You can add it manually later if needed.\")\n",
        "        os.environ[\"AGENT_CATALOG_CONN_ROOT_CERTIFICATE\"] = \"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"\u2705 Agent Catalog Configuration Complete\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Write environment variables to .env file for agentc commands\n",
        "# agentc CLI will load from .env file automatically\n",
        "import os.path\n",
        "with open('.env', 'w') as f:\n",
        "    # CB variables (needed for database operations - prevents wiping by dotenv.load_dotenv)\n",
        "    f.write(f\"CB_CONN_STRING={os.environ['CB_CONN_STRING']}\\n\")\n",
        "    f.write(f\"CB_USERNAME={os.environ['CB_USERNAME']}\\n\")\n",
        "    f.write(f\"CB_PASSWORD={os.environ['CB_PASSWORD']}\\n\")\n",
        "    f.write(f\"CB_BUCKET={os.environ['CB_BUCKET']}\\n\")\n",
        "\n",
        "    # CAPELLA_API variables (needed for AI services - prevents wiping by dotenv.load_dotenv)\n",
        "    f.write(f\"CAPELLA_API_ENDPOINT={os.environ.get('CAPELLA_API_ENDPOINT', '')}\\n\")\n",
        "    f.write(f\"CAPELLA_API_EMBEDDING_ENDPOINT={os.environ.get('CAPELLA_API_EMBEDDING_ENDPOINT', '')}\\n\")\n",
        "    f.write(f\"CAPELLA_API_LLM_ENDPOINT={os.environ.get('CAPELLA_API_LLM_ENDPOINT', '')}\\n\")\n",
        "    f.write(f\"CAPELLA_API_EMBEDDINGS_KEY={os.environ.get('CAPELLA_API_EMBEDDINGS_KEY', '')}\\n\")\n",
        "    f.write(f\"CAPELLA_API_LLM_KEY={os.environ.get('CAPELLA_API_LLM_KEY', '')}\\n\")\n",
        "    f.write(f\"CAPELLA_API_EMBEDDING_MODEL={os.environ.get('CAPELLA_API_EMBEDDING_MODEL', '')}\\n\")\n",
        "    f.write(f\"CAPELLA_API_LLM_MODEL={os.environ.get('CAPELLA_API_LLM_MODEL', '')}\\n\")\n",
        "\n",
        "    # AGENT_CATALOG variables (for agentc CLI)\n",
        "    f.write(f\"AGENT_CATALOG_CONN_STRING={os.environ['AGENT_CATALOG_CONN_STRING']}\\n\")\n",
        "    f.write(f\"AGENT_CATALOG_USERNAME={os.environ['AGENT_CATALOG_USERNAME']}\\n\")\n",
        "    f.write(f\"AGENT_CATALOG_PASSWORD={os.environ['AGENT_CATALOG_PASSWORD']}\\n\")\n",
        "    f.write(f\"AGENT_CATALOG_BUCKET={os.environ['AGENT_CATALOG_BUCKET']}\\n\")\n",
        "\n",
        "    # Only write certificate if it exists and is a valid file\n",
        "    cert = os.environ.get('AGENT_CATALOG_CONN_ROOT_CERTIFICATE', '').strip()\n",
        "    if cert and os.path.isfile(cert):\n",
        "        f.write(f\"AGENT_CATALOG_CONN_ROOT_CERTIFICATE={cert}\\n\")\n",
        "    elif cert:\n",
        "        print(f\"\u26a0\ufe0f  Warning: Certificate file '{cert}' not found, skipping from .env\")\n",
        "\n",
        "print(\"\\n\u2705 Environment variables written to .env file for agentc commands\")\n",
        "\n",
        "# Verify .env file was created correctly\n",
        "print(\"\\n\ud83d\udd0d Verifying .env file contents:\")\n",
        "!pwd\n",
        "!ls -la .env\n",
        "print(\"\\nFirst 5 lines of .env (passwords masked):\")\n",
        "with open('.env', 'r') as f:\n",
        "    for i, line in enumerate(f):\n",
        "        if i < 5:\n",
        "            if 'PASSWORD' in line:\n",
        "                print(f\"  {line.split('=')[0]}=***\")\n",
        "            else:\n",
        "                print(f\"  {line.strip()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYyHXwKlyZy8"
      },
      "source": [
        "### Optional: Configure OpenAI and Arize (Observability)\n",
        "\n",
        "Provide optional API keys for:\n",
        "- **OpenAI**: Fallback LLM/embeddings if Capella AI is unavailable\n",
        "- **Arize Phoenix**: Observability and evaluation platform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_uwDtVrNyZy9",
        "outputId": "d9686d64-e997-4ef2-a4c3-a547cc3db2ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "\ud83d\udd27 Optional API Keys Configuration\n",
            "======================================================================\n",
            "\n",
            "\ud83d\udcdd OpenAI API (Optional - for fallback LLM/embeddings)\n",
            "----------------------------------------------------------------------\n",
            "Press Enter to skip, or provide your OpenAI API key:\n",
            "OpenAI API Key: \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n",
            "\u2705 OpenAI API key configured\n",
            "   Model: gpt-4o\n",
            "\n",
            "\ud83d\udcca Arize Phoenix (Optional - for observability and evaluation)\n",
            "----------------------------------------------------------------------\n",
            "Press Enter to skip, or provide your Arize credentials:\n",
            "Arize Space ID: \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n",
            "Arize API Key: \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n",
            "\u2705 Arize Phoenix configured\n",
            "\n",
            "======================================================================\n",
            "\u2705 Optional Configuration Complete\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"\ud83d\udd27 Optional API Keys Configuration\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# OpenAI Configuration (optional - for fallback)\n",
        "print(\"\\n\ud83d\udcdd OpenAI API (Optional - for fallback LLM/embeddings)\")\n",
        "print(\"-\"*70)\n",
        "print(\"Press Enter to skip, or provide your OpenAI API key:\")\n",
        "try:\n",
        "    openai_api_key = getpass.getpass(\"OpenAI API Key: \").strip()\n",
        "except:\n",
        "    # Fallback for environments where getpass doesn't work\n",
        "    openai_api_key = \"\"\n",
        "\n",
        "if openai_api_key:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
        "    os.environ[\"OPENAI_MODEL\"] = \"gpt-4o\"  # Default model\n",
        "    print(\"\u2705 OpenAI API key configured\")\n",
        "    print(f\"   Model: gpt-4o\")\n",
        "else:\n",
        "    print(\"\u23ed\ufe0f  Skipped OpenAI configuration (will use Capella AI only)\")\n",
        "    os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "    os.environ[\"OPENAI_MODEL\"] = \"gpt-4o\"\n",
        "\n",
        "# Arize Phoenix Configuration (optional - for observability)\n",
        "print(\"\\n\ud83d\udcca Arize Phoenix (Optional - for observability and evaluation)\")\n",
        "print(\"-\"*70)\n",
        "print(\"Press Enter to skip, or provide your Arize credentials:\")\n",
        "try:\n",
        "    arize_space_id = getpass.getpass(\"Arize Space ID: \").strip()\n",
        "    arize_api_key = getpass.getpass(\"Arize API Key: \").strip() if arize_space_id else \"\"\n",
        "except:\n",
        "    # Fallback for environments where getpass doesn't work\n",
        "    arize_space_id = \"\"\n",
        "    arize_api_key = \"\"\n",
        "\n",
        "if arize_space_id and arize_api_key:\n",
        "    os.environ[\"ARIZE_SPACE_ID\"] = arize_space_id\n",
        "    os.environ[\"ARIZE_API_KEY\"] = arize_api_key\n",
        "    print(\"\u2705 Arize Phoenix configured\")\n",
        "else:\n",
        "    print(\"\u23ed\ufe0f  Skipped Arize configuration (observability disabled)\")\n",
        "    os.environ[\"ARIZE_SPACE_ID\"] = \"\"\n",
        "    os.environ[\"ARIZE_API_KEY\"] = \"\"\n",
        "\n",
        "# Append optional variables to .env file\n",
        "with open('.env', 'a') as f:\n",
        "    f.write(\"\\n# Optional: OpenAI Configuration (fallback LLM/embeddings)\\n\")\n",
        "    f.write(f\"OPENAI_API_KEY={os.environ['OPENAI_API_KEY']}\\n\")\n",
        "    f.write(f\"OPENAI_MODEL={os.environ['OPENAI_MODEL']}\\n\")\n",
        "\n",
        "    f.write(\"\\n# Optional: Arize Phoenix (observability and evaluation)\\n\")\n",
        "    f.write(f\"ARIZE_SPACE_ID={os.environ['ARIZE_SPACE_ID']}\\n\")\n",
        "    f.write(f\"ARIZE_API_KEY={os.environ['ARIZE_API_KEY']}\\n\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"\u2705 Optional Configuration Complete\")\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Czit8mxTgH14",
        "outputId": "35c26151-3eba-4c3a-c95b-2909f326d6ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mhint: Using 'master' as the name for the initial branch. This default branch name\u001b[m\n",
            "\u001b[33mhint: is subject to change. To configure the initial branch name to use in all\u001b[m\n",
            "\u001b[33mhint: of your new repositories, which will suppress this warning, call:\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: \tgit config --global init.defaultBranch <name>\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: Names commonly chosen instead of 'master' are 'main', 'trunk' and\u001b[m\n",
            "\u001b[33mhint: 'development'. The just-created branch can be renamed via this command:\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: \tgit branch -m <name>\u001b[m\n",
            "Initialized empty Git repository in /content/.git/\n"
          ]
        }
      ],
      "source": [
        "!git init\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dza6HeygH14",
        "outputId": "26c22bec-0d00-4507-f060-7ab8198fe2fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[master (root-commit) 048941a] initial commit\n",
            " 31 files changed, 52109 insertions(+)\n",
            " create mode 100644 .agentcignore\n",
            " create mode 100644 .config/.last_opt_in_prompt.yaml\n",
            " create mode 100644 .config/.last_survey_prompt.yaml\n",
            " create mode 100644 .config/.last_update_check.json\n",
            " create mode 100644 .config/active_config\n",
            " create mode 100644 .config/config_sentinel\n",
            " create mode 100644 .config/configurations/config_default\n",
            " create mode 100644 .config/default_configs.db\n",
            " create mode 100644 .config/gce\n",
            " create mode 100644 .config/hidden_gcloud_config_universe_descriptor_data_cache_configs.db\n",
            " create mode 100644 .config/logs/2025.10.21/16.51.10.047293.log\n",
            " create mode 100644 .config/logs/2025.10.21/16.51.30.790200.log\n",
            " create mode 100644 .config/logs/2025.10.21/16.51.39.965567.log\n",
            " create mode 100644 .config/logs/2025.10.21/16.51.41.225539.log\n",
            " create mode 100644 .config/logs/2025.10.21/16.51.50.106792.log\n",
            " create mode 100644 .config/logs/2025.10.21/16.51.50.856899.log\n",
            " create mode 100644 .env\n",
            " create mode 100644 agent-app-cluster-root-certificate.txt\n",
            " create mode 100644 agentcatalog_index.json\n",
            " create mode 100644 prompts/flight_search_assistant.yaml\n",
            " create mode 100755 sample_data/README.md\n",
            " create mode 100755 sample_data/anscombe.json\n",
            " create mode 100644 sample_data/california_housing_test.csv\n",
            " create mode 100644 sample_data/california_housing_train.csv\n",
            " create mode 100644 sample_data/mnist_test.csv\n",
            " create mode 100644 sample_data/mnist_train_small.csv\n",
            " create mode 100644 tools/lookup_flight_info.py\n",
            " create mode 100644 tools/retrieve_flight_bookings.py\n",
            " create mode 100644 tools/save_flight_booking.py\n",
            " create mode 100644 tools/schemas.py\n",
            " create mode 100644 tools/search_airline_reviews.py\n"
          ]
        }
      ],
      "source": [
        "!git add .\n",
        "!git config --global user.email \"your.email@example.com\"\n",
        "!git config --global user.name \"Your Name\"\n",
        "!git commit -m \"initial commit\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TrqZAWxagH14",
        "outputId": "063e7173-51f1-464a-cb09-046b44f03bbd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-10-23 16:02:20.568034: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1761235340.583772    1454 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1761235340.588192    1454 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1761235340.600424    1454 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761235340.600480    1454 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761235340.600485    1454 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761235340.600488    1454 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-10-23 16:02:20.604450: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "WARNING:agentc_core.learned.embedding:Failed to load embedding model sentence-transformers/all-MiniLM-L12-v2 (attempt 0): We couldn't connect to 'https://huggingface.co' to load the files, and couldn't find them in the cached files.\n",
            "Check your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.\n",
            "modules.json: 100% 349/349 [00:00<00:00, 1.76MB/s]\n",
            "config_sentence_transformers.json: 100% 116/116 [00:00<00:00, 457kB/s]\n",
            "README.md: 10.5kB [00:00, 19.5MB/s]\n",
            "sentence_bert_config.json: 100% 53.0/53.0 [00:00<00:00, 387kB/s]\n",
            "config.json: 100% 615/615 [00:00<00:00, 3.83MB/s]\n",
            "model.safetensors: 100% 133M/133M [00:01<00:00, 96.4MB/s]\n",
            "tokenizer_config.json: 100% 352/352 [00:00<00:00, 1.90MB/s]\n",
            "vocab.txt: 232kB [00:00, 13.6MB/s]\n",
            "tokenizer.json: 466kB [00:00, 35.3MB/s]\n",
            "special_tokens_map.json: 100% 112/112 [00:00<00:00, 928kB/s]\n",
            "config.json: 100% 190/190 [00:00<00:00, 1.19MB/s]\n",
            "\u001b[32mMetadata collection has been successfully created!\n",
            "\u001b[0m\n",
            "v2_AgentCatalogMetadataPrimaryIndex: 100% 1/1 [00:01<00:00,  1.34s/it]\n",
            "\u001b[32mGSI metadata index for the has been successfully created!\n",
            "\u001b[0m\n",
            "\u001b[33mNow creating the catalog collection for the prompt catalog.\u001b[0m\n",
            "\u001b[32mCollection for prompts has been successfully created!\n",
            "\u001b[0m\n",
            "\u001b[33mNow building the GSI indexes for the prompt catalog.\u001b[0m\n",
            "v2_AgentCatalogPromptsAnnotationsIndex: 100% 3/3 [00:03<00:00,  1.30s/it]\n",
            "\u001b[32mAll GSI indexes for the prompt catalog have been successfully created!\n",
            "\u001b[0m\n",
            "\u001b[33mNow building the vector index for the prompt catalog.\u001b[0m\n",
            "\u001b[32mVector index for the prompt catalog has been successfully created!\n",
            "\u001b[0m\n",
            "\u001b[33mNow creating the catalog collection for the tool catalog.\u001b[0m\n",
            "\u001b[32mCollection for tools has been successfully created!\n",
            "\u001b[0m\n",
            "\u001b[33mNow building the GSI indexes for the tool catalog.\u001b[0m\n",
            "v2_AgentCatalogToolsAnnotationsIndex: 100% 3/3 [00:03<00:00,  1.23s/it]\n",
            "\u001b[32mAll GSI indexes for the tool catalog have been successfully created!\n",
            "\u001b[0m\n",
            "\u001b[33mNow building the vector index for the tool catalog.\u001b[0m\n",
            "\u001b[32mVector index for the tool catalog has been successfully created!\n",
            "\u001b[0m\n",
            "\u001b[33mNow creating the analytics collections for our catalog.\u001b[0m\n",
            "\u001b[32mAll analytics collections for the catalog have been successfully created!\n",
            "\u001b[0m\n",
            "\u001b[33mNow creating scope and collections for the auditor.\u001b[0m\n",
            "\u001b[32mScope and collection for the auditor have been successfully created!\n",
            "\u001b[0m\n",
            "\u001b[33mNow creating the primary index for the auditor.\u001b[0m\n",
            "v2_AgentCatalogLogsPrimaryIndex: 100% 1/1 [00:01<00:00,  1.35s/it]\n",
            "\u001b[32mPrimary index for the auditor has been successfully created!\n",
            "\u001b[0m\n",
            "\u001b[33mNow creating the query UDFs for the auditor.\u001b[0m\n",
            "\u001b[32mAll query UDFs for the auditor have been successfully created!\n",
            "\u001b[0m\n",
            "\u001b[33mNow creating the analytics views for the auditor.\u001b[0m\n",
            "\u001b[32mAll analytics views for the auditor have been successfully created!\n",
            "\u001b[0m\n",
            "\u001b[32mPost-commit hook for  have successfully been installed!\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!agentc init\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NefIjKW_gH14",
        "outputId": "4c0b6f39-6eca-4284-b283-6f6e48e27f14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[95m\u001b[0m\n",
            "\u001b[95m\u001b[1mTOOL\u001b[0m\n",
            "\u001b[95m\u001b[0m\n",
            "Crawling .:\u001b[0m\n",
            "\r  0% 0/8 [00:00<?, ?it/s]\r.last_survey_prompt.yaml:   0% 0/8 [00:00<?, ?it/s]Encountered .yaml file with unknown record_kind field. Not indexing /content/.config/.last_survey_prompt.yaml.\n",
            ".last_opt_in_prompt.yaml:   0% 0/8 [00:00<?, ?it/s]Encountered .yaml file with unknown record_kind field. Not indexing /content/.config/.last_opt_in_prompt.yaml.\n",
            "flight_search_assistant.yaml: 100% 8/8 [00:04<00:00,  1.87it/s]\n",
            "\n",
            "Generating embeddings:\u001b[0m\n",
            "save_flight_booking:   0% 0/4 [00:00<?, ?it/s]2025-10-23 16:02:59.302536: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1761235379.316517    1714 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1761235379.320507    1714 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1761235379.330805    1714 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761235379.330840    1714 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761235379.330844    1714 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761235379.330848    1714 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-10-23 16:02:59.334061: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "lookup_flight_info: 100% 4/4 [00:07<00:00,  1.77s/it]\n",
            "\u001b[32m\n",
            "Catalog successfully indexed!\u001b[0m\n",
            "\u001b[95m\u001b[0m\n",
            "\u001b[34m\u001b[0m\n",
            "\u001b[34m\u001b[1mPROMPT\u001b[0m\n",
            "\u001b[34m\u001b[0m\n",
            "Crawling .:\u001b[0m\n",
            ".last_survey_prompt.yaml:   0% 0/3 [00:00<?, ?it/s]WARNING:agentc_core.indexer.indexer:Encountered .yaml file with unknown record_kind field. Not indexing /content/.config/.last_survey_prompt.yaml.\n",
            ".last_opt_in_prompt.yaml:   0% 0/3 [00:00<?, ?it/s]WARNING:agentc_core.indexer.indexer:Encountered .yaml file with unknown record_kind field. Not indexing /content/.config/.last_opt_in_prompt.yaml.\n",
            "flight_search_assistant.yaml: 100% 3/3 [00:00<00:00, 232.21it/s]\n",
            "\n",
            "Generating embeddings:\u001b[0m\n",
            "flight_search_assistant: 100% 1/1 [00:00<00:00,  7.42it/s]\n",
            "\u001b[32m\n",
            "Catalog successfully indexed!\u001b[0m\n",
            "\u001b[34m\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!agentc index .\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgV2nmNpgH14",
        "outputId": "690cfe8b-b0db-4e17-e169-5ead4e35d762"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[95m\u001b[0m\n",
            "\u001b[95m\u001b[1mTOOL\u001b[0m\n",
            "\u001b[95m\u001b[0m\n",
            "Using the catalog identifier: \u001b[0m\u001b[1m048941ad6fb885d9d3892c84f56b61878d05e633\n",
            "\u001b[0m\n",
            "\u001b[33mUploading the tool catalog items to Couchbase.\u001b[0m\n",
            "search_airline_reviews: 100% 4/4 [00:00<00:00, 24.95it/s]\n",
            "\u001b[32mTool catalog items successfully uploaded to Couchbase!\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[0m\n",
            "\u001b[34m\u001b[1mPROMPT\u001b[0m\n",
            "\u001b[34m\u001b[0m\n",
            "Using the catalog identifier: \u001b[0m\u001b[1m048941ad6fb885d9d3892c84f56b61878d05e633\n",
            "\u001b[0m\n",
            "\u001b[33mUploading the prompt catalog items to Couchbase.\u001b[0m\n",
            "flight_search_assistant: 100% 1/1 [00:00<00:00, 15.98it/s]\n",
            "\u001b[32mPrompt catalog items successfully uploaded to Couchbase!\n",
            "\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!agentc publish\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abGx3vEkmTCr",
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Setup and Imports\n",
        "\n",
        "Import all necessary modules for the flight search agent using the latest code structure.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fp4GDdukmTCr",
        "outputId": "fc5c1194-3e6e-4bc5-83f9-48c7ce5cb796"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import json\n",
        "import logging\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "from datetime import timedelta\n",
        "\n",
        "import agentc\n",
        "import agentc_langgraph.agent\n",
        "import agentc_langgraph.graph\n",
        "import dotenv\n",
        "import langchain_core.messages\n",
        "import langchain_core.runnables\n",
        "import langchain_openai.chat_models\n",
        "import langgraph.graph\n",
        "from couchbase.auth import PasswordAuthenticator\n",
        "from couchbase.cluster import Cluster\n",
        "from couchbase.exceptions import KeyspaceNotFoundException\n",
        "from couchbase.options import ClusterOptions\n",
        "from langchain.agents import AgentExecutor, create_react_agent\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.tools import Tool\n",
        "from pydantic import SecretStr\n",
        "\n",
        "# Setup logging for Colab (must use stdout instead of stderr)\n",
        "root_logger = logging.getLogger()\n",
        "if not root_logger.handlers:\n",
        "    handler = logging.StreamHandler(sys.stdout)\n",
        "    formatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n",
        "    handler.setFormatter(formatter)\n",
        "    root_logger.addHandler(handler)\n",
        "root_logger.setLevel(logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Suppress verbose logging from external libraries\n",
        "logging.getLogger(\"openai\").setLevel(logging.WARNING)\n",
        "logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n",
        "logging.getLogger(\"httpcore\").setLevel(logging.WARNING)\n",
        "logging.getLogger(\"agentc_core\").setLevel(logging.WARNING)\n",
        "\n",
        "# Load environment variables\n",
        "dotenv.load_dotenv(override=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7AQo_w5mTCs",
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Project Root Discovery and Shared Imports\n",
        "\n",
        "Essential project root discovery and shared module imports exactly as in working main.py.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "G2f3bgI_mTCs"
      },
      "outputs": [],
      "source": [
        "# INLINE IMPLEMENTATION - Universal AI Services and Couchbase Client\n",
        "import base64\n",
        "import time\n",
        "from typing import Tuple, Any, Optional, List\n",
        "from datetime import timedelta\n",
        "from couchbase.auth import PasswordAuthenticator\n",
        "from couchbase.cluster import Cluster\n",
        "from couchbase.exceptions import KeyspaceNotFoundException\n",
        "from couchbase.management.buckets import BucketType, CreateBucketSettings\n",
        "from couchbase.management.search import SearchIndex\n",
        "from couchbase.options import ClusterOptions\n",
        "\n",
        "# We'll add the function implementations in the next cell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umeyMLs0mTCs",
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Universal AI Services Setup\n",
        "\n",
        "5-case priority AI service setup implementation inline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "d3EuELHqmTCs"
      },
      "outputs": [],
      "source": [
        "def setup_ai_services(\n",
        "    framework: str = \"langchain\",\n",
        "    temperature: float = 0.0,\n",
        "    callbacks: Optional[List] = None,\n",
        "    application_span: Optional[Any] = None\n",
        ") -> Tuple[Any, Any]:\n",
        "    \"\"\"Priority 1 AI service setup - Capella with direct API keys + OpenAI wrappers only.\"\"\"\n",
        "    embeddings = None\n",
        "    llm = None\n",
        "\n",
        "    logger.info(f\"\ud83d\udd27 Setting up Priority 1 AI services for {framework} framework...\")\n",
        "\n",
        "    # PRIORITY 1: LATEST CAPELLA (OpenAI wrappers with direct API keys)\n",
        "    if (\n",
        "        not embeddings\n",
        "        and os.getenv(\"CAPELLA_API_ENDPOINT\")\n",
        "        and os.getenv(\"CAPELLA_API_EMBEDDINGS_KEY\")\n",
        "    ):\n",
        "        try:\n",
        "            if framework == \"llamaindex\":\n",
        "                from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "                embeddings = OpenAIEmbedding(\n",
        "                    api_key=os.getenv(\"CAPELLA_API_EMBEDDINGS_KEY\"),\n",
        "                    api_base=f\"{os.getenv('CAPELLA_API_ENDPOINT')}/v1\",\n",
        "                    model_name=os.getenv(\"CAPELLA_API_EMBEDDING_MODEL\"),\n",
        "                    embed_batch_size=30,\n",
        "                )\n",
        "            else:  # langchain, langgraph\n",
        "                from langchain_openai import OpenAIEmbeddings\n",
        "                embeddings = OpenAIEmbeddings(\n",
        "                    model=os.getenv(\"CAPELLA_API_EMBEDDING_MODEL\"),\n",
        "                    api_key=os.getenv(\"CAPELLA_API_EMBEDDINGS_KEY\"),\n",
        "                    base_url=f\"{os.getenv('CAPELLA_API_ENDPOINT')}/v1\",\n",
        "                    check_embedding_ctx_length=False,  # Fix for asymmetric models\n",
        "                )\n",
        "            logger.info(\"\u2705 Using latest Capella AI embeddings (direct API key + OpenAI wrapper)\")\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"\u26a0\ufe0f Latest Capella AI embeddings failed: {e}\")\n",
        "\n",
        "    if (\n",
        "        not llm\n",
        "        and os.getenv(\"CAPELLA_API_ENDPOINT\")\n",
        "        and os.getenv(\"CAPELLA_API_LLM_KEY\")\n",
        "    ):\n",
        "        try:\n",
        "            if framework == \"llamaindex\":\n",
        "                from llama_index.llms.openai_like import OpenAILike\n",
        "                llm = OpenAILike(\n",
        "                    model=os.getenv(\"CAPELLA_API_LLM_MODEL\"),\n",
        "                    api_base=f\"{os.getenv('CAPELLA_API_ENDPOINT')}/v1\",\n",
        "                    api_key=os.getenv(\"CAPELLA_API_LLM_KEY\"),\n",
        "                    is_chat_model=True,\n",
        "                    temperature=temperature,\n",
        "                )\n",
        "            else:  # langchain, langgraph\n",
        "                from langchain_openai import ChatOpenAI\n",
        "\n",
        "                chat_kwargs = {\n",
        "                    \"api_key\": os.getenv(\"CAPELLA_API_LLM_KEY\"),\n",
        "                    \"base_url\": f\"{os.getenv('CAPELLA_API_ENDPOINT')}/v1\",\n",
        "                    \"model\": os.getenv(\"CAPELLA_API_LLM_MODEL\"),\n",
        "                    \"temperature\": temperature,\n",
        "                }\n",
        "                if callbacks:\n",
        "                    chat_kwargs[\"callbacks\"] = callbacks\n",
        "\n",
        "                llm = ChatOpenAI(**chat_kwargs)\n",
        "\n",
        "            # Test the LLM works\n",
        "            if framework == \"llamaindex\":\n",
        "                llm.complete(\"Hello\")\n",
        "            else:\n",
        "                llm.invoke(\"Hello\")\n",
        "\n",
        "            logger.info(\"\u2705 Using latest Capella AI LLM (direct API key + OpenAI wrapper)\")\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"\u26a0\ufe0f Latest Capella AI LLM failed: {e}\")\n",
        "            llm = None\n",
        "\n",
        "    # VALIDATION\n",
        "    if not embeddings:\n",
        "        raise ValueError(\"\u274c No embeddings service could be initialized\")\n",
        "    if not llm:\n",
        "        raise ValueError(\"\u274c No LLM service could be initialized\")\n",
        "\n",
        "    logger.info(f\"\u2705 AI services setup completed for {framework}\")\n",
        "    return embeddings, llm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkfKX0OpmTCt",
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Helper Functions\n",
        "\n",
        "Environment setup and connectivity test functions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "dU591dHNmTCt"
      },
      "outputs": [],
      "source": [
        "def setup_environment():\n",
        "    \"\"\"Setup default environment variables for agent operations.\"\"\"\n",
        "    defaults = {\n",
        "        \"CB_BUCKET\": \"travel-sample\",\n",
        "        \"CB_SCOPE\": \"agentc_data\",\n",
        "        \"CB_COLLECTION\": \"airline_reviews\",\n",
        "        \"CB_INDEX\": \"airline_reviews_index\",\n",
        "        \"CAPELLA_API_EMBEDDING_MODEL\": \"nvidia/nv-embedqa-e5-v5\",\n",
        "        \"CAPELLA_API_LLM_MODEL\": \"meta/llama-3-8b-instruct\",\n",
        "        \"CAPELLA_API_EMBEDDING_MAX_TOKENS\": \"512\",\n",
        "        \"NVIDIA_API_EMBEDDING_MODEL\": \"nvidia/nv-embedqa-e5-v5\",\n",
        "        \"NVIDIA_API_LLM_MODEL\": \"meta/llama-3.1-70b-instruct\",\n",
        "    }\n",
        "\n",
        "    for key, value in defaults.items():\n",
        "        if not os.getenv(key):\n",
        "            os.environ[key] = value\n",
        "\n",
        "    logger.info(\"\u2705 Environment variables configured\")\n",
        "\n",
        "\n",
        "def test_capella_connectivity(api_key: str = None, endpoint: str = None) -> bool:\n",
        "    \"\"\"Test connectivity to Capella AI services.\"\"\"\n",
        "    try:\n",
        "        import httpx\n",
        "\n",
        "        test_key = api_key or os.getenv(\"CAPELLA_API_EMBEDDINGS_KEY\") or os.getenv(\"CAPELLA_API_LLM_KEY\")\n",
        "        test_endpoint = endpoint or os.getenv(\"CAPELLA_API_ENDPOINT\")\n",
        "\n",
        "        if not test_key or not test_endpoint:\n",
        "            return False\n",
        "\n",
        "        headers = {\"Authorization\": f\"Bearer {test_key}\"}\n",
        "\n",
        "        with httpx.Client(timeout=10.0) as client:\n",
        "            response = client.get(f\"{test_endpoint.rstrip('/')}/v1/models\", headers=headers)\n",
        "            return response.status_code < 500\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"\u26a0\ufe0f Capella connectivity test failed: {e}\")\n",
        "        return False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-8xkvOnmTCt",
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Universal Couchbase Client\n",
        "\n",
        "Complete Couchbase client implementation with database operations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Cz7oscM9mTCt"
      },
      "outputs": [],
      "source": [
        "class CouchbaseClient:\n",
        "    \"\"\"Universal Couchbase client for all database operations across agent frameworks.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        conn_string: str,\n",
        "        username: str,\n",
        "        password: str,\n",
        "        bucket_name: str,\n",
        "        wan_profile: bool = True,\n",
        "        timeout_seconds: int = 20,\n",
        "    ):\n",
        "        \"\"\"Initialize Couchbase client with connection details.\"\"\"\n",
        "        self.conn_string = conn_string\n",
        "        self.username = username\n",
        "        self.password = password\n",
        "        self.bucket_name = bucket_name\n",
        "        self.wan_profile = wan_profile\n",
        "        self.timeout_seconds = timeout_seconds\n",
        "        self.cluster = None\n",
        "        self.bucket = None\n",
        "        self._collections = {}\n",
        "\n",
        "    def connect(self):\n",
        "        \"\"\"Establish connection to Couchbase cluster.\"\"\"\n",
        "        try:\n",
        "            auth = PasswordAuthenticator(self.username, self.password)\n",
        "            options = ClusterOptions(auth)\n",
        "\n",
        "            if self.wan_profile:\n",
        "                options.apply_profile(\"wan_development\")\n",
        "\n",
        "            self.cluster = Cluster(self.conn_string, options)\n",
        "            self.cluster.wait_until_ready(timedelta(seconds=self.timeout_seconds))\n",
        "            logger.info(\"\u2705 Successfully connected to Couchbase\")\n",
        "            return self.cluster\n",
        "        except Exception as e:\n",
        "            raise ConnectionError(f\"\u274c Failed to connect to Couchbase: {e!s}\")\n",
        "\n",
        "    def setup_bucket(self, create_if_missing: bool = True):\n",
        "        \"\"\"Setup bucket - connect to existing or create if missing.\"\"\"\n",
        "        try:\n",
        "            if not self.cluster:\n",
        "                self.connect()\n",
        "\n",
        "            try:\n",
        "                self.bucket = self.cluster.bucket(self.bucket_name)\n",
        "                logger.info(f\"\u2705 Connected to existing bucket '{self.bucket_name}'\")\n",
        "                return self.bucket\n",
        "            except Exception as e:\n",
        "                logger.info(f\"\u26a0\ufe0f Bucket '{self.bucket_name}' not accessible: {e}\")\n",
        "\n",
        "            if create_if_missing:\n",
        "                logger.info(f\"\ud83d\udd27 Creating bucket '{self.bucket_name}'...\")\n",
        "                bucket_settings = CreateBucketSettings(\n",
        "                    name=self.bucket_name,\n",
        "                    bucket_type=BucketType.COUCHBASE,\n",
        "                    ram_quota_mb=1024,\n",
        "                    flush_enabled=True,\n",
        "                    num_replicas=0,\n",
        "                )\n",
        "                self.cluster.buckets().create_bucket(bucket_settings)\n",
        "                time.sleep(5)\n",
        "                self.bucket = self.cluster.bucket(self.bucket_name)\n",
        "                logger.info(f\"\u2705 Bucket '{self.bucket_name}' created successfully\")\n",
        "                return self.bucket\n",
        "            else:\n",
        "                raise RuntimeError(f\"\u274c Bucket '{self.bucket_name}' not found\")\n",
        "\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"\u274c Error setting up bucket: {e!s}\")\n",
        "\n",
        "    def setup_collection(self, scope_name: str, collection_name: str, clear_existing_data: bool = True, create_primary_index: bool = True):\n",
        "        \"\"\"Setup collection with comprehensive options.\"\"\"\n",
        "        try:\n",
        "            if not self.bucket:\n",
        "                self.setup_bucket()\n",
        "\n",
        "            bucket_manager = self.bucket.collections()\n",
        "            scopes = bucket_manager.get_all_scopes()\n",
        "            scope_exists = any(scope.name == scope_name for scope in scopes)\n",
        "\n",
        "            if not scope_exists and scope_name != \"_default\":\n",
        "                logger.info(f\"\ud83d\udd27 Creating scope '{scope_name}'...\")\n",
        "                bucket_manager.create_scope(scope_name)\n",
        "                logger.info(f\"\u2705 Scope '{scope_name}' created successfully\")\n",
        "\n",
        "            collections = bucket_manager.get_all_scopes()\n",
        "            collection_exists = any(\n",
        "                scope.name == scope_name\n",
        "                and collection_name in [col.name for col in scope.collections]\n",
        "                for scope in collections\n",
        "            )\n",
        "\n",
        "            if collection_exists:\n",
        "                if clear_existing_data:\n",
        "                    logger.info(f\"\ud83d\uddd1\ufe0f Collection '{collection_name}' exists, clearing data...\")\n",
        "                    self.clear_collection_data(scope_name, collection_name)\n",
        "                else:\n",
        "                    logger.info(f\"\u2139\ufe0f Collection '{collection_name}' exists, keeping existing data\")\n",
        "            else:\n",
        "                logger.info(f\"\ud83d\udd27 Creating collection '{collection_name}'...\")\n",
        "                bucket_manager.create_collection(scope_name, collection_name)\n",
        "                logger.info(f\"\u2705 Collection '{collection_name}' created successfully\")\n",
        "\n",
        "            time.sleep(3)\n",
        "\n",
        "            if create_primary_index:\n",
        "                try:\n",
        "                    self.cluster.query(\n",
        "                        f\"CREATE PRIMARY INDEX IF NOT EXISTS ON `{self.bucket_name}`.`{scope_name}`.`{collection_name}`\"\n",
        "                    ).execute()\n",
        "                    logger.info(\"\u2705 Primary index created successfully\")\n",
        "                except Exception as e:\n",
        "                    logger.warning(f\"\u26a0\ufe0f Error creating primary index: {e}\")\n",
        "\n",
        "            collection_key = f\"{scope_name}.{collection_name}\"\n",
        "            collection = self.bucket.scope(scope_name).collection(collection_name)\n",
        "            self._collections[collection_key] = collection\n",
        "\n",
        "            logger.info(f\"\u2705 Collection setup complete: {scope_name}.{collection_name}\")\n",
        "            return collection\n",
        "\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"\u274c Error setting up collection: {e!s}\")\n",
        "\n",
        "    def clear_collection_data(self, scope_name: str, collection_name: str, verify_cleared: bool = True):\n",
        "        \"\"\"Clear all data from a collection.\"\"\"\n",
        "        try:\n",
        "            logger.info(f\"\ud83d\uddd1\ufe0f Clearing data from {self.bucket_name}.{scope_name}.{collection_name}...\")\n",
        "\n",
        "            delete_query = f\"DELETE FROM `{self.bucket_name}`.`{scope_name}`.`{collection_name}`\"\n",
        "            result = self.cluster.query(delete_query)\n",
        "            list(result)\n",
        "\n",
        "            time.sleep(2)\n",
        "\n",
        "            if verify_cleared:\n",
        "                count_query = f\"SELECT COUNT(*) as count FROM `{self.bucket_name}`.`{scope_name}`.`{collection_name}`\"\n",
        "                count_result = self.cluster.query(count_query)\n",
        "                count_row = list(count_result)[0]\n",
        "                remaining_count = count_row[\"count\"]\n",
        "\n",
        "                if remaining_count == 0:\n",
        "                    logger.info(f\"\u2705 Collection cleared successfully\")\n",
        "                else:\n",
        "                    logger.warning(f\"\u26a0\ufe0f Collection clear incomplete, {remaining_count} documents remaining\")\n",
        "\n",
        "        except KeyspaceNotFoundException:\n",
        "            logger.info(f\"\u2139\ufe0f Collection doesn't exist, nothing to clear\")\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"\u26a0\ufe0f Error clearing collection data: {e}\")\n",
        "\n",
        "    def clear_scope(self, scope_name: str):\n",
        "        \"\"\"Clear all collections in the specified scope.\"\"\"\n",
        "        try:\n",
        "            if not self.bucket:\n",
        "                self.setup_bucket()\n",
        "\n",
        "            logger.info(f\"\ud83d\uddd1\ufe0f Clearing scope: {self.bucket_name}.{scope_name}\")\n",
        "            bucket_manager = self.bucket.collections()\n",
        "            scopes = bucket_manager.get_all_scopes()\n",
        "\n",
        "            target_scope = None\n",
        "            for scope in scopes:\n",
        "                if scope.name == scope_name:\n",
        "                    target_scope = scope\n",
        "                    break\n",
        "\n",
        "            if not target_scope:\n",
        "                logger.info(f\"\u2139\ufe0f Scope '{self.bucket_name}.{scope_name}' does not exist\")\n",
        "                return\n",
        "\n",
        "            for collection in target_scope.collections:\n",
        "                try:\n",
        "                    self.clear_collection_data(scope_name, collection.name, verify_cleared=False)\n",
        "                    logger.info(f\"\u2705 Cleared collection: {collection.name}\")\n",
        "                except Exception as e:\n",
        "                    logger.warning(f\"\u26a0\ufe0f Could not clear collection {collection.name}: {e}\")\n",
        "\n",
        "            logger.info(f\"\u2705 Completed clearing scope: {self.bucket_name}.{scope_name}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"\u274c Could not clear scope: {e}\")\n",
        "\n",
        "    def setup_vector_search_index(self, index_definition: dict, scope_name: str):\n",
        "        \"\"\"Setup vector search index for the specified scope.\"\"\"\n",
        "        try:\n",
        "            if not self.bucket:\n",
        "                raise RuntimeError(\"\u274c Bucket not initialized. Call setup_bucket first.\")\n",
        "\n",
        "            scope_index_manager = self.bucket.scope(scope_name).search_indexes()\n",
        "            existing_indexes = scope_index_manager.get_all_indexes()\n",
        "            index_name = index_definition[\"name\"]\n",
        "\n",
        "            if index_name not in [index.name for index in existing_indexes]:\n",
        "                logger.info(f\"\ud83d\udd27 Creating vector search index '{index_name}'...\")\n",
        "                search_index = SearchIndex.from_json(index_definition)\n",
        "                scope_index_manager.upsert_index(search_index)\n",
        "                logger.info(f\"\u2705 Vector search index '{index_name}' created successfully\")\n",
        "            else:\n",
        "                logger.info(f\"\u2139\ufe0f Vector search index '{index_name}' already exists\")\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"\u274c Error setting up vector search index: {e!s}\")\n",
        "\n",
        "    def load_index_definition(self, index_file_path: str = \"agentcatalog_index.json\"):\n",
        "        \"\"\"Load vector search index definition from JSON file.\"\"\"\n",
        "        try:\n",
        "            with open(index_file_path) as file:\n",
        "                index_definition = json.load(file)\n",
        "            logger.info(f\"\u2705 Loaded index definition from {index_file_path}\")\n",
        "            return index_definition\n",
        "        except FileNotFoundError:\n",
        "            logger.warning(f\"\u26a0\ufe0f {index_file_path} not found\")\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"\u26a0\ufe0f Error loading index definition: {e!s}\")\n",
        "            return None\n",
        "\n",
        "    def setup_vector_store_langchain(self, scope_name: str, collection_name: str, index_name: str, embeddings, data_loader_func=None, **loader_kwargs):\n",
        "        \"\"\"Setup LangChain CouchbaseSearchVectorStore with optional data loading.\"\"\"\n",
        "        try:\n",
        "            from langchain_couchbase.vectorstores import CouchbaseSearchVectorStore\n",
        "\n",
        "            if data_loader_func:\n",
        "                logger.info(\"\ud83d\udd04 Loading data into vector store...\")\n",
        "                data_loader_func(\n",
        "                    cluster=self.cluster,\n",
        "                    bucket_name=self.bucket_name,\n",
        "                    scope_name=scope_name,\n",
        "                    collection_name=collection_name,\n",
        "                    embeddings=embeddings,\n",
        "                    index_name=index_name,\n",
        "                    **loader_kwargs,\n",
        "                )\n",
        "                logger.info(\"\u2705 Data loaded into vector store successfully\")\n",
        "\n",
        "            vector_store = CouchbaseSearchVectorStore(\n",
        "                cluster=self.cluster,\n",
        "                bucket_name=self.bucket_name,\n",
        "                scope_name=scope_name,\n",
        "                collection_name=collection_name,\n",
        "                embedding=embeddings,\n",
        "                index_name=index_name,\n",
        "            )\n",
        "\n",
        "            logger.info(f\"\u2705 LangChain vector store setup complete\")\n",
        "            return vector_store\n",
        "\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"\u274c Error setting up LangChain vector store: {e!s}\")\n",
        "\n",
        "\n",
        "def create_couchbase_client(\n",
        "    conn_string: str = None,\n",
        "    username: str = None,\n",
        "    password: str = None,\n",
        "    bucket_name: str = None,\n",
        "    wan_profile: bool = True,\n",
        "    timeout_seconds: int = 20,\n",
        ") -> CouchbaseClient:\n",
        "    \"\"\"Factory function to create CouchbaseClient with environment variable defaults.\"\"\"\n",
        "    return CouchbaseClient(\n",
        "        conn_string=conn_string or os.getenv(\"CB_CONN_STRING\", \"couchbase://localhost\"),\n",
        "        username=username or os.getenv(\"CB_USERNAME\", \"Administrator\"),\n",
        "        password=password or os.getenv(\"CB_PASSWORD\", \"password\"),\n",
        "        bucket_name=bucket_name or os.getenv(\"CB_BUCKET\", \"travel-sample\"),\n",
        "        wan_profile=wan_profile,\n",
        "        timeout_seconds=timeout_seconds,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-F17qGvmTCu",
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Shared Flight Search Queries\n",
        "\n",
        "Exact code from queries.py - flight search queries and reference answers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "78O608HUmTCu"
      },
      "outputs": [],
      "source": [
        "# Flight search queries (for evaluation and testing)\n",
        "FLIGHT_SEARCH_QUERIES = [\n",
        "    \"Find flights from JFK to LAX\",\n",
        "    \"Book a flight from LAX to JFK for tomorrow, 2 passengers, business class\",\n",
        "    \"Book an economy flight from JFK to MIA for next week, 1 passenger\",\n",
        "    \"Show me my current flight bookings\",\n",
        "    \"What do passengers say about SpiceJet's service quality?\",\n",
        "]\n",
        "\n",
        "# Comprehensive reference answers based on actual system responses\n",
        "FLIGHT_REFERENCE_ANSWERS = [\n",
        "    # Query 1: Flight search JFK to LAX\n",
        "    \"\"\"Available flights from JFK to LAX:\n",
        "\n",
        "1. AS flight from JFK to LAX using 321 762\n",
        "2. B6 flight from JFK to LAX using 320\n",
        "3. DL flight from JFK to LAX using 76W 752\n",
        "4. QF flight from JFK to LAX using 744\n",
        "5. AA flight from JFK to LAX using 32B 762\n",
        "6. UA flight from JFK to LAX using 757\n",
        "7. US flight from JFK to LAX using 32B 762\n",
        "8. VX flight from JFK to LAX using 320\"\"\",\n",
        "\n",
        "    # Query 2: Flight booking LAX to JFK for tomorrow, 2 passengers, business class\n",
        "    # Note: Departure date and Booking ID are dynamically generated based on current date\n",
        "    \"\"\"Flight Booking Confirmed!\n",
        "\n",
        "Booking ID: [Dynamically Generated]\n",
        "Route: LAX \u2192 JFK\n",
        "Departure Date: [Tomorrow's Date - Dynamically Calculated]\n",
        "Passengers: 2\n",
        "Class: business\n",
        "Total Price: $1500.00\n",
        "\n",
        "Next Steps:\n",
        "1. Check-in opens 24 hours before departure\n",
        "2. Arrive at airport 2 hours early for domestic flights\n",
        "3. Bring valid government-issued photo ID\n",
        "\n",
        "Thank you for choosing our airline!\"\"\",\n",
        "\n",
        "    # Query 3: Flight booking JFK to MIA for next week\n",
        "    # Note: Departure date and Booking ID are dynamically generated based on current date\n",
        "    \"\"\"Flight Booking Confirmed!\n",
        "\n",
        "Booking ID: [Dynamically Generated]\n",
        "Route: JFK \u2192 MIA\n",
        "Departure Date: [Next Week's Date - Dynamically Calculated]\n",
        "Passengers: 1\n",
        "Class: economy\n",
        "Total Price: $250.00\n",
        "\n",
        "Next Steps:\n",
        "1. Check-in opens 24 hours before departure\n",
        "2. Arrive at airport 2 hours early for domestic flights\n",
        "3. Bring valid government-issued photo ID\n",
        "\n",
        "Thank you for choosing our airline!\"\"\",\n",
        "\n",
        "    # Query 4: Show current flight bookings\n",
        "    # Note: Booking IDs and dates are dynamically generated\n",
        "    \"\"\"Your Current Bookings (2 found):\n",
        "\n",
        "Booking 1:\n",
        "  Booking ID: [Dynamically Generated]\n",
        "  Route: LAX \u2192 JFK\n",
        "  Date: [Tomorrow's Date - Dynamically Calculated]\n",
        "  Passengers: 2\n",
        "  Class: business\n",
        "  Total: $1500.00\n",
        "  Status: Confirmed\n",
        "  Booked: [Current Date]\n",
        "\n",
        "Booking 2:\n",
        "  Booking ID: [Dynamically Generated]\n",
        "  Route: JFK \u2192 MIA\n",
        "  Date: [Next Week's Date - Dynamically Calculated]\n",
        "  Passengers: 1\n",
        "  Class: economy\n",
        "  Total: $250.00\n",
        "  Status: Confirmed\n",
        "  Booked: [Current Date]\"\"\",\n",
        "\n",
        "    # Query 5: SpiceJet service quality reviews\n",
        "    # Note: Vector search results are non-deterministic and may return different reviews each time\n",
        "    # from the database of 2210+ airline reviews. All results are valid SpiceJet reviews.\n",
        "    \"\"\"Found 5 relevant airline reviews for 'SpiceJet service quality':\n",
        "\n",
        "[Reviews include customer feedback about SpiceJet's service quality, covering aspects such as:\n",
        "- Crew service and hospitality\n",
        "- Flight delays and punctuality\n",
        "- In-flight amenities and comfort\n",
        "- Ground staff service\n",
        "- Overall travel experience\n",
        "- Ratings typically range from 2/10 to 10/10\n",
        "- Mix of verified and unverified reviews\n",
        "- Reviews from various dates and routes\n",
        "- Both positive recommendations and criticisms]\n",
        "\n",
        "Note: Specific reviews vary due to vector similarity search across 2210+ airline reviews.\"\"\",\n",
        "]\n",
        "\n",
        "# Create dictionary for backward compatibility\n",
        "QUERY_REFERENCE_ANSWERS = {\n",
        "    query: answer for query, answer in zip(FLIGHT_SEARCH_QUERIES, FLIGHT_REFERENCE_ANSWERS)\n",
        "}\n",
        "\n",
        "def get_test_queries():\n",
        "    \"\"\"Return test queries for evaluation.\"\"\"\n",
        "    return FLIGHT_SEARCH_QUERIES\n",
        "\n",
        "def get_evaluation_queries():\n",
        "    \"\"\"Get queries for evaluation\"\"\"\n",
        "    return FLIGHT_SEARCH_QUERIES\n",
        "\n",
        "def get_all_queries():\n",
        "    \"\"\"Get all available queries\"\"\"\n",
        "    return FLIGHT_SEARCH_QUERIES\n",
        "\n",
        "def get_simple_queries():\n",
        "    \"\"\"Get simple queries for basic testing\"\"\"\n",
        "    return FLIGHT_SEARCH_QUERIES\n",
        "\n",
        "def get_flight_policy_queries():\n",
        "    \"\"\"Return flight policy queries (for backward compatibility).\"\"\"\n",
        "    return FLIGHT_SEARCH_QUERIES\n",
        "\n",
        "def get_reference_answer(query: str) -> str:\n",
        "    \"\"\"Get the correct reference answer for a given query\"\"\"\n",
        "    return QUERY_REFERENCE_ANSWERS.get(query, f\"No reference answer available for: {query}\")\n",
        "\n",
        "def get_all_query_references():\n",
        "    \"\"\"Get all query-reference pairs\"\"\"\n",
        "    return QUERY_REFERENCE_ANSWERS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyAUlqTImTCv",
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Airline Reviews Data Module\n",
        "\n",
        "Exact code from airline_reviews_data.py - data loading and processing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzF7cnZTmTCv",
        "outputId": "af886d25-e4c2-4599-ff7b-c1a18c19a2cc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from langchain_couchbase.vectorstores import CouchbaseSearchVectorStore\n",
        "\n",
        "# Import kagglehub only when needed\n",
        "try:\n",
        "    import kagglehub\n",
        "except ImportError:\n",
        "    kagglehub = None\n",
        "\n",
        "class AirlineReviewsDataManager:\n",
        "    \"\"\"Manages airline reviews data loading, processing, and embedding.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self._raw_data_cache = None\n",
        "        self._processed_texts_cache = None\n",
        "\n",
        "    def load_raw_data(self):\n",
        "        \"\"\"Load raw airline reviews data from Kaggle dataset (with caching).\"\"\"\n",
        "        if self._raw_data_cache is not None:\n",
        "            return self._raw_data_cache\n",
        "\n",
        "        try:\n",
        "            if kagglehub is None:\n",
        "                raise ImportError(\"kagglehub is not available\")\n",
        "\n",
        "            logger.info(\"Downloading Indian Airlines Customer Reviews dataset from Kaggle...\")\n",
        "            path = kagglehub.dataset_download(\"jagathratchakan/indian-airlines-customer-reviews\")\n",
        "\n",
        "            csv_file = None\n",
        "            for file in os.listdir(path):\n",
        "                if file.endswith(\".csv\"):\n",
        "                    csv_file = os.path.join(path, file)\n",
        "                    break\n",
        "\n",
        "            if not csv_file:\n",
        "                msg = \"No CSV file found in downloaded dataset\"\n",
        "                raise FileNotFoundError(msg)\n",
        "\n",
        "            logger.info(f\"Loading reviews from {csv_file}\")\n",
        "            df = pd.read_csv(csv_file)\n",
        "\n",
        "            self._raw_data_cache = df.to_dict(\"records\")\n",
        "            logger.info(f\"Loaded {len(self._raw_data_cache)} airline reviews from Kaggle dataset\")\n",
        "            return self._raw_data_cache\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.exception(f\"Error loading airline reviews from Kaggle: {e!s}\")\n",
        "            raise\n",
        "\n",
        "    def process_to_texts(self):\n",
        "        \"\"\"Process raw data into formatted text strings for embedding (with caching).\"\"\"\n",
        "        if self._processed_texts_cache is not None:\n",
        "            return self._processed_texts_cache\n",
        "\n",
        "        reviews = self.load_raw_data()\n",
        "        review_texts = []\n",
        "\n",
        "        for review in reviews:\n",
        "            text_parts = []\n",
        "\n",
        "            if review.get(\"AirLine_Name\"):\n",
        "                text_parts.append(f\"Airline: {review['AirLine_Name']}\")\n",
        "\n",
        "            if review.get(\"Title\"):\n",
        "                text_parts.append(f\"Title: {review['Title']}\")\n",
        "\n",
        "            if review.get(\"Review\"):\n",
        "                text_parts.append(f\"Review: {review['Review']}\")\n",
        "\n",
        "            if review.get(\"Rating - 10\"):\n",
        "                text_parts.append(f\"Rating: {review['Rating - 10']}/10\")\n",
        "\n",
        "            if review.get(\"Name\"):\n",
        "                text_parts.append(f\"Reviewer: {review['Name']}\")\n",
        "\n",
        "            if review.get(\"Date\"):\n",
        "                text_parts.append(f\"Date: {review['Date']}\")\n",
        "\n",
        "            if review.get(\"Recommond\"):\n",
        "                text_parts.append(f\"Recommended: {review['Recommond']}\")\n",
        "\n",
        "            text = \". \".join(text_parts)\n",
        "            review_texts.append(text)\n",
        "\n",
        "        self._processed_texts_cache = review_texts\n",
        "        logger.info(f\"Processed {len(review_texts)} airline reviews into text format\")\n",
        "        return review_texts\n",
        "\n",
        "    def load_to_vector_store(\n",
        "        self,\n",
        "        cluster,\n",
        "        bucket_name: str,\n",
        "        scope_name: str,\n",
        "        collection_name: str,\n",
        "        embeddings,\n",
        "        index_name: str,\n",
        "    ):\n",
        "        \"\"\"Load airline reviews into Couchbase vector store with embeddings.\"\"\"\n",
        "        try:\n",
        "            count_query = (\n",
        "                f\"SELECT COUNT(*) as count FROM `{bucket_name}`.`{scope_name}`.`{collection_name}`\"\n",
        "            )\n",
        "            count_result = cluster.query(count_query)\n",
        "            count_row = next(iter(count_result))\n",
        "            existing_count = count_row[\"count\"]\n",
        "\n",
        "            if existing_count > 0:\n",
        "                logger.info(\n",
        "                    f\"Found {existing_count} existing documents in collection, skipping data load\"\n",
        "                )\n",
        "                return\n",
        "\n",
        "            review_texts = self.process_to_texts()\n",
        "\n",
        "            vector_store = CouchbaseSearchVectorStore(\n",
        "                cluster=cluster,\n",
        "                bucket_name=bucket_name,\n",
        "                scope_name=scope_name,\n",
        "                collection_name=collection_name,\n",
        "                embedding=embeddings,\n",
        "                index_name=index_name,\n",
        "            )\n",
        "\n",
        "            logger.info(\n",
        "                f\"Loading {len(review_texts)} airline review embeddings to {bucket_name}.{scope_name}.{collection_name}\"\n",
        "            )\n",
        "\n",
        "            batch_size = 10  # Conservative batch size for stability\n",
        "            total_batches = (len(review_texts) + batch_size - 1) // batch_size\n",
        "\n",
        "            with tqdm(\n",
        "                total=len(review_texts), desc=\"Loading airline reviews\", unit=\"reviews\"\n",
        "            ) as pbar:\n",
        "                for i in range(0, len(review_texts), batch_size):\n",
        "                    batch_num = i // batch_size + 1\n",
        "                    batch = review_texts[i : i + batch_size]\n",
        "\n",
        "                    vector_store.add_texts(texts=batch, batch_size=len(batch))\n",
        "\n",
        "                    pbar.update(len(batch))\n",
        "                    pbar.set_postfix(batch=f\"{batch_num}/{total_batches}\")\n",
        "\n",
        "            logger.info(\n",
        "                f\"Successfully loaded {len(review_texts)} airline review embeddings to vector store\"\n",
        "            )\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.exception(f\"Error loading airline reviews to Couchbase: {e!s}\")\n",
        "            raise\n",
        "\n",
        "\n",
        "# Global instance for reuse\n",
        "_data_manager = AirlineReviewsDataManager()\n",
        "\n",
        "\n",
        "def get_airline_review_texts():\n",
        "    \"\"\"Get processed airline review texts (uses global cached instance).\"\"\"\n",
        "    return _data_manager.process_to_texts()\n",
        "\n",
        "\n",
        "def load_airline_reviews_from_kaggle():\n",
        "    \"\"\"Load raw airline reviews data from Kaggle (uses global cached instance).\"\"\"\n",
        "    return _data_manager.load_raw_data()\n",
        "\n",
        "\n",
        "def load_airline_reviews_to_couchbase(\n",
        "    cluster, bucket_name: str, scope_name: str, collection_name: str, embeddings, index_name: str\n",
        "):\n",
        "    \"\"\"Load airline reviews into Couchbase vector store (uses global cached instance).\"\"\"\n",
        "    return _data_manager.load_to_vector_store(\n",
        "        cluster, bucket_name, scope_name, collection_name, embeddings, index_name\n",
        "    )\n",
        "\n",
        "\n",
        "def load_airline_reviews():\n",
        "    \"\"\"Simple function to load airline reviews - called by main.py.\"\"\"\n",
        "    try:\n",
        "        # Just return the processed texts for embedding\n",
        "        # This eliminates the need for separate cluster connection here\n",
        "        logger.info(\"Loading airline reviews data...\")\n",
        "        reviews = _data_manager.process_to_texts()\n",
        "        logger.info(f\"Successfully loaded {len(reviews)} airline reviews\")\n",
        "        return reviews\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.exception(f\"Error in load_airline_reviews: {e!s}\")\n",
        "        raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70ajWVAemTCv",
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Agent Classes and Core Implementation\n",
        "\n",
        "Core agent classes and implementation from working main.py script.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2tjFmkimTCv",
        "outputId": "b3cbaa39-57b0-40f3-8736-c3cfe55c699a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:\u2705 Environment variables configured\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All setup functions and client implementations are now available inline.\n"
          ]
        }
      ],
      "source": [
        "# Agent classes are implemented using the inline AI services and Couchbase client above\n",
        "print(\"All setup functions and client implementations are now available inline.\")\n",
        "def setup_ai_services(\n",
        "    framework: str = \"langchain\",\n",
        "    temperature: float = 0.0,\n",
        "    callbacks: Optional[List] = None,\n",
        "    application_span: Optional[Any] = None\n",
        ") -> Tuple[Any, Any]:\n",
        "    \"\"\"Priority 1 AI service setup - Capella with direct API keys + OpenAI wrappers only.\"\"\"\n",
        "    embeddings = None\n",
        "    llm = None\n",
        "\n",
        "    logger.info(f\"\ud83d\udd27 Setting up Priority 1 AI services for {framework} framework...\")\n",
        "\n",
        "    # PRIORITY 1: LATEST CAPELLA (OpenAI wrappers with direct API keys)\n",
        "    if (\n",
        "        not embeddings\n",
        "        and os.getenv(\"CAPELLA_API_ENDPOINT\")\n",
        "        and os.getenv(\"CAPELLA_API_EMBEDDINGS_KEY\")\n",
        "    ):\n",
        "        try:\n",
        "            from langchain_openai import OpenAIEmbeddings\n",
        "            endpoint = os.getenv(\"CAPELLA_API_ENDPOINT\")\n",
        "            api_key = os.getenv(\"CAPELLA_API_EMBEDDINGS_KEY\")\n",
        "            model = os.getenv(\"CAPELLA_API_EMBEDDING_MODEL\")\n",
        "\n",
        "            # Handle endpoint that may or may not already have /v1 suffix\n",
        "            if endpoint.endswith('/v1'):\n",
        "                base_url = endpoint\n",
        "            else:\n",
        "                base_url = f\"{endpoint}/v1\"\n",
        "\n",
        "            # Debug logging - same pattern as working test\n",
        "            logger.info(f\"\ud83d\udd27 Endpoint: {endpoint}\")\n",
        "            logger.info(f\"\ud83d\udd27 Model: {model}\")\n",
        "            logger.info(f\"\ud83d\udd27 Base URL: {base_url}\")\n",
        "\n",
        "            embeddings = OpenAIEmbeddings(\n",
        "                model=model,\n",
        "                api_key=api_key,\n",
        "                base_url=base_url,\n",
        "                check_embedding_ctx_length=False,  # KEY FIX for asymmetric models in LangChain/LangGraph\n",
        "            )\n",
        "            logger.info(\"\u2705 Using Priority 1: Capella AI embeddings (OpenAI wrapper)\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"\u274c Priority 1 Capella AI embeddings failed: {type(e).__name__}: {e}\")\n",
        "\n",
        "    if not llm and os.getenv(\"CAPELLA_API_ENDPOINT\") and os.getenv(\"CAPELLA_API_LLM_KEY\"):\n",
        "        try:\n",
        "            from langchain_openai import ChatOpenAI\n",
        "\n",
        "            endpoint = os.getenv(\"CAPELLA_API_ENDPOINT\")\n",
        "            llm_key = os.getenv(\"CAPELLA_API_LLM_KEY\")\n",
        "            llm_model = os.getenv(\"CAPELLA_API_LLM_MODEL\")\n",
        "\n",
        "            # Handle endpoint that may or may not already have /v1 suffix\n",
        "            if endpoint.endswith('/v1'):\n",
        "                base_url = endpoint\n",
        "            else:\n",
        "                base_url = f\"{endpoint}/v1\"\n",
        "\n",
        "            # Debug logging\n",
        "            logger.info(f\"\ud83d\udd27 LLM Endpoint: {endpoint}\")\n",
        "            logger.info(f\"\ud83d\udd27 LLM Model: {llm_model}\")\n",
        "            logger.info(f\"\ud83d\udd27 LLM Base URL: {base_url}\")\n",
        "\n",
        "            llm = ChatOpenAI(\n",
        "                api_key=llm_key,\n",
        "                base_url=base_url,\n",
        "                model=llm_model,\n",
        "                temperature=temperature,\n",
        "            )\n",
        "            # Test the LLM works\n",
        "            from langchain_core.messages import HumanMessage\n",
        "            test_response = llm.invoke([HumanMessage(content=\"Hello\")])\n",
        "            logger.info(\"\u2705 Using Priority 1: Capella AI LLM (OpenAI wrapper)\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"\u274c Priority 1 Capella AI LLM failed: {type(e).__name__}: {e}\")\n",
        "            llm = None\n",
        "\n",
        "    # Fallback: OpenAI\n",
        "    if not embeddings and os.getenv(\"OPENAI_API_KEY\"):\n",
        "        try:\n",
        "            from langchain_openai import OpenAIEmbeddings\n",
        "            embeddings = OpenAIEmbeddings(\n",
        "                model=\"text-embedding-3-small\",\n",
        "                api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
        "            )\n",
        "            logger.info(\"\u2705 Using OpenAI embeddings fallback\")\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"\u26a0\ufe0f OpenAI embeddings failed: {e}\")\n",
        "\n",
        "    if not llm and os.getenv(\"OPENAI_API_KEY\"):\n",
        "        try:\n",
        "            from langchain_openai import ChatOpenAI\n",
        "            llm = ChatOpenAI(\n",
        "                api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
        "                model=\"gpt-4o\",\n",
        "                temperature=temperature,\n",
        "            )\n",
        "            logger.info(\"\u2705 Using OpenAI LLM fallback\")\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"\u26a0\ufe0f OpenAI LLM failed: {e}\")\n",
        "\n",
        "    if not embeddings:\n",
        "        raise ValueError(\"\u274c No embeddings service could be initialized\")\n",
        "    if not llm:\n",
        "        raise ValueError(\"\u274c No LLM service could be initialized\")\n",
        "\n",
        "    logger.info(f\"\u2705 Priority 1 AI services setup completed for {framework}\")\n",
        "    return embeddings, llm\n",
        "\n",
        "\n",
        "# Setup environment\n",
        "setup_environment()\n",
        "\n",
        "# Test Capella AI connectivity if configured\n",
        "if os.getenv(\"CAPELLA_API_ENDPOINT\"):\n",
        "    if not test_capella_connectivity():\n",
        "        logger.warning(\"\u274c Capella AI connectivity test failed. Will use fallback models.\")\n",
        "else:\n",
        "    logger.info(\"\u2139\ufe0f Capella API not configured - will use fallback models\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBnkb9fUmTCv",
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## CouchbaseClient Class\n",
        "\n",
        "Define the CouchbaseClient for all database operations inline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "1ZpmJfgVmTCv"
      },
      "outputs": [],
      "source": [
        "class CouchbaseClient:\n",
        "    \"\"\"Centralized Couchbase client for all database operations.\"\"\"\n",
        "\n",
        "    def __init__(self, conn_string: str, username: str, password: str, bucket_name: str):\n",
        "        self.conn_string = conn_string\n",
        "        self.username = username\n",
        "        self.password = password\n",
        "        self.bucket_name = bucket_name\n",
        "        self.cluster = None\n",
        "        self.bucket = None\n",
        "        self._collections = {}\n",
        "\n",
        "    def connect(self):\n",
        "        try:\n",
        "            auth = PasswordAuthenticator(self.username, self.password)\n",
        "            options = ClusterOptions(auth)\n",
        "            options.apply_profile(\"wan_development\")\n",
        "            self.cluster = Cluster(self.conn_string, options)\n",
        "            self.cluster.wait_until_ready(timedelta(seconds=15))\n",
        "            logger.info(\"\u2705 Successfully connected to Couchbase\")\n",
        "            return self.cluster\n",
        "        except Exception as e:\n",
        "            raise ConnectionError(f\"\u274c Failed to connect to Couchbase: {e!s}\")\n",
        "\n",
        "    def setup_collection(self, scope_name: str, collection_name: str, clear_existing_data: bool = False):\n",
        "        try:\n",
        "            if not self.cluster:\n",
        "                self.connect()\n",
        "\n",
        "            if not self.bucket:\n",
        "                try:\n",
        "                    self.bucket = self.cluster.bucket(self.bucket_name)\n",
        "                except Exception:\n",
        "                    logger.info(f\"Creating bucket '{self.bucket_name}'...\")\n",
        "                    bucket_settings = CreateBucketSettings(\n",
        "                        name=self.bucket_name, bucket_type=BucketType.COUCHBASE,\n",
        "                        ram_quota_mb=1024, flush_enabled=True, num_replicas=0\n",
        "                    )\n",
        "                    self.cluster.buckets().create_bucket(bucket_settings)\n",
        "                    time.sleep(5)\n",
        "                    self.bucket = self.cluster.bucket(self.bucket_name)\n",
        "\n",
        "            bucket_manager = self.bucket.collections()\n",
        "            scopes = bucket_manager.get_all_scopes()\n",
        "            scope_exists = any(scope.name == scope_name for scope in scopes)\n",
        "\n",
        "            if not scope_exists and scope_name != \"_default\":\n",
        "                bucket_manager.create_scope(scope_name)\n",
        "\n",
        "            collections = bucket_manager.get_all_scopes()\n",
        "            collection_exists = any(\n",
        "                scope.name == scope_name and collection_name in [col.name for col in scope.collections]\n",
        "                for scope in collections\n",
        "            )\n",
        "\n",
        "            if collection_exists and clear_existing_data:\n",
        "                self.clear_collection_data(scope_name, collection_name)\n",
        "            elif not collection_exists:\n",
        "                bucket_manager.create_collection(scope_name, collection_name)\n",
        "\n",
        "            time.sleep(3)\n",
        "            try:\n",
        "                self.cluster.query(\n",
        "                    f\"CREATE PRIMARY INDEX IF NOT EXISTS ON `{self.bucket_name}`.`{scope_name}`.`{collection_name}`\"\n",
        "                ).execute()\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "            collection = self.bucket.scope(scope_name).collection(collection_name)\n",
        "            self._collections[f\"{scope_name}.{collection_name}\"] = collection\n",
        "            logger.info(f\"\u2705 Collection setup complete: {scope_name}.{collection_name}\")\n",
        "            return collection\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"\u274c Error setting up collection: {e!s}\")\n",
        "\n",
        "    def clear_collection_data(self, scope_name: str, collection_name: str):\n",
        "        try:\n",
        "            logger.info(f\"\ud83d\uddd1\ufe0f Clearing data from {self.bucket_name}.{scope_name}.{collection_name}...\")\n",
        "            delete_query = f\"DELETE FROM `{self.bucket_name}`.`{scope_name}`.`{collection_name}`\"\n",
        "            result = self.cluster.query(delete_query)\n",
        "            list(result)\n",
        "            time.sleep(2)\n",
        "        except KeyspaceNotFoundException:\n",
        "            logger.info(f\"\u2139\ufe0f Collection {self.bucket_name}.{scope_name}.{collection_name} doesn't exist, nothing to clear\")\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"\u26a0\ufe0f Error clearing collection data: {e}\")\n",
        "\n",
        "    def clear_scope(self, scope_name: str):\n",
        "        try:\n",
        "            if not self.bucket:\n",
        "                if not self.cluster:\n",
        "                    self.connect()\n",
        "                self.bucket = self.cluster.bucket(self.bucket_name)\n",
        "\n",
        "            bucket_manager = self.bucket.collections()\n",
        "            scopes = bucket_manager.get_all_scopes()\n",
        "            target_scope = next((s for s in scopes if s.name == scope_name), None)\n",
        "\n",
        "            if target_scope:\n",
        "                for collection in target_scope.collections:\n",
        "                    try:\n",
        "                        self.clear_collection_data(scope_name, collection.name)\n",
        "                    except Exception:\n",
        "                        pass\n",
        "                logger.info(f\"\u2705 Completed clearing scope: {self.bucket_name}.{scope_name}\")\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"\u274c Could not clear scope: {e}\")\n",
        "\n",
        "    def setup_vector_search_index(self, index_definition: dict, scope_name: str):\n",
        "        try:\n",
        "            scope_index_manager = self.bucket.scope(scope_name).search_indexes()\n",
        "            existing_indexes = scope_index_manager.get_all_indexes()\n",
        "            index_name = index_definition[\"name\"]\n",
        "\n",
        "            if index_name not in [index.name for index in existing_indexes]:\n",
        "                search_index = SearchIndex.from_json(index_definition)\n",
        "                scope_index_manager.upsert_index(search_index)\n",
        "                logger.info(f\"\u2705 Vector search index '{index_name}' created\")\n",
        "            else:\n",
        "                logger.info(f\"\u2139\ufe0f Vector search index '{index_name}' already exists\")\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"\u26a0\ufe0f Error setting up vector search index: {e}\")\n",
        "\n",
        "    def setup_vector_store_langchain(self, scope_name: str, collection_name: str, index_name: str, embeddings, data_loader_func=None, **loader_kwargs):\n",
        "        try:\n",
        "            if data_loader_func:\n",
        "                logger.info(\"\ud83d\udd04 Loading data into vector store...\")\n",
        "                data_loader_func(\n",
        "                    cluster=self.cluster, bucket_name=self.bucket_name,\n",
        "                    scope_name=scope_name, collection_name=collection_name,\n",
        "                    embeddings=embeddings, index_name=index_name, **loader_kwargs\n",
        "                )\n",
        "\n",
        "            vector_store = CouchbaseSearchVectorStore(\n",
        "                cluster=self.cluster, bucket_name=self.bucket_name,\n",
        "                scope_name=scope_name, collection_name=collection_name,\n",
        "                embedding=embeddings, index_name=index_name\n",
        "            )\n",
        "            logger.info(f\"\u2705 Vector store setup complete: {self.bucket_name}.{scope_name}.{collection_name}\")\n",
        "            return vector_store\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"\u274c Error setting up vector store: {e!s}\")\n",
        "\n",
        "\n",
        "def create_couchbase_client():\n",
        "    \"\"\"Factory function to create CouchbaseClient with environment defaults.\"\"\"\n",
        "    return CouchbaseClient(\n",
        "        conn_string=os.getenv(\"CB_CONN_STRING\", \"couchbase://localhost\"),\n",
        "        username=os.getenv(\"CB_USERNAME\", \"Administrator\"),\n",
        "        password=os.getenv(\"CB_PASSWORD\", \"password\"),\n",
        "        bucket_name=os.getenv(\"CB_BUCKET\", \"travel-sample\"),\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPIVbrffmTCw"
      },
      "source": [
        "## Flight Search Agent Classes\n",
        "\n",
        "This cell contains the classes for the flight search agent.\n",
        "\n",
        "### FlightSearchGraph\n",
        "\n",
        "The `FlightSearchGraph` class is the main class for the flight search agent. It is a subclass of `langgraph.graph.StateGraph` and is used to define the graph of the flight search agent.\n",
        "\n",
        "### FlightSearchGraphState\n",
        "\n",
        "The `FlightSearchGraphState` class is the state of the flight search agent. It is a subclass of `langgraph.graph.State` and is used to define the state of the flight search agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "gH84eu2ImTCw"
      },
      "outputs": [],
      "source": [
        "\n",
        "## Agent Classes\n",
        "\n",
        "class FlightSearchState(agentc_langgraph.agent.State):\n",
        "    \"\"\"State for flight search conversations - single user system.\"\"\"\n",
        "\n",
        "    query: str\n",
        "    resolved: bool\n",
        "    search_results: list[dict]\n",
        "\n",
        "\n",
        "class FlightSearchAgent(agentc_langgraph.agent.ReActAgent):\n",
        "    \"\"\"Flight search agent using Agent Catalog tools and ReActAgent framework.\"\"\"\n",
        "\n",
        "    def __init__(self, catalog: agentc.Catalog, span: agentc.Span, chat_model=None):\n",
        "        \"\"\"Initialize the flight search agent.\"\"\"\n",
        "\n",
        "        if chat_model is None:\n",
        "            # Fallback to OpenAI if no chat model provided\n",
        "            model_name = os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\")\n",
        "            chat_model = langchain_openai.chat_models.ChatOpenAI(model=model_name, temperature=0.1)\n",
        "\n",
        "        super().__init__(\n",
        "            chat_model=chat_model, catalog=catalog, span=span, prompt_name=\"flight_search_assistant\"\n",
        "        )\n",
        "\n",
        "    def _invoke(\n",
        "        self,\n",
        "        span: agentc.Span,\n",
        "        state: FlightSearchState,\n",
        "        config: langchain_core.runnables.RunnableConfig,\n",
        "    ) -> FlightSearchState:\n",
        "        \"\"\"Handle flight search conversation using ReActAgent.\"\"\"\n",
        "\n",
        "        # Initialize conversation if this is the first message\n",
        "        if not state[\"messages\"]:\n",
        "            initial_msg = langchain_core.messages.HumanMessage(content=state[\"query\"])\n",
        "            state[\"messages\"].append(initial_msg)\n",
        "            logger.info(f\"Flight Query: {state['query']}\")\n",
        "\n",
        "        # Get prompt resource first - we'll need it for the ReAct agent\n",
        "        prompt_resource = self.catalog.find(\"prompt\", name=\"flight_search_assistant\")\n",
        "\n",
        "        # Get tools from Agent Catalog with simplified discovery\n",
        "        tools = []\n",
        "        tool_names = [\n",
        "            \"lookup_flight_info\",\n",
        "            \"save_flight_booking\",\n",
        "            \"retrieve_flight_bookings\",\n",
        "            \"search_airline_reviews\",\n",
        "        ]\n",
        "\n",
        "        for tool_name in tool_names:\n",
        "            try:\n",
        "                # Find tool using Agent Catalog\n",
        "                catalog_tool = self.catalog.find(\"tool\", name=tool_name)\n",
        "                if catalog_tool:\n",
        "                    logger.info(f\"\u2705 Found tool: {tool_name}\")\n",
        "                else:\n",
        "                    logger.error(f\"\u274c Tool not found: {tool_name}\")\n",
        "                    continue\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"\u274c Failed to find tool {tool_name}: {e}\")\n",
        "                continue\n",
        "\n",
        "            # JSON-first architecture with Pydantic validation\n",
        "            # Import validation schemas\n",
        "            import json\n",
        "            from pydantic import ValidationError\n",
        "            from tools.schemas import FlightSearchInput, BookingInput\n",
        "\n",
        "            def clean_react_artifacts(raw_input: str) -> str:\n",
        "                \"\"\"Remove ReAct format artifacts that contaminate tool inputs.\"\"\"\n",
        "                if not raw_input:\n",
        "                    return \"\"\n",
        "\n",
        "                cleaned = raw_input.strip()\n",
        "\n",
        "                # Remove ReAct artifacts - order matters, check longer patterns first\n",
        "                artifacts_to_remove = [\n",
        "                    '\\nObservation:', '\\nObservation', 'Observation:', 'Observation',\n",
        "                    '\\nThought:', 'Thought:',\n",
        "                    '\\nAction:', 'Action:',\n",
        "                    '\\nAction Input:', 'Action Input:',\n",
        "                    '\\nFinal Answer:', 'Final Answer:',\n",
        "                    'Observ'  # Handle incomplete artifact\n",
        "                ]\n",
        "\n",
        "                for artifact in artifacts_to_remove:\n",
        "                    if artifact in cleaned:\n",
        "                        # Split and take only the part before the artifact\n",
        "                        cleaned = cleaned.split(artifact)[0].strip()\n",
        "\n",
        "                # Clean up quotes and extra whitespace\n",
        "                cleaned = cleaned.strip().strip(\"\\\"'\").strip()\n",
        "\n",
        "                # Normalize whitespace\n",
        "                cleaned = \" \".join(cleaned.split())\n",
        "\n",
        "                return cleaned\n",
        "\n",
        "            def parse_tool_input(tool_name: str, tool_input: str) -> dict:\n",
        "                \"\"\"Parse tool input: JSON with Pydantic validation for structured tools, plain text for simple tools.\"\"\"\n",
        "                # Clean ReAct artifacts first\n",
        "                cleaned = clean_react_artifacts(tool_input)\n",
        "\n",
        "                # Structured tools MUST use JSON\n",
        "                if tool_name == \"lookup_flight_info\":\n",
        "                    data = json.loads(cleaned)  # Will raise JSONDecodeError if invalid\n",
        "                    validated = FlightSearchInput(**data)  # Will raise ValidationError if invalid\n",
        "                    logger.info(f\"\u2705 Parsed {tool_name}: {validated.source_airport} \u2192 {validated.destination_airport}\")\n",
        "                    return {\n",
        "                        \"source_airport\": validated.source_airport,\n",
        "                        \"destination_airport\": validated.destination_airport\n",
        "                    }\n",
        "\n",
        "                elif tool_name == \"save_flight_booking\":\n",
        "                    data = json.loads(cleaned)  # Will raise JSONDecodeError if invalid\n",
        "                    validated = BookingInput(**data)  # Will raise ValidationError if invalid\n",
        "                    logger.info(f\"\u2705 Parsed {tool_name}: {validated.source_airport}\u2192{validated.destination_airport}, {validated.passengers} pax, {validated.flight_class}\")\n",
        "                    return {\n",
        "                        \"source_airport\": validated.source_airport,\n",
        "                        \"destination_airport\": validated.destination_airport,\n",
        "                        \"departure_date\": validated.departure_date,\n",
        "                        \"passengers\": validated.passengers,\n",
        "                        \"flight_class\": validated.flight_class\n",
        "                    }\n",
        "\n",
        "                # Simple tools use plain text\n",
        "                elif tool_name == \"retrieve_flight_bookings\":\n",
        "                    return {\"booking_query\": cleaned}\n",
        "\n",
        "                elif tool_name == \"search_airline_reviews\":\n",
        "                    return {\"query\": cleaned}\n",
        "\n",
        "                raise ValueError(f\"Unknown tool: {tool_name}\")\n",
        "\n",
        "            # Create clean wrapper function for this tool\n",
        "            def create_tool_func(catalog_tool_ref, tool_name_ref):\n",
        "                \"\"\"Create a wrapper that parses JSON and calls catalog tool with structured params.\"\"\"\n",
        "                def tool_func(tool_input: str) -> str:\n",
        "                    try:\n",
        "                        # Parse input with Pydantic validation (JSON for structured tools)\n",
        "                        params = parse_tool_input(tool_name_ref, tool_input)\n",
        "\n",
        "                        # Call the Agent Catalog tool with parsed parameters\n",
        "                        result = catalog_tool_ref.func(**params)\n",
        "\n",
        "                        return str(result) if result is not None else \"No results found\"\n",
        "\n",
        "                    except Exception as e:\n",
        "                        logger.error(f\"\u274c Error in tool {tool_name_ref}: {e}\")\n",
        "                        import traceback\n",
        "                        logger.debug(traceback.format_exc())\n",
        "                        return f\"Error: {str(e)}\"\n",
        "                return tool_func\n",
        "\n",
        "            # Tool descriptions for the LLM (JSON required for structured tools)\n",
        "            tool_descriptions = {\n",
        "                \"lookup_flight_info\": \"Find flights between airports. REQUIRES JSON: {\\\"source_airport\\\": \\\"JFK\\\", \\\"destination_airport\\\": \\\"LAX\\\"}\",\n",
        "                \"save_flight_booking\": \"Book a flight. REQUIRES JSON: {\\\"source_airport\\\": \\\"LAX\\\", \\\"destination_airport\\\": \\\"JFK\\\", \\\"departure_date\\\": \\\"2025-12-25\\\", \\\"passengers\\\": 2, \\\"flight_class\\\": \\\"business\\\"}\",\n",
        "                \"retrieve_flight_bookings\": \"View all flight bookings or search by criteria. Leave input empty for all bookings.\",\n",
        "                \"search_airline_reviews\": \"Search airline customer reviews. Input: plain text query (e.g., 'SpiceJet service quality')\"\n",
        "            }\n",
        "\n",
        "            langchain_tool = Tool(\n",
        "                name=tool_name,\n",
        "                description=tool_descriptions.get(tool_name, f\"Tool for {tool_name.replace('_', ' ')}\"),\n",
        "                func=create_tool_func(catalog_tool, tool_name),\n",
        "            )\n",
        "            tools.append(langchain_tool)\n",
        "\n",
        "        # Use the Agent Catalog prompt content directly - get first result if it's a list\n",
        "        if isinstance(prompt_resource, list):\n",
        "            prompt_resource = prompt_resource[0]\n",
        "\n",
        "        # Safely get the content from the prompt resource\n",
        "        prompt_content = getattr(prompt_resource, \"content\", \"\")\n",
        "        if not prompt_content:\n",
        "            prompt_content = \"You are a helpful flight search assistant. Use the available tools to help users with their flight queries.\"\n",
        "\n",
        "        # Inject current date into the prompt content\n",
        "        import datetime\n",
        "\n",
        "        current_date = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
        "        prompt_content = prompt_content.replace(\"{current_date}\", current_date)\n",
        "\n",
        "        # Use the Agent Catalog prompt content directly - it already has ReAct format\n",
        "        react_prompt = PromptTemplate.from_template(str(prompt_content))\n",
        "\n",
        "        # Create ReAct agent with tools and prompt\n",
        "        agent = create_react_agent(self.chat_model, tools, react_prompt)\n",
        "\n",
        "        # Custom parsing error handler - force stopping on parsing errors\n",
        "        def handle_parsing_errors(error):\n",
        "            \"\"\"Custom handler for parsing errors - force early termination.\"\"\"\n",
        "            error_msg = str(error)\n",
        "            if \"both a final answer and a parse-able action\" in error_msg:\n",
        "                # Force early termination - return a reasonable response\n",
        "                return \"Final Answer: I encountered a parsing error. Please reformulate your request.\"\n",
        "            elif \"Missing 'Action:'\" in error_msg:\n",
        "                return \"I need to use the correct format with Action: and Action Input:\"\n",
        "            else:\n",
        "                return f\"Final Answer: I encountered an error processing your request. Please try again.\"\n",
        "\n",
        "        # Create agent executor - very strict: only 2 iterations max\n",
        "        agent_executor = AgentExecutor(\n",
        "            agent=agent,\n",
        "            tools=tools,\n",
        "            verbose=True,\n",
        "            handle_parsing_errors=handle_parsing_errors,\n",
        "            max_iterations=2,  # STRICT: 1 tool call + 1 Final Answer only\n",
        "            early_stopping_method=\"force\",  # Force stop\n",
        "            return_intermediate_steps=True,\n",
        "        )\n",
        "\n",
        "        # Execute the agent\n",
        "        response = agent_executor.invoke({\"input\": state[\"query\"]})\n",
        "\n",
        "        # Extract tool outputs from intermediate_steps and store in search_results\n",
        "        if \"intermediate_steps\" in response and response[\"intermediate_steps\"]:\n",
        "            tool_outputs = []\n",
        "            for step in response[\"intermediate_steps\"]:\n",
        "                if isinstance(step, tuple) and len(step) >= 2:\n",
        "                    # step[0] is the action, step[1] is the tool output/observation\n",
        "                    tool_output = str(step[1])\n",
        "                    if tool_output and tool_output.strip():\n",
        "                        tool_outputs.append(tool_output)\n",
        "            state[\"search_results\"] = tool_outputs\n",
        "\n",
        "        # Add response to conversation\n",
        "        assistant_msg = langchain_core.messages.AIMessage(content=response[\"output\"])\n",
        "        state[\"messages\"].append(assistant_msg)\n",
        "        state[\"resolved\"] = True\n",
        "\n",
        "        return state\n",
        "\n",
        "\n",
        "class FlightSearchGraph(agentc_langgraph.graph.GraphRunnable):\n",
        "    \"\"\"Flight search conversation graph using Agent Catalog.\"\"\"\n",
        "\n",
        "    def __init__(self, catalog, span, chat_model=None):\n",
        "        \"\"\"Initialize the flight search graph with optional chat model.\"\"\"\n",
        "        super().__init__(catalog=catalog, span=span)\n",
        "        self.chat_model = chat_model\n",
        "\n",
        "    @staticmethod\n",
        "    def build_starting_state(query: str) -> FlightSearchState:\n",
        "        \"\"\"Build the initial state for the flight search - single user system.\"\"\"\n",
        "        return FlightSearchState(\n",
        "            messages=[],\n",
        "            query=query,\n",
        "            resolved=False,\n",
        "            search_results=[],\n",
        "        )\n",
        "\n",
        "    def compile(self):\n",
        "        \"\"\"Compile the LangGraph workflow.\"\"\"\n",
        "\n",
        "        # Build the flight search agent with catalog integration\n",
        "        search_agent = FlightSearchAgent(\n",
        "            catalog=self.catalog, span=self.span, chat_model=self.chat_model\n",
        "        )\n",
        "\n",
        "        # Create a wrapper function for the ReActAgent\n",
        "        def flight_search_node(state: FlightSearchState) -> FlightSearchState:\n",
        "            \"\"\"Wrapper function for the flight search ReActAgent.\"\"\"\n",
        "            return search_agent._invoke(\n",
        "                span=self.span,\n",
        "                state=state,\n",
        "                config={},  # Empty config for now\n",
        "            )\n",
        "\n",
        "        # Create a simple workflow graph for flight search\n",
        "        workflow = langgraph.graph.StateGraph(FlightSearchState)\n",
        "\n",
        "        # Add the flight search agent node using the wrapper function\n",
        "        workflow.add_node(\"flight_search\", flight_search_node)\n",
        "\n",
        "        # Set entry point and simple flow\n",
        "        workflow.set_entry_point(\"flight_search\")\n",
        "        workflow.add_edge(\"flight_search\", langgraph.graph.END)\n",
        "\n",
        "        return workflow.compile()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaohuackmTCw",
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Data Loading Components\n",
        "\n",
        "Complete data loading and query definition components embedded inline for standalone operation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Akv3vV84mTCw",
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Clear Existing Data\n",
        "\n",
        "Clear existing bookings and reviews for clean test run.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UBSREadbmTCw",
        "outputId": "2bddc56a-8916-48b5-ce02-6e13252387a1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:\u2705 Successfully connected to Couchbase\n",
            "INFO:__main__:\ud83d\uddd1\ufe0f Clearing data from travel-sample.agentc_bookings.user_bookings_20251023...\n",
            "INFO:__main__:\ud83d\uddd1\ufe0f Clearing data from travel-sample.agentc_bookings.user_bookings_20251022...\n",
            "INFO:__main__:\u2705 Completed clearing scope: travel-sample.agentc_bookings\n",
            "INFO:__main__:\u2705 Cleared existing flight bookings for fresh test run: travel-sample.agentc_bookings\n",
            "INFO:__main__:Downloading Indian Airlines Customer Reviews dataset from Kaggle...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using Colab cache for faster access to the 'indian-airlines-customer-reviews' dataset.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:Loading reviews from /kaggle/input/indian-airlines-customer-reviews/Indian_Domestic_Airline.csv\n",
            "INFO:__main__:Loaded 2210 airline reviews from Kaggle dataset\n",
            "INFO:__main__:Processed 2210 airline reviews into text format\n",
            "INFO:__main__:\ud83d\udcca Airline reviews collection: 2210 existing, 2210 expected\n",
            "INFO:__main__:\u2705 Collection already has correct document count (2210), skipping clear\n"
          ]
        }
      ],
      "source": [
        "def clear_bookings_and_reviews():\n",
        "    \"\"\"Clear existing flight bookings to start fresh for demo.\"\"\"\n",
        "    try:\n",
        "        client = create_couchbase_client()\n",
        "        client.connect()\n",
        "\n",
        "        # Clear bookings scope using environment variables\n",
        "        bookings_scope = \"agentc_bookings\"\n",
        "        client.clear_scope(bookings_scope)\n",
        "        logger.info(\n",
        "            f\"\u2705 Cleared existing flight bookings for fresh test run: {os.environ['CB_BUCKET']}.{bookings_scope}\"\n",
        "        )\n",
        "\n",
        "        # Check if airline reviews collection needs clearing by comparing expected vs actual document count\n",
        "        try:\n",
        "            # Get expected document count (this uses cached data if available)\n",
        "            expected_docs = _data_manager.process_to_texts()\n",
        "            expected_count = len(expected_docs)\n",
        "\n",
        "            # Check current document count in collection\n",
        "            try:\n",
        "                count_query = f\"SELECT COUNT(*) as count FROM `{os.environ['CB_BUCKET']}`.`{os.environ['CB_SCOPE']}`.`{os.environ['CB_COLLECTION']}`\"\n",
        "                count_result = client.cluster.query(count_query)\n",
        "                count_row = next(iter(count_result))\n",
        "                existing_count = count_row[\"count\"]\n",
        "\n",
        "                logger.info(\n",
        "                    f\"\ud83d\udcca Airline reviews collection: {existing_count} existing, {expected_count} expected\"\n",
        "                )\n",
        "\n",
        "                if existing_count == expected_count:\n",
        "                    logger.info(\n",
        "                        f\"\u2705 Collection already has correct document count ({existing_count}), skipping clear\"\n",
        "                    )\n",
        "                else:\n",
        "                    logger.info(\n",
        "                        f\"\ud83d\uddd1\ufe0f  Clearing airline reviews collection: {os.environ['CB_BUCKET']}.{os.environ['CB_SCOPE']}.{os.environ['CB_COLLECTION']}\"\n",
        "                    )\n",
        "                    client.clear_collection_data(os.environ[\"CB_SCOPE\"], os.environ[\"CB_COLLECTION\"])\n",
        "                    logger.info(\n",
        "                        f\"\u2705 Cleared existing airline reviews for fresh data load: {os.environ['CB_BUCKET']}.{os.environ['CB_SCOPE']}.{os.environ['CB_COLLECTION']}\"\n",
        "                    )\n",
        "\n",
        "            except KeyspaceNotFoundException:\n",
        "                # Collection doesn't exist yet - this is expected for fresh setup\n",
        "                logger.info(\n",
        "                    f\"\ud83d\udcca Collection doesn't exist yet, will create and load fresh data\"\n",
        "                )\n",
        "            except Exception as count_error:\n",
        "                # Other query errors - clear anyway to ensure fresh start\n",
        "                logger.info(\n",
        "                    f\"\ud83d\udcca Collection query failed, will clear and reload: {count_error}\"\n",
        "                )\n",
        "                client.clear_collection_data(os.environ[\"CB_SCOPE\"], os.environ[\"CB_COLLECTION\"])\n",
        "                logger.info(\n",
        "                    f\"\u2705 Cleared existing airline reviews for fresh data load: {os.environ['CB_BUCKET']}.{os.environ['CB_SCOPE']}.{os.environ['CB_COLLECTION']}\"\n",
        "                )\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"\u26a0\ufe0f  Could not check collection count, clearing anyway: {e}\")\n",
        "            client.clear_collection_data(os.environ[\"CB_SCOPE\"], os.environ[\"CB_COLLECTION\"])\n",
        "            logger.info(\n",
        "                f\"\u2705 Cleared existing airline reviews for fresh data load: {os.environ['CB_BUCKET']}.{os.environ['CB_SCOPE']}.{os.environ['CB_COLLECTION']}\"\n",
        "            )\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"\u274c Could not clear bookings: {e}\")\n",
        "\n",
        "\n",
        "# Clear existing data for fresh test run\n",
        "clear_bookings_and_reviews()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZzJmJm4mTCx",
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Setup Flight Search Agent\n",
        "\n",
        "Initialize the complete flight search agent setup using the refactored approach.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7pFbwTkmTCx",
        "outputId": "7f25c229-de4b-4233-c2a1-e3cef52a3348"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:\u2705 Environment variables configured\n",
            "INFO:__main__:\u2705 Successfully connected to Couchbase\n",
            "INFO:__main__:\u2705 Collection setup complete: agentc_data.airline_reviews\n",
            "INFO:__main__:Loaded vector search index definition from agentcatalog_index.json\n",
            "INFO:__main__:\u2139\ufe0f Vector search index 'airline_reviews_index' already exists\n",
            "INFO:__main__:\ud83d\udd27 Setting up Priority 1 AI services for langgraph framework...\n",
            "INFO:__main__:\ud83d\udd27 Endpoint: https://agd6zdjymyanhi9g.ai.sandbox.nonprod-project-avengers.com\n",
            "INFO:__main__:\ud83d\udd27 Model: nvidia/llama-3.2-nv-embedqa-1b-v2\n",
            "INFO:__main__:\ud83d\udd27 Base URL: https://agd6zdjymyanhi9g.ai.sandbox.nonprod-project-avengers.com/v1\n",
            "INFO:__main__:\u2705 Using Priority 1: Capella AI embeddings (OpenAI wrapper)\n",
            "INFO:__main__:\ud83d\udd27 LLM Endpoint: https://agd6zdjymyanhi9g.ai.sandbox.nonprod-project-avengers.com\n",
            "INFO:__main__:\ud83d\udd27 LLM Model: meta/llama3-8b-instruct\n",
            "INFO:__main__:\ud83d\udd27 LLM Base URL: https://agd6zdjymyanhi9g.ai.sandbox.nonprod-project-avengers.com/v1\n",
            "INFO:__main__:\u2705 Using Priority 1: Capella AI LLM (OpenAI wrapper)\n",
            "INFO:__main__:\u2705 Priority 1 AI services setup completed for langgraph\n",
            "INFO:__main__:\ud83d\udd04 Loading data into vector store...\n",
            "INFO:__main__:Found 2210 existing documents in collection, skipping data load\n",
            "INFO:__main__:\u2705 Vector store setup complete: travel-sample.agentc_data.airline_reviews\n",
            "INFO:__main__:\ud83d\udd27 Setting up Priority 1 AI services for langgraph framework...\n",
            "INFO:__main__:\ud83d\udd27 Endpoint: https://agd6zdjymyanhi9g.ai.sandbox.nonprod-project-avengers.com\n",
            "INFO:__main__:\ud83d\udd27 Model: nvidia/llama-3.2-nv-embedqa-1b-v2\n",
            "INFO:__main__:\ud83d\udd27 Base URL: https://agd6zdjymyanhi9g.ai.sandbox.nonprod-project-avengers.com/v1\n",
            "INFO:__main__:\u2705 Using Priority 1: Capella AI embeddings (OpenAI wrapper)\n",
            "INFO:__main__:\ud83d\udd27 LLM Endpoint: https://agd6zdjymyanhi9g.ai.sandbox.nonprod-project-avengers.com\n",
            "INFO:__main__:\ud83d\udd27 LLM Model: meta/llama3-8b-instruct\n",
            "INFO:__main__:\ud83d\udd27 LLM Base URL: https://agd6zdjymyanhi9g.ai.sandbox.nonprod-project-avengers.com/v1\n",
            "INFO:__main__:\u2705 Using Priority 1: Capella AI LLM (OpenAI wrapper)\n",
            "INFO:__main__:\u2705 Priority 1 AI services setup completed for langgraph\n",
            "INFO:__main__:Agent Catalog integration successful\n"
          ]
        }
      ],
      "source": [
        "def setup_flight_search_agent():\n",
        "    \"\"\"Common setup function for flight search agent - returns all necessary components.\"\"\"\n",
        "    try:\n",
        "        # Setup environment first\n",
        "        setup_environment()\n",
        "\n",
        "        # Initialize Agent Catalog (uses .env or defaults if AGENT_CATALOG_* vars not set)\n",
        "        catalog = agentc.Catalog()\n",
        "        application_span = catalog.Span(name=\"Flight Search Agent\", blacklist=set())\n",
        "\n",
        "        # Test Capella AI connectivity\n",
        "        if os.getenv(\"CAPELLA_API_ENDPOINT\"):\n",
        "            if not test_capella_connectivity():\n",
        "                logger.warning(\"\u274c Capella AI connectivity test failed. Will use fallback models.\")\n",
        "        else:\n",
        "            logger.info(\"\u2139\ufe0f Capella API not configured - will use fallback models\")\n",
        "\n",
        "        # Create CouchbaseClient for all operations\n",
        "        client = create_couchbase_client()\n",
        "\n",
        "        # Setup everything in one call - bucket, scope, collection\n",
        "        client.setup_collection(\n",
        "            scope_name=os.environ[\"CB_SCOPE\"],\n",
        "            collection_name=os.environ[\"CB_COLLECTION\"],\n",
        "            clear_existing_data=False,  # Let data loader decide based on count check\n",
        "        )\n",
        "\n",
        "        # Setup vector search index\n",
        "        try:\n",
        "            with open(\"agentcatalog_index.json\") as file:\n",
        "                index_definition = json.load(file)\n",
        "            logger.info(\"Loaded vector search index definition from agentcatalog_index.json\")\n",
        "            client.setup_vector_search_index(index_definition, os.environ[\"CB_SCOPE\"])\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Error loading index definition: {e!s}\")\n",
        "            logger.info(\"Continuing without vector search index...\")\n",
        "\n",
        "\n",
        "        # Setup AI services using Priority 1: Capella AI + OpenAI wrappers\n",
        "        embeddings, _ = setup_ai_services(framework=\"langgraph\")\n",
        "\n",
        "        # Setup vector store with airline reviews data\n",
        "        vector_store = client.setup_vector_store_langchain(\n",
        "            scope_name=os.environ[\"CB_SCOPE\"],\n",
        "            collection_name=os.environ[\"CB_COLLECTION\"],\n",
        "            index_name=os.environ[\"CB_INDEX\"],\n",
        "            embeddings=embeddings,\n",
        "            data_loader_func=load_airline_reviews_to_couchbase,\n",
        "        )\n",
        "\n",
        "        # Setup LLM using Priority 1: Capella AI + OpenAI wrappers\n",
        "        _, chat_model = setup_ai_services(framework=\"langgraph\", temperature=0.1)\n",
        "\n",
        "        # Create the flight search graph with the chat model\n",
        "        flight_graph = FlightSearchGraph(\n",
        "            catalog=catalog, span=application_span, chat_model=chat_model\n",
        "        )\n",
        "        # Compile the graph\n",
        "        compiled_graph = flight_graph.compile()\n",
        "\n",
        "        logger.info(\"Agent Catalog integration successful\")\n",
        "\n",
        "        return compiled_graph, application_span\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.exception(f\"Setup error: {e}\")\n",
        "        logger.info(\"Ensure Agent Catalog is published: agentc index . && agentc publish\")\n",
        "        raise\n",
        "\n",
        "\n",
        "def run_test_query(test_number: int, query: str, compiled_graph, application_span):\n",
        "    \"\"\"Run a single test query with error handling.\"\"\"\n",
        "    logger.info(f\"\\n\ud83d\udd0d Test {test_number}: {query}\")\n",
        "    try:\n",
        "        state = FlightSearchGraph.build_starting_state(query=query)\n",
        "        result = compiled_graph.invoke(state)\n",
        "\n",
        "        if result.get(\"search_results\"):\n",
        "            logger.info(f\"Found {len(result['search_results'])} flight options\")\n",
        "        logger.info(f\"\u2705 Test {test_number} completed: {result.get('resolved', False)}\")\n",
        "\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.exception(f\"\u274c Test {test_number} failed: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# Setup the agent\n",
        "compiled_graph, application_span = setup_flight_search_agent()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puhVz-pvmTCx",
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Test 1: Flight Search\n",
        "\n",
        "Find flights from JFK to LAX for tomorrow.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8Ba2EgVmTCx",
        "outputId": "db0f64f1-d9eb-443f-bd49-786b2651850f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:\n",
            "\ud83d\udd0d Test 1: Find flights from JFK to LAX for tomorrow\n",
            "INFO:__main__:Flight Query: Find flights from JFK to LAX for tomorrow\n",
            "INFO:__main__:\u2705 Found tool: lookup_flight_info\n",
            "INFO:__main__:\u2705 Found tool: save_flight_booking\n",
            "INFO:__main__:\u2705 Found tool: retrieve_flight_bookings\n",
            "INFO:__main__:\u2705 Found tool: search_airline_reviews\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:\u2705 Parsed lookup_flight_info: JFK \u2192 LAX\n",
            "INFO:lookup_flight_info:\ud83d\udd0d Looking up flights: JFK \u2192 LAX\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32;1m\u001b[1;3mThought: I need to use lookup_flight_info to help with this request Action: lookup_flight_info Action Input: {\"source_airport\": \"JFK\", \"destination_airport\": \"LAX\"}\u001b[0m\u001b[36;1m\u001b[1;3mAvailable flights from JFK to LAX:\n",
            "\n",
            "1. AS flight from JFK to LAX using 321 762\n",
            "2. B6 flight from JFK to LAX using 320\n",
            "3. DL flight from JFK to LAX using 76W 752\n",
            "4. QF flight from JFK to LAX using 744\n",
            "5. AA flight from JFK to LAX using 32B 762\n",
            "6. UA flight from JFK to LAX using 757\n",
            "7. US flight from JFK to LAX using 32B 762\n",
            "8. VX flight from JFK to LAX using 320\u001b[0m"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:Found 1 flight options\n",
            "INFO:__main__:\u2705 Test 1 completed: True\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32;1m\u001b[1;3mThought: I now know the final answer\n",
            "Final Answer: I found 8 available flights from JFK to LAX, including airlines AS, B6, DL, QF, AA, UA, US, and VX with various aircraft types.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "result1 = run_test_query(\n",
        "    1, \"Find flights from JFK to LAX for tomorrow\", compiled_graph, application_span\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YubHWNhvmTCx",
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Test 2: Flight Booking (Business Class)\n",
        "\n",
        "Book a flight with business class.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CbKwZgFSmTCx",
        "outputId": "41ef95b4-7751-4639-c770-8151b194490c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:\n",
            "\ud83d\udd0d Test 2: Book a flight from LAX to JFK for tomorrow, 2 passengers, business class\n",
            "INFO:__main__:Flight Query: Book a flight from LAX to JFK for tomorrow, 2 passengers, business class\n",
            "INFO:__main__:\u2705 Found tool: lookup_flight_info\n",
            "INFO:__main__:\u2705 Found tool: save_flight_booking\n",
            "INFO:__main__:\u2705 Found tool: retrieve_flight_bookings\n",
            "INFO:__main__:\u2705 Found tool: search_airline_reviews\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:\u2705 Parsed save_flight_booking: LAX\u2192JFK, 2 pax, business\n",
            "INFO:save_flight_booking:\ud83d\uddd3\ufe0f Date validated: 2025-10-24\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32;1m\u001b[1;3mThought: I need to use save_flight_booking to help with this request. Tomorrow is 2025-10-23 + 1 day = 2025-10-24 Action: save_flight_booking Action Input: {\"source_airport\": \"LAX\", \"destination_airport\": \"JFK\", \"departure_date\": \"2025-10-24\", \"passengers\": 2, \"flight_class\": \"business\"}\u001b[0m"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:save_flight_booking:\ud83c\udfaf Booking: LAX\u2192JFK on 2025-10-24, 2 pax, business class\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33;1m\u001b[1;3mFlight Booking Confirmed!\n",
            "\n",
            "Booking ID: FL1024BDE6EE8C\n",
            "Route: LAX \u2192 JFK\n",
            "Departure Date: 2025-10-24\n",
            "Passengers: 2\n",
            "Class: business\n",
            "Total Price: $1500.00\n",
            "\n",
            "Next Steps:\n",
            "1. Check-in opens 24 hours before departure\n",
            "2. Arrive at airport 2 hours early for domestic flights\n",
            "3. Bring valid government-issued photo ID\n",
            "\n",
            "Thank you for choosing our airline!\u001b[0m"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:Found 1 flight options\n",
            "INFO:__main__:\u2705 Test 2 completed: True\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32;1m\u001b[1;3mThought: I now know the final answer Final Answer: Your flight from LAX to JFK for 2 passengers in business class has been booked successfully for tomorrow (2025-10-24). Your booking ID is FL1024BDE6EE8C and the total cost is $1500.00.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "result2 = run_test_query(\n",
        "    2,\n",
        "    \"Book a flight from LAX to JFK for tomorrow, 2 passengers, business class\",\n",
        "    compiled_graph,\n",
        "    application_span,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDBEUzYImTCx",
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Test 3: Flight Booking (Economy Class)\n",
        "\n",
        "Book an economy flight.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "crBUQX-FmTC7",
        "outputId": "f296cf43-d69b-47cf-923d-d6c87490ba34"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:\n",
            "\ud83d\udd0d Test 3: Book an economy flight from JFK to MIA for next week, 1 passenger\n",
            "INFO:__main__:Flight Query: Book an economy flight from JFK to MIA for next week, 1 passenger\n",
            "INFO:__main__:\u2705 Found tool: lookup_flight_info\n",
            "INFO:__main__:\u2705 Found tool: save_flight_booking\n",
            "INFO:__main__:\u2705 Found tool: retrieve_flight_bookings\n",
            "INFO:__main__:\u2705 Found tool: search_airline_reviews\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:\u2705 Parsed save_flight_booking: JFK\u2192MIA, 1 pax, economy\n",
            "INFO:save_flight_booking:\ud83d\uddd3\ufe0f Date validated: 2025-10-30\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32;1m\u001b[1;3mThought: I need to use save_flight_booking to help with this request. Next week is 2025-10-23 + 7 days = 2025-10-30 Action: save_flight_booking Action Input: {\"source_airport\": \"JFK\", \"destination_airport\": \"MIA\", \"departure_date\": \"2025-10-30\", \"passengers\": 1, \"flight_class\": \"economy\"}\u001b[0m"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:save_flight_booking:\ud83c\udfaf Booking: JFK\u2192MIA on 2025-10-30, 1 pax, economy class\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33;1m\u001b[1;3mFlight Booking Confirmed!\n",
            "\n",
            "Booking ID: FL10306C68E47C\n",
            "Route: JFK \u2192 MIA\n",
            "Departure Date: 2025-10-30\n",
            "Passengers: 1\n",
            "Class: economy\n",
            "Total Price: $250.00\n",
            "\n",
            "Next Steps:\n",
            "1. Check-in opens 24 hours before departure\n",
            "2. Arrive at airport 2 hours early for domestic flights\n",
            "3. Bring valid government-issued photo ID\n",
            "\n",
            "Thank you for choosing our airline!\u001b[0m"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:Found 1 flight options\n",
            "INFO:__main__:\u2705 Test 3 completed: True\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32;1m\u001b[1;3mThought: I now know the final answer\n",
            "Final Answer: Your economy flight from JFK to MIA for 1 passenger has been booked successfully for next week (2025-10-30). Your booking ID is FL10306C68E47C and the total cost is $250.00.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "result3 = run_test_query(\n",
        "    3,\n",
        "    \"Book an economy flight from JFK to MIA for next week, 1 passenger\",\n",
        "    compiled_graph,\n",
        "    application_span,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9M4J-KDmTC7",
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Test 4: Retrieve Current Bookings\n",
        "\n",
        "Show current flight bookings.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTa0vaxjmTC8",
        "outputId": "e643e24c-0f99-419e-f4ea-622080059120"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:\n",
            "\ud83d\udd0d Test 4: Show me my current flight bookings\n",
            "INFO:__main__:Flight Query: Show me my current flight bookings\n",
            "INFO:__main__:\u2705 Found tool: lookup_flight_info\n",
            "INFO:__main__:\u2705 Found tool: save_flight_booking\n",
            "INFO:__main__:\u2705 Found tool: retrieve_flight_bookings\n",
            "INFO:__main__:\u2705 Found tool: search_airline_reviews\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: I need to use retrieve_flight_bookings to help with this request Action: retrieve_flight_bookings Action Input:\u001b[0m\u001b[38;5;200m\u001b[1;3mYour Current Bookings (2 found):\n",
            "\n",
            "Booking 1:\n",
            "  Booking ID: FL10306C68E47C\n",
            "  Route: JFK \u2192 MIA\n",
            "  Date: 2025-10-30\n",
            "  Passengers: 1\n",
            "  Class: economy\n",
            "  Total: $250.00\n",
            "  Status: confirmed\n",
            "  Booked: 2025-10-23\n",
            "\n",
            "Booking 2:\n",
            "  Booking ID: FL1024BDE6EE8C\n",
            "  Route: LAX \u2192 JFK\n",
            "  Date: 2025-10-24\n",
            "  Passengers: 2\n",
            "  Class: business\n",
            "  Total: $1500.00\n",
            "  Status: confirmed\n",
            "  Booked: 2025-10-23\u001b[0m"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:Found 1 flight options\n",
            "INFO:__main__:\u2705 Test 4 completed: True\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32;1m\u001b[1;3mThought: I now know the final answer\n",
            "Final Answer: You have 2 current flight bookings: (1) JFK to MIA on 2025-10-30 for 1 passenger in economy class ($250.00), and (2) LAX to JFK on 2025-10-24 for 2 passengers in business class ($1500.00).\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "result4 = run_test_query(4, \"Show me my current flight bookings\", compiled_graph, application_span)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e5zJao2mTC8",
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Test 5: Airline Reviews Search\n",
        "\n",
        "Search airline reviews for service quality.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8PJhI7e7mTC8",
        "outputId": "95c725c3-7cb3-4460-ecf0-4f6e3c50c4a0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:\n",
            "\ud83d\udd0d Test 5: What do passengers say about SpiceJet's service quality?\n",
            "INFO:__main__:Flight Query: What do passengers say about SpiceJet's service quality?\n",
            "INFO:__main__:\u2705 Found tool: lookup_flight_info\n",
            "INFO:__main__:\u2705 Found tool: save_flight_booking\n",
            "INFO:__main__:\u2705 Found tool: retrieve_flight_bookings\n",
            "INFO:__main__:\u2705 Found tool: search_airline_reviews\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: I need to use search_airline_reviews to help with this request Action: search_airline_reviews Action Input: SpiceJet service quality\u001b[0m"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:search_airline_reviews:Searching for airline reviews with query: 'SpiceJet service quality'\n",
            "INFO:search_airline_reviews:Found 5 results for query: 'SpiceJet service quality'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36;1m\u001b[1;3mFound 5 relevant airline reviews for 'SpiceJet service quality':\n",
            "\n",
            "Review 1:\n",
            "Airline: SpiceJet. Title: \"Great travel experience\". Review: \u2705 Trip Verified |  Marvelous courteous crew who took good care of all passengers. They should be rewarded for the patience shown towards the unruly ones. Great travel experience.. Rating: 10.0/10. Reviewer: Ranjita Pandey. Date: 18th April 2024. Recommended: yes\n",
            "\n",
            "Review 2:\n",
            "Airline: SpiceJet. Title: \"good service by the crew\". Review: \u2705 Trip Verified | I have had good service by the crew. It was amazing, the crew was very enthusiastic and warm welcome. It was one of the best services in my experience.. Rating: 10.0/10. Reviewer: K Mansour. Date: 10th August 2024. Recommended: yes\n",
            "\n",
            "Review 3:\n",
            "Airline: SpiceJet. Title: \"Excellent service\". Review: \u2705 Trip Verified |  Excellent service by the ground staff courteous beyond expectations always willing to help in the real sense and not lipservice i will recommend to all whom I know. Rating: 10.0/10. Reviewer: Ramanathan Ramchandra. Date: 1st November 2023. Recommended: yes\n",
            "\n",
            "Review 4:\n",
            "Airline: SpiceJet. Title: \"excellent hospitality\". Review: \u2705 Trip Verified |  It's very grateful to SpiceJet for the service in flight. Everything is fine with excellent hospitality. Very politely behavior and warmly welcome from your cabin crew and all members.. Rating: 10.0/10. Reviewer: A Kajan. Date: 10th August 2024. Recommended: yes\n",
            "\n",
            "Review 5:\n",
            "Airline: SpiceJet. Title: \"delayed both ways by many hours\". Review: Not Verified |  Flight was delayed both ways by many hours. Poor service for the same price as other airlines like IndiGo. No wifi or other amenities to compensate for terrible service.. Rating: 2.0/10. Reviewer: Somil Jain Jain. Date: 20th May 2022. Recommended: no\u001b[0m"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:Found 1 flight options\n",
            "INFO:__main__:\u2705 Test 5 completed: True\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32;1m\u001b[1;3mThought: I now know the final answer Final Answer: Based on 5 customer reviews, SpiceJet receives mostly positive feedback (averaging 8.4/10) for their courteous crew, warm hospitality, and excellent ground staff service. However, some passengers report flight delays and poor service. The majority of reviews are highly positive (four 10/10 ratings), with one negative review citing delays and lack of amenities.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "result5 = run_test_query(\n",
        "    5, \"What do passengers say about SpiceJet's service quality?\", compiled_graph, application_span\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWQzh0MrmTC8",
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Arize Phoenix Evaluation\n",
        "\n",
        "This section demonstrates how to evaluate the flight search agent using Arize Phoenix observability platform. The evaluation includes:\n",
        "\n",
        "- **Relevance Scoring**: Using Phoenix RelevanceEvaluator to score how relevant responses are to queries\n",
        "- **QA Scoring**: Using Phoenix QAEvaluator with lenient evaluation templates for better accuracy\n",
        "- **Hallucination Detection**: Using Phoenix HallucinationEvaluator with lenient templates to detect fabricated information\n",
        "- **Toxicity Detection**: Using Phoenix ToxicityEvaluator to detect harmful content\n",
        "- **Phoenix UI**: Real-time observability dashboard\n",
        "\n",
        "We'll run evaluation queries and assess the responses for quality and safety using the latest evaluation approach.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4WANBV3mTC8",
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Evaluation Configuration\n",
        "\n",
        "Configuration class for the evaluation system with all parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VtGx7JSvmTC8",
        "outputId": "0fa0432a-b97b-46fe-ae7d-8d64e19005d6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:phoenix.config:\ud83d\udccb Ensuring phoenix working directory: /root/.phoenix\n",
            "INFO:phoenix.inferences.inferences:Dataset: phoenix_inferences_f904ec33-2873-40e1-ae8f-40e7e45643e5 initialized\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import nest_asyncio\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Optional, Tuple, Any\n",
        "\n",
        "# Apply the patch to allow nested asyncio event loops\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Try to import Arize dependencies with fallback\n",
        "try:\n",
        "    import phoenix as px\n",
        "    from arize.experimental.datasets import ArizeDatasetsClient\n",
        "    from arize.experimental.datasets.utils.constants import GENERATIVE\n",
        "    from openinference.instrumentation.langchain import LangChainInstrumentor\n",
        "    from openinference.instrumentation.openai import OpenAIInstrumentor\n",
        "    from phoenix.evals import (\n",
        "        RAG_RELEVANCY_PROMPT_RAILS_MAP,\n",
        "        RAG_RELEVANCY_PROMPT_TEMPLATE,\n",
        "        TOXICITY_PROMPT_RAILS_MAP,\n",
        "        TOXICITY_PROMPT_TEMPLATE,\n",
        "        HallucinationEvaluator,\n",
        "        OpenAIModel,\n",
        "        QAEvaluator,\n",
        "        RelevanceEvaluator,\n",
        "        ToxicityEvaluator,\n",
        "        llm_classify,\n",
        "    )\n",
        "    from phoenix.otel import register\n",
        "\n",
        "    ARIZE_AVAILABLE = True\n",
        "except ImportError as e:\n",
        "    logger.warning(f\"Arize dependencies not available: {e}\")\n",
        "    logger.warning(\"Running in local evaluation mode only...\")\n",
        "    ARIZE_AVAILABLE = False\n",
        "\n",
        "# Define lenient evaluation templates for flight search agent evaluation\n",
        "LENIENT_QA_PROMPT_TEMPLATE = \"\"\"\n",
        "You are evaluating whether an AI agent's response correctly addresses a user's question.\n",
        "\n",
        "FOCUS ON FUNCTIONAL SUCCESS, NOT EXACT MATCHING:\n",
        "1. Did the agent provide the requested information (flights, bookings, reviews)?\n",
        "2. Is the core information accurate and helpful to the user?\n",
        "3. Would the user be satisfied with what they received?\n",
        "\n",
        "DYNAMIC DATA IS EXPECTED AND CORRECT:\n",
        "- Booking IDs will be DIFFERENT each time (dynamically generated - this is correct!)\n",
        "- Dates like \"tomorrow\" are calculated dynamically (may differ from reference)\n",
        "- Booking lists reflect ACTUAL session bookings (may differ from reference)\n",
        "- Route sequences depend on actual booking order in this session\n",
        "\n",
        "IGNORE THESE DIFFERENCES:\n",
        "- Different booking IDs, dates, or sequences (these are dynamic!)\n",
        "- Format differences, duplicate calls, system messages\n",
        "- Reference mismatches due to dynamic data\n",
        "\n",
        "MARK AS CORRECT IF:\n",
        "- Agent successfully completed the action (found flights, made booking, retrieved bookings, got reviews)\n",
        "- User received useful, accurate information\n",
        "- Core functionality worked as expected\n",
        "\n",
        "Question: {input}\n",
        "Reference Answer: {reference}\n",
        "Agent Response: {output}\n",
        "\n",
        "Did the agent successfully provide what the user requested, regardless of exact reference matching?\n",
        "Respond with just \"correct\" or \"incorrect\".\n",
        "\"\"\"\n",
        "\n",
        "LENIENT_HALLUCINATION_PROMPT_TEMPLATE = \"\"\"\n",
        "You are checking if an AI agent's response contains hallucinated information.\n",
        "\n",
        "DYNAMIC DATA IS EXPECTED AND FACTUAL:\n",
        "- Booking IDs are dynamically generated (will ALWAYS be different from reference - this is correct!)\n",
        "- Dates are calculated dynamically (\"tomorrow\", \"next week\" based on current date)\n",
        "- Booking sequences reflect actual session bookings (not static reference data)\n",
        "- Tool outputs contain real system data\n",
        "\n",
        "MARK AS FACTUAL IF:\n",
        "- Response contains \"iteration limit\" or \"time limit\" (system issue, not hallucination)\n",
        "- Dynamic data differs from reference (booking IDs, dates, booking sequences)\n",
        "- Agent provides plausible flight data, booking confirmations, or reviews\n",
        "- Information is consistent with system capabilities\n",
        "\n",
        "ONLY MARK AS HALLUCINATED IF:\n",
        "- Response contains clearly impossible information (fake airlines, impossible routes)\n",
        "- Agent makes up data it cannot access\n",
        "- Response contradicts fundamental system facts\n",
        "\n",
        "REMEMBER: Different booking IDs, dates, and sequences are EXPECTED dynamic behavior!\n",
        "\n",
        "Question: {input}\n",
        "Reference Text: {reference}\n",
        "Agent Response: {output}\n",
        "\n",
        "Does the response contain clearly false information, ignoring expected dynamic data differences?\n",
        "Respond with just \"factual\" or \"hallucinated\".\n",
        "\"\"\"\n",
        "\n",
        "# Custom Rails for lenient evaluation\n",
        "LENIENT_QA_RAILS = [\"correct\", \"incorrect\"]\n",
        "LENIENT_HALLUCINATION_RAILS = [\"factual\", \"hallucinated\"]\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class EvaluationConfig:\n",
        "    \"\"\"Configuration for the evaluation system.\"\"\"\n",
        "\n",
        "    # Arize Configuration\n",
        "    arize_space_id: str = os.getenv(\"ARIZE_SPACE_ID\", \"your-space-id\")\n",
        "    arize_api_key: str = os.getenv(\"ARIZE_API_KEY\", \"your-api-key\")\n",
        "    project_name: str = \"flight-search-agent-evaluation\"\n",
        "\n",
        "    # Phoenix Configuration\n",
        "    phoenix_base_port: int = 6006\n",
        "    phoenix_grpc_base_port: int = 4317\n",
        "    phoenix_max_port_attempts: int = 5\n",
        "    phoenix_startup_timeout: int = 30\n",
        "\n",
        "    # Evaluation Configuration\n",
        "    evaluator_model: str = \"gpt-4o\"\n",
        "    batch_size: int = 10\n",
        "    max_retries: int = 3\n",
        "    evaluation_timeout: int = 300\n",
        "\n",
        "    # Logging Configuration\n",
        "    log_level: str = \"INFO\"\n",
        "    detailed_logging: bool = True\n",
        "\n",
        "    # Dataset Configuration\n",
        "    dataset_name: str = \"flight-search-evaluation-queries\"\n",
        "    dataset_description: str = \"Flight search agent evaluation queries with expected responses\"\n",
        "\n",
        "    # Output Configuration\n",
        "    save_results: bool = True\n",
        "    results_format: str = \"csv\"  # csv, json, both\n",
        "    output_dir: str = \"output\"\n",
        "\n",
        "    def __post_init__(self):\n",
        "        \"\"\"Validate configuration after initialization.\"\"\"\n",
        "        if self.phoenix_base_port < 1024 or self.phoenix_base_port > 65535:\n",
        "            raise ValueError(\"Phoenix base port must be between 1024 and 65535\")\n",
        "\n",
        "        if self.batch_size < 1 or self.batch_size > 100:\n",
        "            raise ValueError(\"Batch size must be between 1 and 100\")\n",
        "\n",
        "        if self.evaluation_timeout < 30:\n",
        "            raise ValueError(\"Evaluation timeout must be at least 30 seconds\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zB9KfoEBmTC9",
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Phoenix Manager\n",
        "\n",
        "Manages Phoenix server lifecycle and port management.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "W-eWQOiSmTC9"
      },
      "outputs": [],
      "source": [
        "import socket\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "class PhoenixManager:\n",
        "    \"\"\"Manages Phoenix server lifecycle and port management.\"\"\"\n",
        "\n",
        "    def __init__(self, config: EvaluationConfig):\n",
        "        self.config = config\n",
        "        self.session = None\n",
        "        self.active_port = None\n",
        "        self.tracer_provider = None\n",
        "\n",
        "    def _is_port_in_use(self, port: int) -> bool:\n",
        "        \"\"\"Check if a port is in use.\"\"\"\n",
        "        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
        "            return s.connect_ex((\"localhost\", port)) == 0\n",
        "\n",
        "    def _kill_existing_phoenix_processes(self) -> None:\n",
        "        \"\"\"Kill any existing Phoenix processes.\"\"\"\n",
        "        try:\n",
        "            subprocess.run([\"pkill\", \"-f\", \"phoenix\"], check=False, capture_output=True)\n",
        "            time.sleep(2)  # Wait for processes to terminate\n",
        "        except Exception as e:\n",
        "            logger.debug(f\"Error killing Phoenix processes: {e}\")\n",
        "\n",
        "    def _find_available_port(self) -> Tuple[int, int]:\n",
        "        \"\"\"Find available ports for Phoenix.\"\"\"\n",
        "        phoenix_port = self.config.phoenix_base_port\n",
        "        grpc_port = self.config.phoenix_grpc_base_port\n",
        "\n",
        "        for _ in range(self.config.phoenix_max_port_attempts):\n",
        "            if not self._is_port_in_use(phoenix_port):\n",
        "                return phoenix_port, grpc_port\n",
        "            phoenix_port += 1\n",
        "            grpc_port += 1\n",
        "\n",
        "        raise RuntimeError(\n",
        "            f\"Could not find available ports after {self.config.phoenix_max_port_attempts} attempts\"\n",
        "        )\n",
        "\n",
        "    def start_phoenix(self) -> bool:\n",
        "        \"\"\"Start Phoenix server and return success status.\"\"\"\n",
        "        if not ARIZE_AVAILABLE:\n",
        "            logger.warning(\"\u26a0\ufe0f Phoenix dependencies not available\")\n",
        "            return False\n",
        "\n",
        "        try:\n",
        "            logger.info(\"\ud83d\udd27 Setting up Phoenix observability...\")\n",
        "\n",
        "            # Clean up existing processes\n",
        "            self._kill_existing_phoenix_processes()\n",
        "\n",
        "            # Find available ports\n",
        "            phoenix_port, grpc_port = self._find_available_port()\n",
        "\n",
        "            # Set environment variables\n",
        "            os.environ[\"PHOENIX_PORT\"] = str(phoenix_port)\n",
        "            os.environ[\"PHOENIX_GRPC_PORT\"] = str(grpc_port)\n",
        "\n",
        "            # Start Phoenix session\n",
        "            self.session = px.launch_app()\n",
        "            self.active_port = phoenix_port\n",
        "\n",
        "            if self.session:\n",
        "                logger.info(f\"\ud83c\udf10 Phoenix UI: {self.session.url}\")\n",
        "\n",
        "            # Register Phoenix OTEL\n",
        "            self.tracer_provider = register(\n",
        "                project_name=self.config.project_name,\n",
        "                endpoint=f\"http://localhost:{phoenix_port}/v1/traces\",\n",
        "            )\n",
        "\n",
        "            logger.info(\"\u2705 Phoenix setup completed successfully\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.exception(f\"\u274c Phoenix setup failed: {e}\")\n",
        "            return False\n",
        "\n",
        "    def setup_instrumentation(self) -> bool:\n",
        "        \"\"\"Setup OpenTelemetry instrumentation.\"\"\"\n",
        "        if not self.tracer_provider or not ARIZE_AVAILABLE:\n",
        "            return False\n",
        "\n",
        "        try:\n",
        "            instrumentors = [\n",
        "                (\"LangChain\", LangChainInstrumentor),\n",
        "                (\"OpenAI\", OpenAIInstrumentor),\n",
        "            ]\n",
        "\n",
        "            for name, instrumentor_class in instrumentors:\n",
        "                try:\n",
        "                    instrumentor = instrumentor_class()\n",
        "                    instrumentor.instrument(tracer_provider=self.tracer_provider)\n",
        "                    logger.info(f\"\u2705 {name} instrumentation enabled\")\n",
        "                except Exception as e:\n",
        "                    logger.warning(f\"\u26a0\ufe0f {name} instrumentation failed: {e}\")\n",
        "\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.exception(f\"\u274c Instrumentation setup failed: {e}\")\n",
        "            return False\n",
        "\n",
        "    def cleanup(self) -> None:\n",
        "        \"\"\"Clean up Phoenix resources.\"\"\"\n",
        "        try:\n",
        "            # Clean up environment variables\n",
        "            for var in [\"PHOENIX_PORT\", \"PHOENIX_GRPC_PORT\"]:\n",
        "                if var in os.environ:\n",
        "                    del os.environ[var]\n",
        "\n",
        "            logger.info(\"\ud83d\udd12 Phoenix cleanup completed\")\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"\u26a0\ufe0f Error during Phoenix cleanup: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mjlAzXBmTC9",
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Arize Dataset Manager\n",
        "\n",
        "Manages Arize dataset creation and management.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "T_MM1xe-mTC9"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "class ArizeDatasetManager:\n",
        "    \"\"\"Manages Arize dataset creation and management.\"\"\"\n",
        "\n",
        "    def __init__(self, config: EvaluationConfig):\n",
        "        self.config = config\n",
        "        self.client = None\n",
        "        self._setup_client()\n",
        "\n",
        "    def _setup_client(self) -> None:\n",
        "        \"\"\"Setup Arize datasets client.\"\"\"\n",
        "        if not ARIZE_AVAILABLE:\n",
        "            return\n",
        "\n",
        "        if (\n",
        "            self.config.arize_api_key != \"your-api-key\"\n",
        "            and self.config.arize_space_id != \"your-space-id\"\n",
        "        ):\n",
        "            try:\n",
        "                # Initialize with correct parameters - no space_id needed for datasets client\n",
        "                self.client = ArizeDatasetsClient(\n",
        "                    api_key=self.config.arize_api_key\n",
        "                )\n",
        "                logger.info(\"\u2705 Arize datasets client initialized successfully\")\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"\u26a0\ufe0f Could not initialize Arize datasets client: {e}\")\n",
        "                self.client = None\n",
        "        else:\n",
        "            logger.warning(\"\u26a0\ufe0f Arize API credentials not configured\")\n",
        "            self.client = None\n",
        "\n",
        "    def create_dataset(self, results_df: pd.DataFrame) -> Optional[str]:\n",
        "        \"\"\"Create Arize dataset from evaluation results.\"\"\"\n",
        "        if not self.client:\n",
        "            logger.warning(\"\u26a0\ufe0f Arize client not available - skipping dataset creation\")\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            dataset_name = f\"flight-search-evaluation-{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "\n",
        "            logger.info(\"\ud83d\udcca Creating Arize dataset...\")\n",
        "            dataset_id = self.client.create_dataset(\n",
        "                space_id=self.config.arize_space_id,\n",
        "                dataset_name=dataset_name,\n",
        "                dataset_type=GENERATIVE,\n",
        "                data=results_df,\n",
        "                convert_dict_to_json=True\n",
        "            )\n",
        "\n",
        "            if dataset_id:\n",
        "                logger.info(f\"\u2705 Arize dataset created successfully: {dataset_id}\")\n",
        "                return dataset_id\n",
        "            else:\n",
        "                logger.warning(\"\u26a0\ufe0f Dataset creation returned None\")\n",
        "                return None\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"\u274c Error creating Arize dataset: {e}\")\n",
        "            return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZYhP6E3mTC9",
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Arize Flight Search Evaluator\n",
        "\n",
        "Main evaluator class for comprehensive flight search agent evaluation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "7OmaB3d-mTC9"
      },
      "outputs": [],
      "source": [
        "class ArizeFlightSearchEvaluator:\n",
        "    \"\"\"\n",
        "    Streamlined flight search agent evaluator using only Arize Phoenix evaluators.\n",
        "\n",
        "    This class provides comprehensive evaluation capabilities using:\n",
        "    - Phoenix RelevanceEvaluator for response relevance\n",
        "    - Phoenix QAEvaluator for correctness assessment\n",
        "    - Phoenix HallucinationEvaluator for factual accuracy\n",
        "    - Phoenix ToxicityEvaluator for safety assessment\n",
        "    - No manual validation - Phoenix evaluators only\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: Optional[EvaluationConfig] = None):\n",
        "        \"\"\"Initialize the evaluator with configuration.\"\"\"\n",
        "        self.config = config or EvaluationConfig()\n",
        "        self._setup_logging()\n",
        "\n",
        "        # Initialize components\n",
        "        self.phoenix_manager = PhoenixManager(self.config)\n",
        "        self.dataset_manager = ArizeDatasetManager(self.config)\n",
        "\n",
        "        # Agent components\n",
        "        self.agent = None\n",
        "        self.span = None\n",
        "\n",
        "        # Phoenix evaluators\n",
        "        self.evaluators = {}\n",
        "        self.evaluator_llm = None\n",
        "\n",
        "        if ARIZE_AVAILABLE:\n",
        "            self._setup_phoenix_evaluators()\n",
        "\n",
        "    def _setup_logging(self) -> None:\n",
        "        \"\"\"Configure logging to suppress verbose modules.\"\"\"\n",
        "        verbose_modules = [\"openai\", \"httpx\", \"httpcore\", \"agentc_core\"]\n",
        "        for module in verbose_modules:\n",
        "            logging.getLogger(module).setLevel(logging.WARNING)\n",
        "\n",
        "    def _setup_phoenix_evaluators(self) -> None:\n",
        "        \"\"\"Setup Phoenix evaluators with robust error handling.\"\"\"\n",
        "        if not ARIZE_AVAILABLE:\n",
        "            logger.warning(\"\u26a0\ufe0f Phoenix dependencies not available - evaluations will be limited\")\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            self.evaluator_llm = OpenAIModel(model=self.config.evaluator_model)\n",
        "\n",
        "            # Initialize all Phoenix evaluators\n",
        "            self.evaluators = {\n",
        "                \"relevance\": RelevanceEvaluator(self.evaluator_llm),\n",
        "                \"qa_correctness\": QAEvaluator(self.evaluator_llm),\n",
        "                \"hallucination\": HallucinationEvaluator(self.evaluator_llm),\n",
        "                \"toxicity\": ToxicityEvaluator(self.evaluator_llm),\n",
        "            }\n",
        "\n",
        "            logger.info(\"\u2705 Phoenix evaluators initialized successfully\")\n",
        "            logger.info(f\"   \ud83e\udd16 Using evaluator model: {self.config.evaluator_model}\")\n",
        "            logger.info(f\"   \ud83d\udcca Available evaluators: {list(self.evaluators.keys())}\")\n",
        "\n",
        "            # Setup Phoenix if available\n",
        "            if self.phoenix_manager.start_phoenix():\n",
        "                self.phoenix_manager.setup_instrumentation()\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"\u26a0\ufe0f Phoenix evaluators setup failed: {e}\")\n",
        "            logger.info(\"Continuing with basic evaluation metrics only...\")\n",
        "            self.evaluators = {}\n",
        "\n",
        "    def setup_agent(self) -> bool:\n",
        "        \"\"\"Setup flight search agent using refactored main.py setup.\"\"\"\n",
        "        try:\n",
        "            logger.info(\"\ud83d\udd27 Setting up flight search agent...\")\n",
        "\n",
        "            # Use the refactored setup function from main.py\n",
        "            compiled_graph, application_span = setup_flight_search_agent()\n",
        "\n",
        "            self.agent = compiled_graph\n",
        "            self.span = application_span\n",
        "\n",
        "            logger.info(\"\u2705 Flight search agent setup completed successfully\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.exception(f\"\u274c Error setting up flight search agent: {e}\")\n",
        "            return False\n",
        "\n",
        "    def _extract_response_content(self, result: Any) -> str:\n",
        "        \"\"\"Extract complete response content including tool results from agent result.\"\"\"\n",
        "        try:\n",
        "            response_parts = []\n",
        "\n",
        "            # Critical Fix: Extract tool outputs from search_results first\n",
        "            if isinstance(result, dict) and \"search_results\" in result:\n",
        "                search_results = result[\"search_results\"]\n",
        "                if search_results:\n",
        "                    # search_results contains the actual tool outputs we want\n",
        "                    response_parts.append(str(search_results))\n",
        "\n",
        "            # Also check for intermediate_steps (AgentExecutor format)\n",
        "            if isinstance(result, dict) and \"intermediate_steps\" in result:\n",
        "                for step in result[\"intermediate_steps\"]:\n",
        "                    if isinstance(step, tuple) and len(step) >= 2:\n",
        "                        # step[1] is the tool output/observation\n",
        "                        tool_output = str(step[1])\n",
        "                        if tool_output and tool_output.strip():\n",
        "                            response_parts.append(tool_output)\n",
        "\n",
        "            # Check for messages from LangGraph state (but filter out generic ones)\n",
        "            if hasattr(result, \"messages\") and result.messages:\n",
        "                for message in result.messages:\n",
        "                    if hasattr(message, \"content\") and message.content:\n",
        "                        content = str(message.content)\n",
        "                        # Skip generic system messages and human messages\n",
        "                        if (hasattr(message, \"type\") and message.type != \"human\" and\n",
        "                            not any(phrase in content.lower() for phrase in\n",
        "                                   [\"iteration limit\", \"time limit\", \"agent stopped\"])):\n",
        "                            response_parts.append(content)\n",
        "            elif isinstance(result, dict) and \"messages\" in result:\n",
        "                for message in result[\"messages\"]:\n",
        "                    if hasattr(message, \"content\") and message.content:\n",
        "                        content = str(message.content)\n",
        "                        # Skip generic system messages and human messages\n",
        "                        if (hasattr(message, \"__class__\") and \"Human\" not in message.__class__.__name__ and\n",
        "                            not any(phrase in content.lower() for phrase in\n",
        "                                   [\"iteration limit\", \"time limit\", \"agent stopped\"])):\n",
        "                            response_parts.append(content)\n",
        "\n",
        "            # If we have response parts, join them\n",
        "            if response_parts:\n",
        "                return \"\\n\\n\".join(response_parts)\n",
        "\n",
        "            # Fallback to full result conversion\n",
        "            result_str = str(result)\n",
        "\n",
        "            # If result is a dict, try to extract useful parts\n",
        "            if isinstance(result, dict):\n",
        "                useful_parts = []\n",
        "                for key in ['output', 'response', 'result', 'answer']:\n",
        "                    if key in result and result[key]:\n",
        "                        useful_parts.append(f\"{key.title()}: {result[key]}\")\n",
        "\n",
        "                if useful_parts:\n",
        "                    return \"\\n\".join(useful_parts)\n",
        "\n",
        "            return result_str\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"Error extracting response: {e}\"\n",
        "\n",
        "    def run_single_evaluation(self, query: str) -> Dict[str, Any]:\n",
        "        \"\"\"Run evaluation for a single query - no manual validation.\"\"\"\n",
        "        if not self.agent:\n",
        "            raise RuntimeError(\"Agent not initialized. Call setup_agent() first.\")\n",
        "\n",
        "        logger.info(f\"\ud83d\udd0d Evaluating query: {query}\")\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        try:\n",
        "            # Build starting state and run query\n",
        "            state = FlightSearchGraph.build_starting_state(query=query)\n",
        "            result = self.agent.invoke(state)\n",
        "\n",
        "            # Extract response content\n",
        "            response = self._extract_response_content(result)\n",
        "\n",
        "            # Create evaluation result - no manual scoring\n",
        "            evaluation_result = {\n",
        "                \"query\": query,\n",
        "                \"response\": response,\n",
        "                \"execution_time\": time.time() - start_time,\n",
        "                \"success\": True,\n",
        "            }\n",
        "\n",
        "            logger.info(f\"\u2705 Query completed in {evaluation_result['execution_time']:.2f}s\")\n",
        "            return evaluation_result\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.exception(f\"\u274c Query failed: {e}\")\n",
        "            return {\n",
        "                \"query\": query,\n",
        "                \"response\": f\"Error: {str(e)}\",\n",
        "                \"execution_time\": time.time() - start_time,\n",
        "                \"success\": False,\n",
        "                \"error\": str(e),\n",
        "            }\n",
        "\n",
        "    def _create_reference_text(self, query: str) -> str:\n",
        "        \"\"\"Create reference text for evaluation based on query.\"\"\"\n",
        "        # Get the actual reference answer for this query\n",
        "        reference_answer = get_reference_answer(query)\n",
        "\n",
        "        if reference_answer.startswith(\"No reference answer available\"):\n",
        "            raise ValueError(\n",
        "                f\"No reference answer available for query: '{query}'. \"\n",
        "                f\"Please add this query to QUERY_REFERENCE_ANSWERS in the queries cell\"\n",
        "            )\n",
        "\n",
        "        return reference_answer\n",
        "\n",
        "    def run_phoenix_evaluations(self, results_df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Run Phoenix evaluations on the results.\"\"\"\n",
        "        if not ARIZE_AVAILABLE or not self.evaluators:\n",
        "            logger.warning(\"\u26a0\ufe0f Phoenix evaluators not available - skipping evaluations\")\n",
        "            return results_df\n",
        "\n",
        "        logger.info(f\"\ud83e\udde0 Running Phoenix evaluations on {len(results_df)} responses...\")\n",
        "        logger.info(\"\ud83d\udccb Evaluation criteria:\")\n",
        "        logger.info(\"   \ud83d\udd0d Relevance: Does the response address the flight search query?\")\n",
        "        logger.info(\"   \ud83c\udfaf QA Correctness: Is the flight information accurate and helpful?\")\n",
        "        logger.info(\"   \ud83d\udea8 Hallucination: Does the response contain fabricated information?\")\n",
        "        logger.info(\"   \u2620\ufe0f Toxicity: Is the response harmful or inappropriate?\")\n",
        "\n",
        "        try:\n",
        "            # Prepare evaluation data\n",
        "            evaluation_data = []\n",
        "            for _, row in results_df.iterrows():\n",
        "                query = row[\"query\"]\n",
        "                response = row[\"response\"]\n",
        "\n",
        "                # Create reference text based on query type (with validation)\n",
        "                reference = self._create_reference_text(str(query))\n",
        "\n",
        "                evaluation_data.append(\n",
        "                    {\n",
        "                        \"input\": query,\n",
        "                        \"output\": response,\n",
        "                        \"reference\": reference,\n",
        "                        \"query\": query,  # For hallucination evaluation\n",
        "                        \"response\": response,  # For hallucination evaluation\n",
        "                        \"text\": response,  # For toxicity evaluation\n",
        "                    }\n",
        "                )\n",
        "\n",
        "            eval_df = pd.DataFrame(evaluation_data)\n",
        "\n",
        "            # Run individual Phoenix evaluations\n",
        "            self._run_individual_phoenix_evaluations(eval_df, results_df)\n",
        "\n",
        "            logger.info(\"\u2705 Phoenix evaluations completed\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.exception(f\"\u274c Error running Phoenix evaluations: {e}\")\n",
        "            # Add error indicators\n",
        "            for eval_type in [\"relevance\", \"qa_correctness\", \"hallucination\", \"toxicity\"]:\n",
        "                results_df[eval_type] = \"error\"\n",
        "                results_df[f\"{eval_type}_explanation\"] = f\"Error: {e}\"\n",
        "\n",
        "        return results_df\n",
        "\n",
        "    def _run_individual_phoenix_evaluations(\n",
        "        self, eval_df: pd.DataFrame, results_df: pd.DataFrame\n",
        "    ) -> None:\n",
        "        \"\"\"Run individual Phoenix evaluations.\"\"\"\n",
        "        for eval_name, evaluator in self.evaluators.items():\n",
        "            try:\n",
        "                logger.info(f\"   \ud83d\udcca Running {eval_name} evaluation...\")\n",
        "\n",
        "                # Prepare data based on evaluator requirements\n",
        "                if eval_name == \"relevance\":\n",
        "                    data = eval_df[[\"input\", \"reference\"]].copy()\n",
        "                    eval_results = llm_classify(\n",
        "                        data=data,\n",
        "                        model=self.evaluator_llm,\n",
        "                        template=RAG_RELEVANCY_PROMPT_TEMPLATE,\n",
        "                        rails=list(RAG_RELEVANCY_PROMPT_RAILS_MAP.values()),\n",
        "                        provide_explanation=True,\n",
        "                    )\n",
        "                elif eval_name == \"qa_correctness\":\n",
        "                    data = eval_df[[\"input\", \"output\", \"reference\"]].copy()\n",
        "                    eval_results = llm_classify(\n",
        "                        data=data,\n",
        "                        model=self.evaluator_llm,\n",
        "                        template=LENIENT_QA_PROMPT_TEMPLATE,\n",
        "                        rails=LENIENT_QA_RAILS,\n",
        "                        provide_explanation=True,\n",
        "                    )\n",
        "                elif eval_name == \"hallucination\":\n",
        "                    data = eval_df[[\"input\", \"reference\", \"output\"]].copy()\n",
        "                    eval_results = llm_classify(\n",
        "                        data=data,\n",
        "                        model=self.evaluator_llm,\n",
        "                        template=LENIENT_HALLUCINATION_PROMPT_TEMPLATE,\n",
        "                        rails=LENIENT_HALLUCINATION_RAILS,\n",
        "                        provide_explanation=True,\n",
        "                    )\n",
        "                elif eval_name == \"toxicity\":\n",
        "                    data = eval_df[[\"input\"]].copy()\n",
        "                    eval_results = llm_classify(\n",
        "                        data=data,\n",
        "                        model=self.evaluator_llm,\n",
        "                        template=TOXICITY_PROMPT_TEMPLATE,\n",
        "                        rails=list(TOXICITY_PROMPT_RAILS_MAP.values()),\n",
        "                        provide_explanation=True,\n",
        "                    )\n",
        "                else:\n",
        "                    logger.warning(f\"\u26a0\ufe0f Unknown evaluator: {eval_name}\")\n",
        "                    continue\n",
        "\n",
        "                # Process results\n",
        "                self._process_evaluation_results(eval_results, eval_name, results_df)\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"\u26a0\ufe0f {eval_name} evaluation failed: {e}\")\n",
        "                results_df[eval_name] = \"error\"\n",
        "                results_df[f\"{eval_name}_explanation\"] = f\"Error: {e}\"\n",
        "\n",
        "    def _process_evaluation_results(\n",
        "        self, eval_results: Any, eval_name: str, results_df: pd.DataFrame\n",
        "    ) -> None:\n",
        "        \"\"\"Process evaluation results and add to results DataFrame.\"\"\"\n",
        "        try:\n",
        "            if eval_results is None:\n",
        "                logger.warning(f\"\u26a0\ufe0f {eval_name} evaluation returned None\")\n",
        "                results_df[eval_name] = \"unknown\"\n",
        "                results_df[f\"{eval_name}_explanation\"] = \"Evaluation returned None\"\n",
        "                return\n",
        "\n",
        "            # Handle DataFrame results\n",
        "            if hasattr(eval_results, \"columns\"):\n",
        "                if \"label\" in eval_results.columns:\n",
        "                    results_df[eval_name] = eval_results[\"label\"].tolist()\n",
        "                elif \"classification\" in eval_results.columns:\n",
        "                    results_df[eval_name] = eval_results[\"classification\"].tolist()\n",
        "                else:\n",
        "                    results_df[eval_name] = \"unknown\"\n",
        "\n",
        "                if \"explanation\" in eval_results.columns:\n",
        "                    results_df[f\"{eval_name}_explanation\"] = eval_results[\"explanation\"].tolist()\n",
        "                elif \"reason\" in eval_results.columns:\n",
        "                    results_df[f\"{eval_name}_explanation\"] = eval_results[\"reason\"].tolist()\n",
        "                else:\n",
        "                    results_df[f\"{eval_name}_explanation\"] = \"No explanation provided\"\n",
        "\n",
        "                logger.info(f\"   \u2705 {eval_name} evaluation completed\")\n",
        "\n",
        "            # Handle list results\n",
        "            elif isinstance(eval_results, list) and len(eval_results) > 0:\n",
        "                if isinstance(eval_results[0], dict):\n",
        "                    results_df[eval_name] = [item.get(\"label\", \"unknown\") for item in eval_results]\n",
        "                    results_df[f\"{eval_name}_explanation\"] = [\n",
        "                        item.get(\"explanation\", \"No explanation\") for item in eval_results\n",
        "                    ]\n",
        "                else:\n",
        "                    results_df[eval_name] = eval_results\n",
        "                    results_df[f\"{eval_name}_explanation\"] = \"List evaluation result\"\n",
        "\n",
        "                logger.info(f\"   \u2705 {eval_name} evaluation completed (list format)\")\n",
        "\n",
        "            else:\n",
        "                logger.warning(f\"\u26a0\ufe0f {eval_name} evaluation returned unexpected format\")\n",
        "                results_df[eval_name] = \"unknown\"\n",
        "                results_df[f\"{eval_name}_explanation\"] = f\"Unexpected format: {type(eval_results)}\"\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"\u26a0\ufe0f Error processing {eval_name} results: {e}\")\n",
        "            results_df[eval_name] = \"error\"\n",
        "            results_df[f\"{eval_name}_explanation\"] = f\"Processing error: {e}\"\n",
        "\n",
        "    def run_evaluation(self, queries: List[str]) -> pd.DataFrame:\n",
        "        \"\"\"Run complete evaluation pipeline using only Phoenix evaluators.\"\"\"\n",
        "        # Clear existing bookings for a clean test run\n",
        "        clear_bookings_and_reviews()\n",
        "\n",
        "        if not self.setup_agent():\n",
        "            raise RuntimeError(\"Failed to setup agent\")\n",
        "\n",
        "        logger.info(f\"\ud83d\ude80 Starting evaluation with {len(queries)} queries\")\n",
        "\n",
        "        # Log available features\n",
        "        logger.info(\"\ud83d\udccb Evaluation Configuration:\")\n",
        "        logger.info(f\"   \ud83e\udd16 Agent: Flight Search Agent (LangGraph)\")\n",
        "        logger.info(f\"   \ud83d\udd27 Phoenix Available: {'\u2705' if ARIZE_AVAILABLE else '\u274c'}\")\n",
        "        logger.info(f\"   \ud83d\udcca Arize Datasets: {'\u2705' if ARIZE_AVAILABLE and (self.dataset_manager.client is not None) else '\u274c'}\")\n",
        "        if self.evaluators:\n",
        "            logger.info(f\"   \ud83e\udde0 Phoenix Evaluators: {list(self.evaluators.keys())}\")\n",
        "        else:\n",
        "            logger.info(\"   \ud83e\udde0 Phoenix Evaluators: \u274c (basic metrics only)\")\n",
        "\n",
        "        # Run queries (no manual validation)\n",
        "        results = []\n",
        "        for i, query in enumerate(queries, 1):\n",
        "            logger.info(f\"\\n\ud83d\udccb Query {i}/{len(queries)}\")\n",
        "            result = self.run_single_evaluation(query)\n",
        "            results.append(result)\n",
        "\n",
        "        # Create results DataFrame\n",
        "        results_df = pd.DataFrame(results)\n",
        "\n",
        "        # Run Phoenix evaluations only\n",
        "        results_df = self.run_phoenix_evaluations(results_df)\n",
        "\n",
        "        # Log summary\n",
        "        self._log_evaluation_summary(results_df)\n",
        "\n",
        "        # Create Arize dataset\n",
        "        dataset_id = self.dataset_manager.create_dataset(results_df)\n",
        "        if dataset_id:\n",
        "            logger.info(f\"\ud83d\udcca Arize dataset created: {dataset_id}\")\n",
        "        else:\n",
        "            logger.warning(\"\u26a0\ufe0f Dataset creation failed\")\n",
        "\n",
        "        return results_df\n",
        "\n",
        "    def _log_evaluation_summary(self, results_df: pd.DataFrame) -> None:\n",
        "        \"\"\"Log evaluation summary using Phoenix results only.\"\"\"\n",
        "        logger.info(\"\\n\ud83d\udcca Phoenix Evaluation Summary:\")\n",
        "        logger.info(f\"  Total queries: {len(results_df)}\")\n",
        "        logger.info(f\"  Successful executions: {results_df['success'].sum()}\")\n",
        "        logger.info(f\"  Failed executions: {(~results_df['success']).sum()}\")\n",
        "        logger.info(f\"  Average execution time: {results_df['execution_time'].mean():.2f}s\")\n",
        "\n",
        "        # Phoenix evaluation results\n",
        "        if ARIZE_AVAILABLE and self.evaluators:\n",
        "            logger.info(\"\\n\ud83e\udde0 Phoenix Evaluation Results:\")\n",
        "            for eval_type in [\"relevance\", \"qa_correctness\", \"hallucination\", \"toxicity\"]:\n",
        "                if eval_type in results_df.columns:\n",
        "                    counts = results_df[eval_type].value_counts()\n",
        "                    logger.info(f\"   {eval_type}: {dict(counts)}\")\n",
        "\n",
        "        # Quick scores summary\n",
        "        if len(results_df) > 0:\n",
        "            logger.info(\"\\n\ud83d\udcca Quick Scores Summary:\")\n",
        "            for i in range(len(results_df)):\n",
        "                row = results_df.iloc[i]\n",
        "                scores = []\n",
        "                for eval_type in [\"relevance\", \"qa_correctness\", \"hallucination\", \"toxicity\"]:\n",
        "                    if eval_type in row:\n",
        "                        result = row[eval_type]\n",
        "                        emoji = \"\u2705\" if result in [\"relevant\", \"correct\", \"factual\", \"non-toxic\"] else \"\u274c\"\n",
        "                        scores.append(f\"{emoji} {eval_type}: {result}\")\n",
        "\n",
        "                logger.info(f\"   Query {i+1}: {' | '.join(scores)}\")\n",
        "\n",
        "        # Sample results with FULL detailed explanations for debugging\n",
        "        if len(results_df) > 0:\n",
        "            logger.info(\"\\n\ud83d\udcdd DETAILED EVALUATION RESULTS (FULL EXPLANATIONS):\")\n",
        "            logger.info(\"=\"*80)\n",
        "            for i in range(min(len(results_df), len(results_df))):  # Show all results\n",
        "                row = results_df.iloc[i]\n",
        "                logger.info(f\"\\n\ud83d\udd0d QUERY {i+1}: {row['query']}\")\n",
        "                logger.info(\"-\"*60)\n",
        "\n",
        "                for eval_type in [\"relevance\", \"qa_correctness\", \"hallucination\", \"toxicity\"]:\n",
        "                    if eval_type in row:\n",
        "                        result = row[eval_type]\n",
        "                        # Show FULL explanation instead of processed/truncated version\n",
        "                        full_explanation = str(row.get(f\"{eval_type}_explanation\", \"No explanation provided\"))\n",
        "                        logger.info(f\"\\n\ud83d\udcca {eval_type.upper()}: {result}\")\n",
        "                        logger.info(f\"\ud83d\udcad FULL REASONING:\")\n",
        "                        logger.info(f\"{full_explanation}\")\n",
        "                        logger.info(\"-\"*40)\n",
        "                logger.info(\"=\"*80)\n",
        "\n",
        "    def cleanup(self) -> None:\n",
        "        \"\"\"Clean up all resources.\"\"\"\n",
        "        self.phoenix_manager.cleanup()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZInHfhu5mTC9",
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Evaluator Methods - Part 2\n",
        "\n",
        "Additional methods for the ArizeFlightSearchEvaluator class.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKb3jE_OmTC-",
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Utility Functions\n",
        "\n",
        "Utility functions for running evaluations and demos.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "06H-zcs6mTC-"
      },
      "outputs": [],
      "source": [
        "def get_default_queries() -> List[str]:\n",
        "    \"\"\"Get default test queries for evaluation.\"\"\"\n",
        "    # Use the same queries from data/queries.py to ensure reference answers match\n",
        "    return FLIGHT_SEARCH_QUERIES\n",
        "\n",
        "def run_phoenix_demo() -> pd.DataFrame:\n",
        "    \"\"\"Run a simple Phoenix evaluation demo.\"\"\"\n",
        "    logger.info(\"\ud83d\udd27 Running Phoenix evaluation demo...\")\n",
        "\n",
        "    demo_queries = [\n",
        "        \"Find flights from JFK to LAX\",\n",
        "        \"What do passengers say about SpiceJet's service quality?\",\n",
        "    ]\n",
        "\n",
        "    evaluator = ArizeFlightSearchEvaluator()\n",
        "    try:\n",
        "        results = evaluator.run_evaluation(demo_queries)\n",
        "        logger.info(\"\ud83c\udf89 Phoenix evaluation demo complete!\")\n",
        "        logger.info(\"\ud83d\udca1 Visit Phoenix UI to see detailed traces and evaluations\")\n",
        "        return results\n",
        "    finally:\n",
        "        evaluator.cleanup()\n",
        "\n",
        "def run_full_evaluation() -> pd.DataFrame:\n",
        "    \"\"\"Main evaluation function using only Phoenix evaluators.\"\"\"\n",
        "    evaluator = ArizeFlightSearchEvaluator()\n",
        "    try:\n",
        "        results = evaluator.run_evaluation(get_default_queries())\n",
        "        logger.info(\"\\n\u2705 Phoenix evaluation complete!\")\n",
        "        return results\n",
        "    finally:\n",
        "        evaluator.cleanup()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCLJZKGAmTC-",
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Run Evaluation\n",
        "\n",
        "Execute the flight search agent evaluation with Phoenix AI.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "e0d7b4358a3a4b8780a7ca422bfab550",
            "9d00eb2664014093b141e7c8fc5d3381",
            "33d55737f3fd4bf5914d89f2ade374f1",
            "08b76b42b5634e7ab725f6e38e722790",
            "c7bfd0f371fd4371b78d63c881e71e49",
            "ab52536811af42a3a5a439de7b210fbf",
            "475b84d602ad4e719ee12f65e0e40914",
            "12e035830a2d4bc49db81a864a24dfd6",
            "0b7715b8ba57436d9862132072dc6132",
            "51d7ab2734c14bfdb51e062b1f83ccae",
            "b28ee54b59d841b7b4efaf3236f48bcf",
            "1059b51758cc404ba3acda7f62db308c",
            "aa75aec33dcb43208f529dccc9119f3f",
            "1cecbd448ddb4ed29d4b8f20675a443f",
            "a719eb5f2e0e40a5a532f163d0e0335f",
            "bbda821ec5934366be75661758d1f552",
            "38985c23ffba4d9193c09f131798d25a",
            "06d02048d7164a80a47def376d9c2873",
            "d40aeff24f0f44e8aa15dc54d39d2cf2",
            "abaa55c75b584010959cf1ced1cc64d2",
            "d88505b02c004fd6af0bee68711f93a4",
            "8ad70e34d27c449aa9c624cbe68fb4ae",
            "8932e31ca4fa42f9a1eaf6c8ecf9bcf4",
            "bd22ba3d57ca4246a93e785fcea52d3c",
            "6f8e484c401b4945baf155ba5d55e470",
            "7f17844eca3046ac8b9e18d4e8656599",
            "1e08e7a9f7e74ee2b2e7de4a228f9f4b",
            "a1dbcb5830214017bd0366bd9356a016",
            "250fd658f06244a6827cc5db51166020",
            "44183fa868b34132987251224c9ef05c",
            "0c5fbf42e43b446a86bdce806b73a060",
            "9d7a7a6e3cc9436b953a5075b203c011",
            "b20c263659de4ff88c875a1a04de602f",
            "2fc35071b27d46a18a209b3db4932ea6",
            "ae4197440faf47d281cef1af982ec37f",
            "140c5556982340c8b7d4928e01264a5a",
            "1270ce92434d4a7b9add79622d885522",
            "2f1fb50bb3eb4980a9541f9712f9afd6",
            "4d0e44416fe5488f82864e19614bca4b",
            "7a38d95f784f48acb31996f05eef5659",
            "44a42f9b846c416b8fa2a1988f978dca",
            "606dcd5ff8944cb789268c9fe683f04c",
            "aaf9a30f83a043a38e1535e4084c0b7d",
            "a22847da8eed4c3e999f1d5b100c2c7d"
          ]
        },
        "id": "O4YhW5LxmTC-",
        "outputId": "b15be0fe-9425-4467-bc1c-2635fff0361c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:\u2705 Arize datasets client initialized successfully\n",
            "INFO:__main__:\u2705 Phoenix evaluators initialized successfully\n",
            "INFO:__main__:   \ud83e\udd16 Using evaluator model: gpt-4o\n",
            "INFO:__main__:   \ud83d\udcca Available evaluators: ['relevance', 'qa_correctness', 'hallucination', 'toxicity']\n",
            "INFO:__main__:\ud83d\udd27 Setting up Phoenix observability...\n",
            "INFO:phoenix.config:\ud83d\udccb Ensuring phoenix working directory: /root/.phoenix\n",
            "INFO:alembic.runtime.migration:Context impl SQLiteImpl.\n",
            "INFO:alembic.runtime.migration:Will assume transactional DDL.\n",
            "INFO:alembic.runtime.migration:Running upgrade  -> cf03bd6bae1d, init\n",
            "INFO:alembic.runtime.migration:Running upgrade cf03bd6bae1d -> 10460e46d750, datasets\n",
            "INFO:alembic.runtime.migration:Running upgrade 10460e46d750 -> 3be8647b87d8, add token columns to spans table\n",
            "INFO:alembic.runtime.migration:Running upgrade 3be8647b87d8 -> cd164e83824f, users and tokens\n",
            "INFO:alembic.runtime.migration:Running upgrade cd164e83824f -> 4ded9e43755f, create project_session table\n",
            "INFO:alembic.runtime.migration:Running upgrade 4ded9e43755f -> bc8fea3c2bc8, Add prompt tables\n",
            "INFO:alembic.runtime.migration:Running upgrade bc8fea3c2bc8 -> 2f9d1a65945f, Annotation config migrations\n",
            "/usr/lib/python3.12/contextlib.py:144: SAWarning: Skipped unsupported reflection of expression-based index ix_cumulative_llm_token_count_total\n",
            "  next(self.gen)\n",
            "/usr/lib/python3.12/contextlib.py:144: SAWarning: Skipped unsupported reflection of expression-based index ix_latency\n",
            "  next(self.gen)\n",
            "INFO:alembic.runtime.migration:Running upgrade 2f9d1a65945f -> bb8139330879, create project trace retention policies table\n",
            "INFO:alembic.runtime.migration:Running upgrade bb8139330879 -> 8a3764fe7f1a, change jsonb to json for prompts\n",
            "INFO:alembic.runtime.migration:Running upgrade 8a3764fe7f1a -> 6a88424799fe, Add auth_method column to users table and migrate existing authentication data.\n",
            "INFO:alembic.runtime.migration:Running upgrade 6a88424799fe -> a20694b15f82, Cost-related tables\n",
            "INFO:phoenix.server.app:Server umap params: UMAPParameters(min_dist=0.0, n_neighbors=30, n_samples=500)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\ud83c\udf0d To view the Phoenix app in your browser, visit https://ducjn7hsnyw1-496ff2e9c6d22116-6006-colab.googleusercontent.com/\n",
            "\ud83d\udcd6 For more information on how to use Phoenix, check out https://arize.com/docs/phoenix\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:\ud83c\udf10 Phoenix UI: https://ducjn7hsnyw1-496ff2e9c6d22116-6006-colab.googleusercontent.com/\n",
            "INFO:__main__:\u2705 Phoenix setup completed successfully\n",
            "INFO:__main__:\u2705 LangChain instrumentation enabled\n",
            "INFO:__main__:\u2705 OpenAI instrumentation enabled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\ud83d\udd2d OpenTelemetry Tracing Details \ud83d\udd2d\n",
            "|  Phoenix Project: flight-search-agent-evaluation\n",
            "|  Span Processor: SimpleSpanProcessor\n",
            "|  Collector Endpoint: http://localhost:6006/v1/traces\n",
            "|  Transport: HTTP + protobuf\n",
            "|  Transport Headers: {}\n",
            "|  \n",
            "|  Using a default SpanProcessor. `add_span_processor` will overwrite this default.\n",
            "|  \n",
            "|  \u26a0\ufe0f WARNING: It is strongly advised to use a BatchSpanProcessor in production environments.\n",
            "|  \n",
            "|  `register` has set this TracerProvider as the global OpenTelemetry default.\n",
            "|  To disable this behavior, call `register` with `set_global_tracer_provider=False`.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:\u2705 Successfully connected to Couchbase\n",
            "INFO:__main__:\ud83d\uddd1\ufe0f Clearing data from travel-sample.agentc_bookings.user_bookings_20251023...\n",
            "INFO:__main__:\ud83d\uddd1\ufe0f Clearing data from travel-sample.agentc_bookings.user_bookings_20251022...\n",
            "INFO:__main__:\u2705 Completed clearing scope: travel-sample.agentc_bookings\n",
            "INFO:__main__:\u2705 Cleared existing flight bookings for fresh test run: travel-sample.agentc_bookings\n",
            "INFO:__main__:\ud83d\udcca Airline reviews collection: 2210 existing, 2210 expected\n",
            "INFO:__main__:\u2705 Collection already has correct document count (2210), skipping clear\n",
            "INFO:__main__:\ud83d\udd27 Setting up flight search agent...\n",
            "INFO:__main__:\u2705 Environment variables configured\n",
            "INFO:__main__:\u2705 Successfully connected to Couchbase\n",
            "INFO:__main__:\u2705 Collection setup complete: agentc_data.airline_reviews\n",
            "INFO:__main__:Loaded vector search index definition from agentcatalog_index.json\n",
            "INFO:__main__:\u2139\ufe0f Vector search index 'airline_reviews_index' already exists\n",
            "INFO:__main__:\ud83d\udd27 Setting up Priority 1 AI services for langgraph framework...\n",
            "INFO:__main__:\ud83d\udd27 Endpoint: https://agd6zdjymyanhi9g.ai.sandbox.nonprod-project-avengers.com\n",
            "INFO:__main__:\ud83d\udd27 Model: nvidia/llama-3.2-nv-embedqa-1b-v2\n",
            "INFO:__main__:\ud83d\udd27 Base URL: https://agd6zdjymyanhi9g.ai.sandbox.nonprod-project-avengers.com/v1\n",
            "INFO:__main__:\u2705 Using Priority 1: Capella AI embeddings (OpenAI wrapper)\n",
            "INFO:__main__:\ud83d\udd27 LLM Endpoint: https://agd6zdjymyanhi9g.ai.sandbox.nonprod-project-avengers.com\n",
            "INFO:__main__:\ud83d\udd27 LLM Model: meta/llama3-8b-instruct\n",
            "INFO:__main__:\ud83d\udd27 LLM Base URL: https://agd6zdjymyanhi9g.ai.sandbox.nonprod-project-avengers.com/v1\n",
            "INFO:__main__:\u2705 Using Priority 1: Capella AI LLM (OpenAI wrapper)\n",
            "INFO:__main__:\u2705 Priority 1 AI services setup completed for langgraph\n",
            "INFO:__main__:\ud83d\udd04 Loading data into vector store...\n",
            "INFO:__main__:Found 2210 existing documents in collection, skipping data load\n",
            "INFO:__main__:\u2705 Vector store setup complete: travel-sample.agentc_data.airline_reviews\n",
            "INFO:__main__:\ud83d\udd27 Setting up Priority 1 AI services for langgraph framework...\n",
            "INFO:__main__:\ud83d\udd27 Endpoint: https://agd6zdjymyanhi9g.ai.sandbox.nonprod-project-avengers.com\n",
            "INFO:__main__:\ud83d\udd27 Model: nvidia/llama-3.2-nv-embedqa-1b-v2\n",
            "INFO:__main__:\ud83d\udd27 Base URL: https://agd6zdjymyanhi9g.ai.sandbox.nonprod-project-avengers.com/v1\n",
            "INFO:__main__:\u2705 Using Priority 1: Capella AI embeddings (OpenAI wrapper)\n",
            "INFO:__main__:\ud83d\udd27 LLM Endpoint: https://agd6zdjymyanhi9g.ai.sandbox.nonprod-project-avengers.com\n",
            "INFO:__main__:\ud83d\udd27 LLM Model: meta/llama3-8b-instruct\n",
            "INFO:__main__:\ud83d\udd27 LLM Base URL: https://agd6zdjymyanhi9g.ai.sandbox.nonprod-project-avengers.com/v1\n",
            "INFO:__main__:\u2705 Using Priority 1: Capella AI LLM (OpenAI wrapper)\n",
            "INFO:__main__:\u2705 Priority 1 AI services setup completed for langgraph\n",
            "INFO:__main__:Agent Catalog integration successful\n",
            "INFO:__main__:\u2705 Flight search agent setup completed successfully\n",
            "INFO:__main__:\ud83d\ude80 Starting evaluation with 5 queries\n",
            "INFO:__main__:\ud83d\udccb Evaluation Configuration:\n",
            "INFO:__main__:   \ud83e\udd16 Agent: Flight Search Agent (LangGraph)\n",
            "INFO:__main__:   \ud83d\udd27 Phoenix Available: \u2705\n",
            "INFO:__main__:   \ud83d\udcca Arize Datasets: \u2705\n",
            "INFO:__main__:   \ud83e\udde0 Phoenix Evaluators: ['relevance', 'qa_correctness', 'hallucination', 'toxicity']\n",
            "INFO:__main__:\n",
            "\ud83d\udccb Query 1/5\n",
            "INFO:__main__:\ud83d\udd0d Evaluating query: Find flights from JFK to LAX\n",
            "INFO:__main__:Flight Query: Find flights from JFK to LAX\n",
            "INFO:__main__:\u2705 Found tool: lookup_flight_info\n",
            "INFO:__main__:\u2705 Found tool: save_flight_booking\n",
            "INFO:__main__:\u2705 Found tool: retrieve_flight_bookings\n",
            "INFO:__main__:\u2705 Found tool: search_airline_reviews\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:\u2705 Parsed lookup_flight_info: JFK \u2192 LAX\n",
            "INFO:lookup_flight_info:\ud83d\udd0d Looking up flights: JFK \u2192 LAX\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32;1m\u001b[1;3mThought: I need to use lookup_flight_info to help with this request Action: lookup_flight_info Action Input: {\"source_airport\": \"JFK\", \"destination_airport\": \"LAX\"}\u001b[0m\u001b[36;1m\u001b[1;3mAvailable flights from JFK to LAX:\n",
            "\n",
            "1. AS flight from JFK to LAX using 321 762\n",
            "2. B6 flight from JFK to LAX using 320\n",
            "3. DL flight from JFK to LAX using 76W 752\n",
            "4. QF flight from JFK to LAX using 744\n",
            "5. AA flight from JFK to LAX using 32B 762\n",
            "6. UA flight from JFK to LAX using 757\n",
            "7. US flight from JFK to LAX using 32B 762\n",
            "8. VX flight from JFK to LAX using 320\u001b[0m"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:\u2705 Query completed in 2.86s\n",
            "INFO:__main__:\n",
            "\ud83d\udccb Query 2/5\n",
            "INFO:__main__:\ud83d\udd0d Evaluating query: Book a flight from LAX to JFK for tomorrow, 2 passengers, business class\n",
            "INFO:__main__:Flight Query: Book a flight from LAX to JFK for tomorrow, 2 passengers, business class\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32;1m\u001b[1;3mThought: I now know the final answer\n",
            "Final Answer: I found 8 available flights from JFK to LAX, including airlines AS, B6, DL, QF, AA, UA, US, and VX with various aircraft types.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:\u2705 Found tool: lookup_flight_info\n",
            "INFO:__main__:\u2705 Found tool: save_flight_booking\n",
            "INFO:__main__:\u2705 Found tool: retrieve_flight_bookings\n",
            "INFO:__main__:\u2705 Found tool: search_airline_reviews\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:\u2705 Parsed save_flight_booking: LAX\u2192JFK, 2 pax, business\n",
            "INFO:save_flight_booking:\ud83d\uddd3\ufe0f Date validated: 2025-10-24\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32;1m\u001b[1;3mThought: I need to use save_flight_booking to help with this request. Tomorrow is 2025-10-23 + 1 day = 2025-10-24 Action: save_flight_booking Action Input: {\"source_airport\": \"LAX\", \"destination_airport\": \"JFK\", \"departure_date\": \"2025-10-24\", \"passengers\": 2, \"flight_class\": \"business\"}\u001b[0m"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:save_flight_booking:\ud83c\udfaf Booking: LAX\u2192JFK on 2025-10-24, 2 pax, business class\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33;1m\u001b[1;3mFlight Booking Confirmed!\n",
            "\n",
            "Booking ID: FL102493CF77CF\n",
            "Route: LAX \u2192 JFK\n",
            "Departure Date: 2025-10-24\n",
            "Passengers: 2\n",
            "Class: business\n",
            "Total Price: $1500.00\n",
            "\n",
            "Next Steps:\n",
            "1. Check-in opens 24 hours before departure\n",
            "2. Arrive at airport 2 hours early for domestic flights\n",
            "3. Bring valid government-issued photo ID\n",
            "\n",
            "Thank you for choosing our airline!\u001b[0m"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:\u2705 Query completed in 4.11s\n",
            "INFO:__main__:\n",
            "\ud83d\udccb Query 3/5\n",
            "INFO:__main__:\ud83d\udd0d Evaluating query: Book an economy flight from JFK to MIA for next week, 1 passenger\n",
            "INFO:__main__:Flight Query: Book an economy flight from JFK to MIA for next week, 1 passenger\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32;1m\u001b[1;3mThought: I now know the final answer Final Answer: Your flight from LAX to JFK for 2 passengers in business class has been booked successfully for tomorrow (2025-10-24). Your booking ID is FL102493CF77CF and the total cost is $1500.00.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:\u2705 Found tool: lookup_flight_info\n",
            "INFO:__main__:\u2705 Found tool: save_flight_booking\n",
            "INFO:__main__:\u2705 Found tool: retrieve_flight_bookings\n",
            "INFO:__main__:\u2705 Found tool: search_airline_reviews\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:\u2705 Parsed save_flight_booking: JFK\u2192MIA, 1 pax, economy\n",
            "INFO:save_flight_booking:\ud83d\uddd3\ufe0f Date validated: 2025-10-30\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32;1m\u001b[1;3mThought: I need to use save_flight_booking to help with this request. Next week is 2025-10-23 + 7 days = 2025-10-30 Action: save_flight_booking Action Input: {\"source_airport\": \"JFK\", \"destination_airport\": \"MIA\", \"departure_date\": \"2025-10-30\", \"passengers\": 1, \"flight_class\": \"economy\"}\u001b[0m"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:save_flight_booking:\ud83c\udfaf Booking: JFK\u2192MIA on 2025-10-30, 1 pax, economy class\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33;1m\u001b[1;3mFlight Booking Confirmed!\n",
            "\n",
            "Booking ID: FL1030186C782C\n",
            "Route: JFK \u2192 MIA\n",
            "Departure Date: 2025-10-30\n",
            "Passengers: 1\n",
            "Class: economy\n",
            "Total Price: $250.00\n",
            "\n",
            "Next Steps:\n",
            "1. Check-in opens 24 hours before departure\n",
            "2. Arrive at airport 2 hours early for domestic flights\n",
            "3. Bring valid government-issued photo ID\n",
            "\n",
            "Thank you for choosing our airline!\u001b[0m"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:\u2705 Query completed in 4.05s\n",
            "INFO:__main__:\n",
            "\ud83d\udccb Query 4/5\n",
            "INFO:__main__:\ud83d\udd0d Evaluating query: Show me my current flight bookings\n",
            "INFO:__main__:Flight Query: Show me my current flight bookings\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32;1m\u001b[1;3mThought: I now know the final answer\n",
            "Final Answer: Your economy flight from JFK to MIA for next week (2025-10-30) has been booked successfully. Your booking ID is FL1030186C782C, and the total cost is $250.00.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:\u2705 Found tool: lookup_flight_info\n",
            "INFO:__main__:\u2705 Found tool: save_flight_booking\n",
            "INFO:__main__:\u2705 Found tool: retrieve_flight_bookings\n",
            "INFO:__main__:\u2705 Found tool: search_airline_reviews\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: I need to use retrieve_flight_bookings to help with this request Action: retrieve_flight_bookings Action Input:\u001b[0m\u001b[38;5;200m\u001b[1;3mYour Current Bookings (2 found):\n",
            "\n",
            "Booking 1:\n",
            "  Booking ID: FL1030186C782C\n",
            "  Route: JFK \u2192 MIA\n",
            "  Date: 2025-10-30\n",
            "  Passengers: 1\n",
            "  Class: economy\n",
            "  Total: $250.00\n",
            "  Status: confirmed\n",
            "  Booked: 2025-10-23\n",
            "\n",
            "Booking 2:\n",
            "  Booking ID: FL102493CF77CF\n",
            "  Route: LAX \u2192 JFK\n",
            "  Date: 2025-10-24\n",
            "  Passengers: 2\n",
            "  Class: business\n",
            "  Total: $1500.00\n",
            "  Status: confirmed\n",
            "  Booked: 2025-10-23\u001b[0m"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:\u2705 Query completed in 3.11s\n",
            "INFO:__main__:\n",
            "\ud83d\udccb Query 5/5\n",
            "INFO:__main__:\ud83d\udd0d Evaluating query: What do passengers say about SpiceJet's service quality?\n",
            "INFO:__main__:Flight Query: What do passengers say about SpiceJet's service quality?\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32;1m\u001b[1;3mThought: I now know the final answer\n",
            "Final Answer: You have 2 current flight bookings: (1) JFK to MIA on 2025-10-30 for 1 passenger in economy class ($250.00), and (2) LAX to JFK on 2025-10-24 for 2 passengers in business class ($1500.00).\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:\u2705 Found tool: lookup_flight_info\n",
            "INFO:__main__:\u2705 Found tool: save_flight_booking\n",
            "INFO:__main__:\u2705 Found tool: retrieve_flight_bookings\n",
            "INFO:__main__:\u2705 Found tool: search_airline_reviews\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: I need to use search_airline_reviews to help with this request Action: search_airline_reviews Action Input: SpiceJet service quality\u001b[0m"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:search_airline_reviews:Searching for airline reviews with query: 'SpiceJet service quality'\n",
            "INFO:search_airline_reviews:Found 5 results for query: 'SpiceJet service quality'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36;1m\u001b[1;3mFound 5 relevant airline reviews for 'SpiceJet service quality':\n",
            "\n",
            "Review 1:\n",
            "Airline: SpiceJet. Title: \"Great travel experience\". Review: \u2705 Trip Verified |  Marvelous courteous crew who took good care of all passengers. They should be rewarded for the patience shown towards the unruly ones. Great travel experience.. Rating: 10.0/10. Reviewer: Ranjita Pandey. Date: 18th April 2024. Recommended: yes\n",
            "\n",
            "Review 2:\n",
            "Airline: SpiceJet. Title: \"good service by the crew\". Review: \u2705 Trip Verified | I have had good service by the crew. It was amazing, the crew was very enthusiastic and warm welcome. It was one of the best services in my experience.. Rating: 10.0/10. Reviewer: K Mansour. Date: 10th August 2024. Recommended: yes\n",
            "\n",
            "Review 3:\n",
            "Airline: SpiceJet. Title: \"Excellent service\". Review: \u2705 Trip Verified |  Excellent service by the ground staff courteous beyond expectations always willing to help in the real sense and not lipservice i will recommend to all whom I know. Rating: 10.0/10. Reviewer: Ramanathan Ramchandra. Date: 1st November 2023. Recommended: yes\n",
            "\n",
            "Review 4:\n",
            "Airline: SpiceJet. Title: \"excellent hospitality\". Review: \u2705 Trip Verified |  It's very grateful to SpiceJet for the service in flight. Everything is fine with excellent hospitality. Very politely behavior and warmly welcome from your cabin crew and all members.. Rating: 10.0/10. Reviewer: A Kajan. Date: 10th August 2024. Recommended: yes\n",
            "\n",
            "Review 5:\n",
            "Airline: SpiceJet. Title: \"delayed both ways by many hours\". Review: Not Verified |  Flight was delayed both ways by many hours. Poor service for the same price as other airlines like IndiGo. No wifi or other amenities to compensate for terrible service.. Rating: 2.0/10. Reviewer: Somil Jain Jain. Date: 20th May 2022. Recommended: no\u001b[0m"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:\u2705 Query completed in 4.07s\n",
            "INFO:__main__:\ud83e\udde0 Running Phoenix evaluations on 5 responses...\n",
            "INFO:__main__:\ud83d\udccb Evaluation criteria:\n",
            "INFO:__main__:   \ud83d\udd0d Relevance: Does the response address the flight search query?\n",
            "INFO:__main__:   \ud83c\udfaf QA Correctness: Is the flight information accurate and helpful?\n",
            "INFO:__main__:   \ud83d\udea8 Hallucination: Does the response contain fabricated information?\n",
            "INFO:__main__:   \u2620\ufe0f Toxicity: Is the response harmful or inappropriate?\n",
            "INFO:__main__:   \ud83d\udcca Running relevance evaluation...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32;1m\u001b[1;3mThought: I now know the final answer Final Answer: Based on 5 customer reviews, SpiceJet receives mostly positive feedback (averaging 8.4/10) for their courteous crew, warm hospitality, and excellent ground staff service. However, some passengers report flight delays and poor service. The majority of reviews are highly positive (four 10/10 ratings), with one negative review citing delays and lack of amenities.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e0d7b4358a3a4b8780a7ca422bfab550",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "llm_classify |          | 0/5 (0.0%) | \u23f3 00:00<? | ?it/s"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:   \u2705 relevance evaluation completed\n",
            "INFO:__main__:   \ud83d\udcca Running qa_correctness evaluation...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1059b51758cc404ba3acda7f62db308c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "llm_classify |          | 0/5 (0.0%) | \u23f3 00:00<? | ?it/s"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:   \u2705 qa_correctness evaluation completed\n",
            "INFO:__main__:   \ud83d\udcca Running hallucination evaluation...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8932e31ca4fa42f9a1eaf6c8ecf9bcf4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "llm_classify |          | 0/5 (0.0%) | \u23f3 00:00<? | ?it/s"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:   \u2705 hallucination evaluation completed\n",
            "INFO:__main__:   \ud83d\udcca Running toxicity evaluation...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2fc35071b27d46a18a209b3db4932ea6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "llm_classify |          | 0/5 (0.0%) | \u23f3 00:00<? | ?it/s"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:   \u2705 toxicity evaluation completed\n",
            "INFO:__main__:\u2705 Phoenix evaluations completed\n",
            "INFO:__main__:\n",
            "\ud83d\udcca Phoenix Evaluation Summary:\n",
            "INFO:__main__:  Total queries: 5\n",
            "INFO:__main__:  Successful executions: 5\n",
            "INFO:__main__:  Failed executions: 0\n",
            "INFO:__main__:  Average execution time: 3.64s\n",
            "INFO:__main__:\n",
            "\ud83e\udde0 Phoenix Evaluation Results:\n",
            "INFO:__main__:   relevance: {'relevant': np.int64(5)}\n",
            "INFO:__main__:   qa_correctness: {'correct': np.int64(5)}\n",
            "INFO:__main__:   hallucination: {'factual': np.int64(5)}\n",
            "INFO:__main__:   toxicity: {'non-toxic': np.int64(5)}\n",
            "INFO:__main__:\n",
            "\ud83d\udcca Quick Scores Summary:\n",
            "INFO:__main__:   Query 1: \u2705 relevance: relevant | \u2705 qa_correctness: correct | \u2705 hallucination: factual | \u2705 toxicity: non-toxic\n",
            "INFO:__main__:   Query 2: \u2705 relevance: relevant | \u2705 qa_correctness: correct | \u2705 hallucination: factual | \u2705 toxicity: non-toxic\n",
            "INFO:__main__:   Query 3: \u2705 relevance: relevant | \u2705 qa_correctness: correct | \u2705 hallucination: factual | \u2705 toxicity: non-toxic\n",
            "INFO:__main__:   Query 4: \u2705 relevance: relevant | \u2705 qa_correctness: correct | \u2705 hallucination: factual | \u2705 toxicity: non-toxic\n",
            "INFO:__main__:   Query 5: \u2705 relevance: relevant | \u2705 qa_correctness: correct | \u2705 hallucination: factual | \u2705 toxicity: non-toxic\n",
            "INFO:__main__:\n",
            "\ud83d\udcdd DETAILED EVALUATION RESULTS (FULL EXPLANATIONS):\n",
            "INFO:__main__:================================================================================\n",
            "INFO:__main__:\n",
            "\ud83d\udd0d QUERY 1: Find flights from JFK to LAX\n",
            "INFO:__main__:------------------------------------------------------------\n",
            "INFO:__main__:\n",
            "\ud83d\udcca RELEVANCE: relevant\n",
            "INFO:__main__:\ud83d\udcad FULL REASONING:\n",
            "INFO:__main__:The question asks for flights from JFK to LAX. The reference text provides a list of available flights from JFK to LAX, including the airline codes and aircraft types. This information directly answers the question by listing the flights that are available for the specified route.\n",
            "INFO:__main__:----------------------------------------\n",
            "INFO:__main__:\n",
            "\ud83d\udcca QA_CORRECTNESS: correct\n",
            "INFO:__main__:\ud83d\udcad FULL REASONING:\n",
            "INFO:__main__:The agent successfully provided the user with a list of available flights from JFK to LAX, including the airlines and aircraft types. The response matches the core information requested by the user, and the user would be satisfied with the information provided.\n",
            "INFO:__main__:----------------------------------------\n",
            "INFO:__main__:\n",
            "\ud83d\udcca HALLUCINATION: factual\n",
            "INFO:__main__:\ud83d\udcad FULL REASONING:\n",
            "INFO:__main__:The agent's response matches the reference text exactly, listing the same flights and airlines from JFK to LAX. There is no impossible information or contradictions present. The response is consistent with the system's capabilities and does not contain any hallucinated data.\n",
            "INFO:__main__:----------------------------------------\n",
            "INFO:__main__:\n",
            "\ud83d\udcca TOXICITY: non-toxic\n",
            "INFO:__main__:\ud83d\udcad FULL REASONING:\n",
            "INFO:__main__:To determine whether the text is toxic, we need to analyze the content for any signs of hateful language, demeaning statements, inappropriate language, or threats of violence. The text in question is: 'Find flights from JFK to LAX'. This text is a straightforward request or command to find flights between two airports, JFK and LAX. It does not contain any language that could be considered hateful, demeaning, or inappropriate. There are no threats of physical or emotional violence present in the text. The language is neutral and purely informational, focusing on a travel-related task. Therefore, the text does not meet any criteria for toxicity.\n",
            "INFO:__main__:----------------------------------------\n",
            "INFO:__main__:================================================================================\n",
            "INFO:__main__:\n",
            "\ud83d\udd0d QUERY 2: Book a flight from LAX to JFK for tomorrow, 2 passengers, business class\n",
            "INFO:__main__:------------------------------------------------------------\n",
            "INFO:__main__:\n",
            "\ud83d\udcca RELEVANCE: relevant\n",
            "INFO:__main__:\ud83d\udcad FULL REASONING:\n",
            "INFO:__main__:The question asks for booking a flight from LAX to JFK for tomorrow, for 2 passengers in business class. The reference text confirms a flight booking with the same details: a route from LAX to JFK, a departure date set for tomorrow, 2 passengers, and business class. Additionally, it provides a booking ID, total price, and next steps for the flight, which are all relevant to the question. Therefore, the reference text contains information that directly answers the question.\n",
            "INFO:__main__:----------------------------------------\n",
            "INFO:__main__:\n",
            "\ud83d\udcca QA_CORRECTNESS: correct\n",
            "INFO:__main__:\ud83d\udcad FULL REASONING:\n",
            "INFO:__main__:The agent successfully booked a flight from LAX to JFK for 2 passengers in business class for the next day. The response includes a dynamically generated booking ID, the correct route, class, number of passengers, and total price. The additional information about check-in and airport arrival is also provided, which is helpful to the user. The core functionality of booking the flight was completed as requested.\n",
            "INFO:__main__:----------------------------------------\n",
            "INFO:__main__:\n",
            "\ud83d\udcca HALLUCINATION: factual\n",
            "INFO:__main__:\ud83d\udcad FULL REASONING:\n",
            "INFO:__main__:The agent's response contains dynamically generated data such as the booking ID and the departure date, which are expected to differ from the reference text. The response provides plausible flight booking details, including the route, class, number of passengers, and total price, which align with the system's capabilities. There is no clearly impossible information or contradictions present in the response.\n",
            "INFO:__main__:----------------------------------------\n",
            "INFO:__main__:\n",
            "\ud83d\udcca TOXICITY: non-toxic\n",
            "INFO:__main__:\ud83d\udcad FULL REASONING:\n",
            "INFO:__main__:To determine whether the text is toxic, we need to analyze the content for any signs of hateful language, demeaning statements, inappropriate language, or threats of violence. The text in question is a straightforward request to book a flight from LAX to JFK for two passengers in business class. It does not contain any language that could be considered hateful or demeaning. There are no inappropriate words or phrases, nor are there any threats of physical or emotional violence. The text is purely transactional and focuses on travel arrangements without any negative or harmful language. Therefore, the text does not meet the criteria for toxicity.\n",
            "INFO:__main__:----------------------------------------\n",
            "INFO:__main__:================================================================================\n",
            "INFO:__main__:\n",
            "\ud83d\udd0d QUERY 3: Book an economy flight from JFK to MIA for next week, 1 passenger\n",
            "INFO:__main__:------------------------------------------------------------\n",
            "INFO:__main__:\n",
            "\ud83d\udcca RELEVANCE: relevant\n",
            "INFO:__main__:\ud83d\udcad FULL REASONING:\n",
            "INFO:__main__:The question asks for booking an economy flight from JFK to MIA for next week for 1 passenger. The reference text provides confirmation of a flight booking with the same details: a route from JFK to MIA, for 1 passenger, in economy class, and scheduled for next week. This directly matches the request in the question, indicating that the reference text contains the information needed to answer the question.\n",
            "INFO:__main__:----------------------------------------\n",
            "INFO:__main__:\n",
            "\ud83d\udcca QA_CORRECTNESS: correct\n",
            "INFO:__main__:\ud83d\udcad FULL REASONING:\n",
            "INFO:__main__:The agent successfully booked an economy flight from JFK to MIA for next week, as requested by the user. The response includes all necessary details such as the booking ID, route, departure date, number of passengers, class, and total price. The next steps provided are also helpful for the user. The dynamic elements like booking ID and date are correctly handled, and the core functionality of booking the flight is achieved.\n",
            "INFO:__main__:----------------------------------------\n",
            "INFO:__main__:\n",
            "\ud83d\udcca HALLUCINATION: factual\n",
            "INFO:__main__:\ud83d\udcad FULL REASONING:\n",
            "INFO:__main__:The response contains dynamically generated data such as the booking ID and the departure date, which are expected to differ from the reference text. The flight details, including the route, class, and price, are plausible and consistent with the system's capabilities. There is no indication of impossible information or contradictions in the response.\n",
            "INFO:__main__:----------------------------------------\n",
            "INFO:__main__:\n",
            "\ud83d\udcca TOXICITY: non-toxic\n",
            "INFO:__main__:\ud83d\udcad FULL REASONING:\n",
            "INFO:__main__:To determine whether the text is toxic, we need to analyze the content for any signs of hateful language, demeaning statements, inappropriate language, or threats of violence. The text provided is a straightforward request to book a flight from JFK to MIA for one passenger next week. It does not contain any language that could be considered hateful or demeaning. There are no inappropriate words or phrases, nor are there any threats of physical or emotional violence. The text is purely transactional and neutral in tone, focusing solely on the logistics of booking a flight. Therefore, it does not meet the criteria for toxicity.\n",
            "INFO:__main__:----------------------------------------\n",
            "INFO:__main__:================================================================================\n",
            "INFO:__main__:\n",
            "\ud83d\udd0d QUERY 4: Show me my current flight bookings\n",
            "INFO:__main__:------------------------------------------------------------\n",
            "INFO:__main__:\n",
            "\ud83d\udcca RELEVANCE: relevant\n",
            "INFO:__main__:\ud83d\udcad FULL REASONING:\n",
            "INFO:__main__:The question asks for the user's current flight bookings. The reference text provides detailed information about two current flight bookings, including booking IDs, routes, dates, number of passengers, class, total cost, status, and booking date. This information directly answers the question by listing the current flight bookings, making the reference text relevant to the question.\n",
            "INFO:__main__:----------------------------------------\n",
            "INFO:__main__:\n",
            "\ud83d\udcca QA_CORRECTNESS: correct\n",
            "INFO:__main__:\ud83d\udcad FULL REASONING:\n",
            "INFO:__main__:The agent successfully provided the user with their current flight bookings, including details such as booking ID, route, date, number of passengers, class, total cost, status, and booking date. The core information is accurate and helpful, and the user would be satisfied with the response. The differences in booking IDs and dates are expected due to dynamic data generation.\n",
            "INFO:__main__:----------------------------------------\n",
            "INFO:__main__:\n",
            "\ud83d\udcca HALLUCINATION: factual\n",
            "INFO:__main__:\ud83d\udcad FULL REASONING:\n",
            "INFO:__main__:The agent's response contains dynamic data such as booking IDs and dates, which are expected to differ from the reference text. The flight routes, passenger details, class, and total amounts are consistent with plausible flight booking information. There is no indication of impossible information or contradictions with system capabilities.\n",
            "INFO:__main__:----------------------------------------\n",
            "INFO:__main__:\n",
            "\ud83d\udcca TOXICITY: non-toxic\n",
            "INFO:__main__:\ud83d\udcad FULL REASONING:\n",
            "INFO:__main__:To determine whether the text is toxic, we need to analyze it based on the definition of toxicity provided. The text in question is: 'Show me my current flight bookings'.\n",
            "\n",
            "1. **Hateful Statements**: The text does not contain any language that could be interpreted as hateful towards any individual or group.\n",
            "\n",
            "2. **Demeaning or Disparaging Language**: There is no language in the text that demeans or disparages anyone. It is a straightforward request for information.\n",
            "\n",
            "3. **Inappropriate Language**: The text does not use any inappropriate or offensive language. It is polite and neutral.\n",
            "\n",
            "4. **Threats of Physical or Emotional Violence**: There are no threats, either physical or emotional, present in the text.\n",
            "\n",
            "Given these points, the text does not meet any of the criteria for being considered toxic. It is a simple, non-confrontational request for information about flight bookings.\n",
            "INFO:__main__:----------------------------------------\n",
            "INFO:__main__:================================================================================\n",
            "INFO:__main__:\n",
            "\ud83d\udd0d QUERY 5: What do passengers say about SpiceJet's service quality?\n",
            "INFO:__main__:------------------------------------------------------------\n",
            "INFO:__main__:\n",
            "\ud83d\udcca RELEVANCE: relevant\n",
            "INFO:__main__:\ud83d\udcad FULL REASONING:\n",
            "INFO:__main__:The question asks about passengers' opinions on SpiceJet's service quality. The reference text provides a summary of reviews that specifically address SpiceJet's service quality, including aspects such as crew service, flight punctuality, in-flight amenities, ground staff service, and overall travel experience. These reviews include both positive and negative feedback, which directly relates to what passengers say about the service quality. Therefore, the reference text contains relevant information that can help answer the question.\n",
            "INFO:__main__:----------------------------------------\n",
            "INFO:__main__:\n",
            "\ud83d\udcca QA_CORRECTNESS: correct\n",
            "INFO:__main__:\ud83d\udcad FULL REASONING:\n",
            "INFO:__main__:The agent successfully provided the requested information about SpiceJet's service quality by presenting five relevant reviews. These reviews cover various aspects such as crew service, hospitality, flight delays, and overall travel experience. The reviews include both positive and negative feedback, which aligns with the user's request for information on service quality. The dynamic nature of the reviews and ratings is expected, and the agent's response is consistent with the core functionality of retrieving and summarizing customer feedback.\n",
            "INFO:__main__:----------------------------------------\n",
            "INFO:__main__:\n",
            "\ud83d\udcca HALLUCINATION: factual\n",
            "INFO:__main__:\ud83d\udcad FULL REASONING:\n",
            "INFO:__main__:The agent's response is consistent with the reference text, which mentions a mix of positive and negative reviews for SpiceJet's service quality. The reviews in the agent's response cover aspects such as crew service, hospitality, and flight delays, which align with the reference text. The dynamic data such as reviewer names, dates, and specific review content are expected to vary due to the nature of the data retrieval process. Therefore, the response is factual.\n",
            "INFO:__main__:----------------------------------------\n",
            "INFO:__main__:\n",
            "\ud83d\udcca TOXICITY: non-toxic\n",
            "INFO:__main__:\ud83d\udcad FULL REASONING:\n",
            "INFO:__main__:The text in question is a simple inquiry about passengers' opinions on SpiceJet's service quality. It does not contain any language that is hateful, demeaning, or disparaging. There are no inappropriate words or phrases, nor is there any suggestion of physical or emotional violence. The text is neutral and simply seeks information about customer feedback. Therefore, it does not meet the criteria for toxicity.\n",
            "INFO:__main__:----------------------------------------\n",
            "INFO:__main__:================================================================================\n",
            "INFO:__main__:\ud83d\udcca Creating Arize dataset...\n",
            "INFO:__main__:\u2705 Arize dataset created successfully: RGF0YXNldDozMjM4MDA6dTBJNg==\n",
            "INFO:__main__:\ud83d\udcca Arize dataset created: RGF0YXNldDozMjM4MDA6dTBJNg==\n",
            "INFO:__main__:\n",
            "\u2705 Phoenix evaluation complete!\n",
            "INFO:__main__:\ud83d\udd12 Phoenix cleanup completed\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\ud83d\udcca EVALUATION RESULTS:\n",
            "==================================================\n",
            "\n",
            "\ud83d\udd0d Query 1: Find flights from JFK to LAX\n",
            "\u23f1\ufe0f Execution time: 2.86s\n",
            "\u2705 Success: True\n",
            "   \u2705 Relevance: relevant\n",
            "   \u2705 Qa_Correctness: correct\n",
            "   \u2705 Hallucination: factual\n",
            "   \u2705 Toxicity: non-toxic\n",
            "\n",
            "\ud83d\udd0d Query 2: Book a flight from LAX to JFK for tomorrow, 2 passengers, business class\n",
            "\u23f1\ufe0f Execution time: 4.11s\n",
            "\u2705 Success: True\n",
            "   \u2705 Relevance: relevant\n",
            "   \u2705 Qa_Correctness: correct\n",
            "   \u2705 Hallucination: factual\n",
            "   \u2705 Toxicity: non-toxic\n",
            "\n",
            "\ud83d\udd0d Query 3: Book an economy flight from JFK to MIA for next week, 1 passenger\n",
            "\u23f1\ufe0f Execution time: 4.05s\n",
            "\u2705 Success: True\n",
            "   \u2705 Relevance: relevant\n",
            "   \u2705 Qa_Correctness: correct\n",
            "   \u2705 Hallucination: factual\n",
            "   \u2705 Toxicity: non-toxic\n",
            "\n",
            "\ud83d\udd0d Query 4: Show me my current flight bookings\n",
            "\u23f1\ufe0f Execution time: 3.11s\n",
            "\u2705 Success: True\n",
            "   \u2705 Relevance: relevant\n",
            "   \u2705 Qa_Correctness: correct\n",
            "   \u2705 Hallucination: factual\n",
            "   \u2705 Toxicity: non-toxic\n",
            "\n",
            "\ud83d\udd0d Query 5: What do passengers say about SpiceJet's service quality?\n",
            "\u23f1\ufe0f Execution time: 4.07s\n",
            "\u2705 Success: True\n",
            "   \u2705 Relevance: relevant\n",
            "   \u2705 Qa_Correctness: correct\n",
            "   \u2705 Hallucination: factual\n",
            "   \u2705 Toxicity: non-toxic\n",
            "\n",
            "\ud83c\udf89 Evaluation completed! Check Phoenix UI for detailed traces.\n"
          ]
        }
      ],
      "source": [
        "# Run the evaluation\n",
        "if __name__ == \"__main__\":\n",
        "    # Choose evaluation mode:\n",
        "\n",
        "    # Option 1: Run demo with 2 queries (quick test)\n",
        "    # results = run_phoenix_demo()\n",
        "\n",
        "    # Option 2: Run full evaluation with all queries\n",
        "    results = run_full_evaluation()\n",
        "\n",
        "    # Display results\n",
        "    if results is not None and len(results) > 0:\n",
        "        print(\"\\n\ud83d\udcca EVALUATION RESULTS:\")\n",
        "        print(\"=\" * 50)\n",
        "        for i, row in results.iterrows():\n",
        "            print(f\"\\n\ud83d\udd0d Query {i+1}: {row['query']}\")\n",
        "            print(f\"\u23f1\ufe0f Execution time: {row['execution_time']:.2f}s\")\n",
        "            print(f\"\u2705 Success: {row['success']}\")\n",
        "\n",
        "            # Show evaluation scores if available\n",
        "            eval_types = [\"relevance\", \"qa_correctness\", \"hallucination\", \"toxicity\"]\n",
        "            for eval_type in eval_types:\n",
        "                if eval_type in row:\n",
        "                    score = row[eval_type]\n",
        "                    emoji = \"\u2705\" if score in [\"relevant\", \"correct\", \"factual\", \"non-toxic\"] else \"\u274c\"\n",
        "                    print(f\"   {emoji} {eval_type.title()}: {score}\")\n",
        "\n",
        "        print(\"\\n\ud83c\udf89 Evaluation completed! Check Phoenix UI for detailed traces.\")\n",
        "    else:\n",
        "        print(\"\u274c No evaluation results generated.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aLNxJYomTC-",
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Summary\n",
        "\n",
        "This self-contained notebook demonstrates a complete flight search agent implementation using LangGraph framework with ReAct agents, Couchbase vector store for airline reviews, and Priority 1 Capella AI services. The agent handles flight search, booking, retrieval, and airline review queries with exact functionality from the working Python files. All components are inline and the notebook runs as a complete workflow when executed cell by cell. Set up environment variables (CB_*, CAPELLA_API_*) and dependencies, then execute all cells sequentially.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}