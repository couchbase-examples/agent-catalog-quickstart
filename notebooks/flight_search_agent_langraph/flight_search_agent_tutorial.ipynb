{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Flight Search Agent Tutorial\n",
        "\n",
        "This notebook demonstrates the Agent Catalog flight search agent using LangGraph with Couchbase vector store and Arize evaluation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Setup and Imports\n",
        "\n",
        "Import all necessary modules for the flight search agent.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import base64\n",
        "import getpass\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "from datetime import timedelta\n",
        "\n",
        "import agentc\n",
        "import agentc_langgraph.agent\n",
        "import agentc_langgraph.graph\n",
        "import dotenv\n",
        "import langchain_core.messages\n",
        "import langchain_core.runnables\n",
        "import langchain_openai.chat_models\n",
        "import langgraph.graph\n",
        "import requests\n",
        "from couchbase.auth import PasswordAuthenticator\n",
        "from couchbase.cluster import Cluster\n",
        "from couchbase.management.buckets import BucketType, CreateBucketSettings\n",
        "from couchbase.management.search import SearchIndex\n",
        "from couchbase.options import ClusterOptions\n",
        "from langchain.agents import AgentExecutor, create_react_agent\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.tools import Tool\n",
        "from langchain_couchbase.vectorstores import CouchbaseVectorStore\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from pydantic import SecretStr\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Suppress verbose logging\n",
        "logging.getLogger(\"openai\").setLevel(logging.WARNING)\n",
        "logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n",
        "logging.getLogger(\"httpcore\").setLevel(logging.WARNING)\n",
        "logging.getLogger(\"agentc_core\").setLevel(logging.WARNING)\n",
        "\n",
        "# Load environment variables\n",
        "dotenv.load_dotenv(override=True)\n",
        "\n",
        "\n",
        "def setup_capella_ai_config():\n",
        "    \"\"\"Setup Capella AI configuration - requires environment variables to be set.\"\"\"\n",
        "    # Verify required environment variables are set (no defaults)\n",
        "    required_capella_vars = [\n",
        "        \"CB_USERNAME\",\n",
        "        \"CB_PASSWORD\", \n",
        "        \"CAPELLA_API_ENDPOINT\",\n",
        "        \"CAPELLA_API_EMBEDDING_MODEL\",\n",
        "        \"CAPELLA_API_LLM_MODEL\",\n",
        "    ]\n",
        "    missing_vars = [var for var in required_capella_vars if not os.getenv(var)]\n",
        "    if missing_vars:\n",
        "        raise ValueError(\n",
        "            f\"Missing required Capella AI environment variables: {missing_vars}\"\n",
        "        )\n",
        "\n",
        "    return {\n",
        "        \"endpoint\": os.getenv(\"CAPELLA_API_ENDPOINT\"),\n",
        "        \"embedding_model\": os.getenv(\"CAPELLA_API_EMBEDDING_MODEL\"),\n",
        "        \"llm_model\": os.getenv(\"CAPELLA_API_LLM_MODEL\"),\n",
        "    }\n",
        "\n",
        "\n",
        "def test_capella_connectivity():\n",
        "    \"\"\"Test connectivity to Capella AI services.\"\"\"\n",
        "    try:\n",
        "        endpoint = os.getenv(\"CAPELLA_API_ENDPOINT\")\n",
        "        if not endpoint:\n",
        "            logger.warning(\"CAPELLA_API_ENDPOINT not configured\")\n",
        "            return False\n",
        "\n",
        "        # Test embedding model (requires API key)\n",
        "        if os.getenv(\"CB_USERNAME\") and os.getenv(\"CB_PASSWORD\"):\n",
        "            api_key = base64.b64encode(\n",
        "                f\"{os.getenv('CB_USERNAME')}:{os.getenv('CB_PASSWORD')}\".encode()\n",
        "            ).decode()\n",
        "\n",
        "            headers = {\n",
        "                \"Authorization\": f\"Basic {api_key}\",\n",
        "                \"Content-Type\": \"application/json\",\n",
        "            }\n",
        "\n",
        "            # Test embedding\n",
        "            logger.info(\"Testing Capella AI connectivity...\")\n",
        "            embedding_data = {\n",
        "                \"model\": os.getenv(\n",
        "                    \"CAPELLA_API_EMBEDDING_MODEL\", \"intfloat/e5-mistral-7b-instruct\"\n",
        "                ),\n",
        "                \"input\": \"test connectivity\",\n",
        "            }\n",
        "\n",
        "            response = requests.post(\n",
        "                f\"{endpoint}/embeddings\", json=embedding_data, headers=headers\n",
        "            )\n",
        "            if response.status_code == 200:\n",
        "                logger.info(\"✅ Capella AI embedding test successful\")\n",
        "                return True\n",
        "            else:\n",
        "                logger.warning(f\"❌ Capella AI embedding test failed: {response.text}\")\n",
        "                return False\n",
        "        else:\n",
        "            logger.warning(\"Capella AI credentials not available\")\n",
        "            return False\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"❌ Capella AI connectivity test failed: {e}\")\n",
        "        return False\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Environment Setup\n",
        "\n",
        "Setup environment variables and configuration.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _set_if_undefined(env_var: str, default_value: str = None):\n",
        "    \"\"\"Set environment variable if not already defined.\"\"\"\n",
        "    if not os.getenv(env_var):\n",
        "        if default_value is None:\n",
        "            value = getpass.getpass(f\"Enter {env_var}: \")\n",
        "        else:\n",
        "            value = default_value\n",
        "        os.environ[env_var] = value\n",
        "\n",
        "\n",
        "def setup_environment():\n",
        "    \"\"\"Setup required environment variables with defaults.\"\"\"\n",
        "    logger.info(\"Setting up environment variables...\")\n",
        "    \n",
        "    required_vars = [\n",
        "        \"CB_CONN_STRING\",\n",
        "        \"CB_USERNAME\", \n",
        "        \"CB_PASSWORD\",\n",
        "        \"CB_BUCKET\",\n",
        "        \"AGENT_CATALOG_CONN_STRING\",\n",
        "        \"AGENT_CATALOG_USERNAME\",\n",
        "        \"AGENT_CATALOG_PASSWORD\",\n",
        "        \"AGENT_CATALOG_BUCKET\",\n",
        "    ]\n",
        "    for var in required_vars:\n",
        "        _set_if_undefined(var)\n",
        "\n",
        "    # Set non-sensitive defaults\n",
        "    non_sensitive_defaults = {\n",
        "        \"CB_BUCKET\": \"travel-sample\",\n",
        "        \"AGENT_CATALOG_CONN_STRING\": \"couchbase://127.0.0.1\",\n",
        "        \"AGENT_CATALOG_USERNAME\": \"Administrator\",\n",
        "        \"AGENT_CATALOG_PASSWORD\": \"password\",\n",
        "        \"AGENT_CATALOG_BUCKET\": \"travel-sample\",\n",
        "    }\n",
        "\n",
        "    for key, default_value in non_sensitive_defaults.items():\n",
        "        if not os.environ.get(key):\n",
        "            os.environ[key] = input(f\"Enter {key} (default: {default_value}): \") or default_value\n",
        "\n",
        "    os.environ[\"CB_INDEX\"] = os.getenv(\"CB_INDEX\", \"airline_reviews_index\")\n",
        "    os.environ[\"CB_SCOPE\"] = os.getenv(\"CB_SCOPE\", \"agentc_data\")\n",
        "    os.environ[\"CB_COLLECTION\"] = os.getenv(\"CB_COLLECTION\", \"airline_reviews\")\n",
        "    \n",
        "    # Optional Capella AI configuration\n",
        "    if os.getenv(\"CAPELLA_API_ENDPOINT\"):\n",
        "        # Ensure endpoint has /v1 suffix for OpenAI compatibility\n",
        "        if not os.getenv(\"CAPELLA_API_ENDPOINT\").endswith(\"/v1\"):\n",
        "            os.environ[\"CAPELLA_API_ENDPOINT\"] = (\n",
        "                os.getenv(\"CAPELLA_API_ENDPOINT\").rstrip(\"/\") + \"/v1\"\n",
        "            )\n",
        "            logger.info(\n",
        "                f\"Added /v1 suffix to endpoint: {os.getenv('CAPELLA_API_ENDPOINT')}\"\n",
        "            )\n",
        "\n",
        "    # Test Capella AI connectivity\n",
        "    test_capella_connectivity()\n",
        "\n",
        "\n",
        "setup_environment()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## CouchbaseClient Class\n",
        "\n",
        "Define the CouchbaseClient for all database operations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CouchbaseClient:\n",
        "    \"\"\"Centralized Couchbase client for all database operations.\"\"\"\n",
        "\n",
        "    def __init__(self, conn_string: str, username: str, password: str, bucket_name: str):\n",
        "        \"\"\"Initialize Couchbase client with connection details.\"\"\"\n",
        "        self.conn_string = conn_string\n",
        "        self.username = username\n",
        "        self.password = password\n",
        "        self.bucket_name = bucket_name\n",
        "        self.cluster = None\n",
        "        self.bucket = None\n",
        "        self._collections = {}\n",
        "\n",
        "    def connect(self):\n",
        "        \"\"\"Establish connection to Couchbase cluster.\"\"\"\n",
        "        try:\n",
        "            auth = PasswordAuthenticator(self.username, self.password)\n",
        "            options = ClusterOptions(auth)\n",
        "            self.cluster = Cluster(self.conn_string, options)\n",
        "            self.cluster.wait_until_ready(timedelta(seconds=10))\n",
        "            logger.info(\"Successfully connected to Couchbase\")\n",
        "            return self.cluster\n",
        "        except Exception as e:\n",
        "            raise ConnectionError(f\"Failed to connect to Couchbase: {e!s}\")\n",
        "\n",
        "    def setup_collection(self, scope_name: str, collection_name: str):\n",
        "        \"\"\"Setup bucket, scope and collection all in one function.\"\"\"\n",
        "        try:\n",
        "            if not self.cluster:\n",
        "                self.connect()\n",
        "\n",
        "            if not self.bucket and self.cluster is not None:\n",
        "                try:\n",
        "                    self.bucket = self.cluster.bucket(self.bucket_name)\n",
        "                    logger.info(f\"Bucket '{self.bucket_name}' exists\")\n",
        "                except Exception:\n",
        "                    logger.info(f\"Creating bucket '{self.bucket_name}'...\")\n",
        "                    bucket_settings = CreateBucketSettings(\n",
        "                        name=self.bucket_name,\n",
        "                        bucket_type=BucketType.COUCHBASE,\n",
        "                        ram_quota_mb=1024,\n",
        "                        flush_enabled=True,\n",
        "                        num_replicas=0,\n",
        "                    )\n",
        "                    self.cluster.buckets().create_bucket(bucket_settings)\n",
        "                    time.sleep(5)\n",
        "                    self.bucket = self.cluster.bucket(self.bucket_name)\n",
        "                    logger.info(f\"Bucket '{self.bucket_name}' created successfully\")\n",
        "\n",
        "            if not self.bucket:\n",
        "                raise RuntimeError(\"Failed to initialize bucket\")\n",
        "\n",
        "            bucket_manager = self.bucket.collections()\n",
        "            scopes = bucket_manager.get_all_scopes()\n",
        "            scope_exists = any(scope.name == scope_name for scope in scopes)\n",
        "\n",
        "            if not scope_exists and scope_name != \"_default\":\n",
        "                logger.info(f\"Creating scope '{scope_name}'...\")\n",
        "                bucket_manager.create_scope(scope_name)\n",
        "                logger.info(f\"Scope '{scope_name}' created successfully\")\n",
        "\n",
        "            collections = bucket_manager.get_all_scopes()\n",
        "            collection_exists = any(\n",
        "                scope.name == scope_name\n",
        "                and collection_name in [col.name for col in scope.collections]\n",
        "                for scope in collections\n",
        "            )\n",
        "\n",
        "            if not collection_exists:\n",
        "                logger.info(f\"Creating collection '{collection_name}'...\")\n",
        "                bucket_manager.create_collection(scope_name, collection_name)\n",
        "                logger.info(f\"Collection '{collection_name}' created successfully\")\n",
        "\n",
        "            collection = self.bucket.scope(scope_name).collection(collection_name)\n",
        "            time.sleep(3)\n",
        "\n",
        "            if self.cluster:\n",
        "                try:\n",
        "                    self.cluster.query(\n",
        "                        f\"CREATE PRIMARY INDEX IF NOT EXISTS ON `{self.bucket_name}`.`{scope_name}`.`{collection_name}`\"\n",
        "                    ).execute()\n",
        "                    logger.info(\"Primary index created successfully\")\n",
        "                except Exception as e:\n",
        "                    logger.warning(f\"Error creating primary index: {e!s}\")\n",
        "\n",
        "            collection_key = f\"{scope_name}.{collection_name}\"\n",
        "            self._collections[collection_key] = collection\n",
        "\n",
        "            logger.info(f\"Collection setup complete for {scope_name}.{collection_name}\")\n",
        "            return collection\n",
        "\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Error setting up collection: {e!s}\")\n",
        "\n",
        "    def get_collection(self, scope_name: str, collection_name: str):\n",
        "        \"\"\"Get a collection, creating it if it doesn't exist.\"\"\"\n",
        "        collection_key = f\"{scope_name}.{collection_name}\"\n",
        "        if collection_key not in self._collections:\n",
        "            self.setup_collection(scope_name, collection_name)\n",
        "        return self._collections[collection_key]\n",
        "\n",
        "    def setup_vector_search_index(self, index_definition: dict, scope_name: str):\n",
        "        \"\"\"Setup vector search index for the specified scope.\"\"\"\n",
        "        try:\n",
        "            if not self.bucket:\n",
        "                raise RuntimeError(\"Bucket not initialized. Call setup_collection first.\")\n",
        "\n",
        "            scope_index_manager = self.bucket.scope(scope_name).search_indexes()\n",
        "            existing_indexes = scope_index_manager.get_all_indexes()\n",
        "            index_name = index_definition[\"name\"]\n",
        "\n",
        "            if index_name not in [index.name for index in existing_indexes]:\n",
        "                logger.info(f\"Creating vector search index '{index_name}'...\")\n",
        "                search_index = SearchIndex.from_json(index_definition)\n",
        "                scope_index_manager.upsert_index(search_index)\n",
        "                logger.info(f\"Vector search index '{index_name}' created successfully\")\n",
        "            else:\n",
        "                logger.info(f\"Vector search index '{index_name}' already exists\")\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Error setting up vector search index: {e!s}\")\n",
        "\n",
        "    def setup_vector_store(\n",
        "        self, scope_name: str, collection_name: str, index_name: str, embeddings\n",
        "    ):\n",
        "        \"\"\"Setup vector store with airline reviews data.\"\"\"\n",
        "        try:\n",
        "            if not self.cluster:\n",
        "                raise RuntimeError(\"Cluster not connected. Call connect first.\")\n",
        "\n",
        "            # Import the unified data manager\n",
        "            sys.path.append(os.path.join(os.getcwd(), \"data\"))\n",
        "            from airline_reviews_data import load_airline_reviews_to_couchbase\n",
        "\n",
        "            logger.info(\"🔄 Setting up vector store with airline reviews data...\")\n",
        "\n",
        "            # Load airline reviews data\n",
        "            load_airline_reviews_to_couchbase(\n",
        "                cluster=self.cluster,\n",
        "                bucket_name=self.bucket_name,\n",
        "                scope_name=scope_name,\n",
        "                collection_name=collection_name,\n",
        "                embeddings=embeddings,\n",
        "                index_name=index_name,\n",
        "            )\n",
        "\n",
        "            # Create and return the vector store instance\n",
        "            vector_store = CouchbaseVectorStore(\n",
        "                cluster=self.cluster,\n",
        "                bucket_name=self.bucket_name,\n",
        "                scope_name=scope_name,\n",
        "                collection_name=collection_name,\n",
        "                embedding=embeddings,\n",
        "                index_name=index_name,\n",
        "            )\n",
        "\n",
        "            logger.info(\n",
        "                f\"✅ Vector store setup complete: {self.bucket_name}.{scope_name}.{collection_name}\"\n",
        "            )\n",
        "            return vector_store\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.exception(f\"Error setting up vector store: {e!s}\")\n",
        "            raise\n",
        "\n",
        "    def clear_scope(self, scope_name: str):\n",
        "        \"\"\"Clear all collections in the specified scope.\"\"\"\n",
        "        try:\n",
        "            if not self.bucket:\n",
        "                if not self.cluster:\n",
        "                    self.connect()\n",
        "                if self.cluster:\n",
        "                    self.bucket = self.cluster.bucket(self.bucket_name)\n",
        "\n",
        "            if not self.bucket:\n",
        "                logger.warning(\"Cannot clear scope - bucket not available\")\n",
        "                return\n",
        "\n",
        "            logger.info(f\"🗑️  Clearing scope: {self.bucket_name}.{scope_name}\")\n",
        "            bucket_manager = self.bucket.collections()\n",
        "            scopes = bucket_manager.get_all_scopes()\n",
        "\n",
        "            target_scope = None\n",
        "            for scope in scopes:\n",
        "                if scope.name == scope_name:\n",
        "                    target_scope = scope\n",
        "                    break\n",
        "\n",
        "            if not target_scope:\n",
        "                logger.info(\n",
        "                    f\"Scope '{self.bucket_name}.{scope_name}' does not exist, nothing to clear\"\n",
        "                )\n",
        "                return\n",
        "\n",
        "            for collection in target_scope.collections:\n",
        "                try:\n",
        "                    delete_query = (\n",
        "                        f\"DELETE FROM `{self.bucket_name}`.`{scope_name}`.`{collection.name}`\"\n",
        "                    )\n",
        "                    if self.cluster:\n",
        "                        self.cluster.query(delete_query).execute()\n",
        "                        logger.info(\n",
        "                            f\"✅ Cleared collection: {self.bucket_name}.{scope_name}.{collection.name}\"\n",
        "                        )\n",
        "                except Exception as e:\n",
        "                    logger.warning(\n",
        "                        f\"❌ Could not clear collection {self.bucket_name}.{scope_name}.{collection.name}: {e}\"\n",
        "                    )\n",
        "\n",
        "            logger.info(f\"✅ Completed clearing scope: {self.bucket_name}.{scope_name}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"❌ Could not clear scope {self.bucket_name}.{scope_name}: {e}\")\n",
        "\n",
        "    def clear_collection(self, scope_name: str, collection_name: str):\n",
        "        \"\"\"Clear a specific collection in the specified scope.\"\"\"\n",
        "        try:\n",
        "            if not self.bucket:\n",
        "                if not self.cluster:\n",
        "                    self.connect()\n",
        "                if self.cluster:\n",
        "                    self.bucket = self.cluster.bucket(self.bucket_name)\n",
        "\n",
        "            if not self.bucket:\n",
        "                logger.warning(f\"Cannot clear collection - bucket not available\")\n",
        "                return\n",
        "\n",
        "            logger.info(\n",
        "                f\"🗑️  Clearing collection: {self.bucket_name}.{scope_name}.{collection_name}\"\n",
        "            )\n",
        "\n",
        "            try:\n",
        "                delete_query = (\n",
        "                    f\"DELETE FROM `{self.bucket_name}`.`{scope_name}`.`{collection_name}`\"\n",
        "                )\n",
        "                if self.cluster:\n",
        "                    result = self.cluster.query(delete_query).execute()\n",
        "                    logger.info(\n",
        "                        f\"✅ Cleared collection: {self.bucket_name}.{scope_name}.{collection_name}\"\n",
        "                    )\n",
        "            except Exception as e:\n",
        "                logger.info(\n",
        "                    f\"Collection {self.bucket_name}.{scope_name}.{collection_name} does not exist or is already empty: {e}\"\n",
        "                )\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.exception(f\"❌ Error clearing collection {scope_name}.{collection_name}: {e}\")\n",
        "            raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Agent Classes\n",
        "\n",
        "Define the FlightSearchAgent and FlightSearchGraph classes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FlightSearchState(agentc_langgraph.agent.State):\n",
        "    \"\"\"State for flight search conversations - single user system.\"\"\"\n",
        "\n",
        "    query: str\n",
        "    resolved: bool\n",
        "    search_results: list[dict]\n",
        "\n",
        "\n",
        "class FlightSearchAgent(agentc_langgraph.agent.ReActAgent):\n",
        "    \"\"\"Flight search agent using Agent Catalog tools and ReActAgent framework.\"\"\"\n",
        "\n",
        "    def __init__(self, catalog: agentc.Catalog, span: agentc.Span, chat_model=None):\n",
        "        \"\"\"Initialize the flight search agent.\"\"\"\n",
        "\n",
        "        if chat_model is None:\n",
        "            # Fallback to OpenAI if no chat model provided\n",
        "            model_name = os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\")\n",
        "            chat_model = langchain_openai.chat_models.ChatOpenAI(model=model_name, temperature=0.1)\n",
        "\n",
        "        super().__init__(\n",
        "            chat_model=chat_model, catalog=catalog, span=span, prompt_name=\"flight_search_assistant\"\n",
        "        )\n",
        "\n",
        "    def _invoke(\n",
        "        self,\n",
        "        span: agentc.Span,\n",
        "        state: FlightSearchState,\n",
        "        config: langchain_core.runnables.RunnableConfig,\n",
        "    ) -> FlightSearchState:\n",
        "        \"\"\"Handle flight search conversation using ReActAgent.\"\"\"\n",
        "\n",
        "        if not state[\"messages\"]:\n",
        "            initial_msg = langchain_core.messages.HumanMessage(content=state[\"query\"])\n",
        "            state[\"messages\"].append(initial_msg)\n",
        "            logger.info(f\"Flight Query: {state['query']}\")\n",
        "\n",
        "        # Get prompt resource first\n",
        "        prompt_resource = self.catalog.find(\"prompt\", name=\"flight_search_assistant\")\n",
        "\n",
        "        # Get tools from Agent Catalog\n",
        "        tools = []\n",
        "        tool_names = [\n",
        "            \"lookup_flight_info\",\n",
        "            \"save_flight_booking\",\n",
        "            \"retrieve_flight_bookings\",\n",
        "            \"search_airline_reviews\",\n",
        "        ]\n",
        "\n",
        "        for tool_name in tool_names:\n",
        "            catalog_tool = None\n",
        "            try:\n",
        "                catalog_tool = self.catalog.find(\"tool\", name=tool_name)\n",
        "                if catalog_tool:\n",
        "                    logger.info(f\"✅ Found tool by name: {tool_name}\")\n",
        "                else:\n",
        "                    logger.warning(\n",
        "                        f\"⚠️  Tool not found by name: {tool_name}, trying prompt fallback\"\n",
        "                    )\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"❌ Failed to find tool by name {tool_name}: {e}\")\n",
        "\n",
        "            # If tool not found by name, try fallback through prompt\n",
        "            if not catalog_tool:\n",
        "                try:\n",
        "                    logger.info(f\"🔄 Trying prompt fallback for tool: {tool_name}\")\n",
        "                    if prompt_resource:\n",
        "                        prompt_tools = getattr(prompt_resource, \"tools\", [])\n",
        "                        for prompt_tool in prompt_tools:\n",
        "                            tool_meta_name = (\n",
        "                                getattr(prompt_tool.meta, \"name\", \"\")\n",
        "                                if hasattr(prompt_tool, \"meta\")\n",
        "                                else \"\"\n",
        "                            )\n",
        "                            if tool_meta_name == tool_name:\n",
        "                                catalog_tool = prompt_tool\n",
        "                                logger.info(f\"✅ Found tool through prompt: {tool_name}\")\n",
        "                                break\n",
        "\n",
        "                    if not catalog_tool:\n",
        "                        logger.error(f\"❌ Tool {tool_name} not found by name or through prompt\")\n",
        "                        continue\n",
        "                except Exception as e:\n",
        "                    logger.error(f\"❌ Prompt fallback failed for tool {tool_name}: {e}\")\n",
        "                    continue\n",
        "\n",
        "            # Create wrapper function to handle proper parameter parsing\n",
        "            def create_tool_wrapper(original_tool, name):\n",
        "                def wrapper_func(tool_input: str) -> str:\n",
        "                    \"\"\"Wrapper to handle proper parameter parsing for each tool.\"\"\"\n",
        "                    try:\n",
        "                        logger.info(f\"🔧 Tool {name} called with input: '{tool_input}'\")\n",
        "\n",
        "                        # Parse input based on tool requirements\n",
        "                        if name == \"lookup_flight_info\":\n",
        "                            # Expected format: \"JFK,LAX\" - parse and pass as separate parameters\n",
        "                            parts = tool_input.replace(\" to \", \",\").replace(\"from \", \"\").split(\",\")\n",
        "                            if len(parts) >= 2:\n",
        "                                source_airport = parts[0].strip()\n",
        "                                destination_airport = parts[1].strip()\n",
        "                                result = original_tool.func(\n",
        "                                    source_airport=source_airport,\n",
        "                                    destination_airport=destination_airport,\n",
        "                                )\n",
        "                            else:\n",
        "                                return f\"Error: lookup_flight_info requires format 'SOURCE,DESTINATION' (e.g., 'JFK,LAX')\"\n",
        "\n",
        "                        elif name == \"save_flight_booking\":\n",
        "                            # Pass the full string directly - tool expects \"source,dest,date\" format\n",
        "                            result = original_tool.func(booking_input=tool_input)\n",
        "\n",
        "                        elif name == \"retrieve_flight_bookings\":\n",
        "                            # Pass the full string directly - tool expects string input\n",
        "                            result = original_tool.func(booking_query=tool_input)\n",
        "\n",
        "                        elif name == \"search_airline_reviews\":\n",
        "                            # Pass the full string directly - tool expects query string\n",
        "                            result = original_tool.func(query=tool_input)\n",
        "\n",
        "                        else:\n",
        "                            # Generic fallback\n",
        "                            result = original_tool.func(tool_input)\n",
        "\n",
        "                        logger.info(f\"✅ Tool {name} executed successfully\")\n",
        "                        return str(result)\n",
        "\n",
        "                    except Exception as e:\n",
        "                        error_msg = f\"Error calling {name}: {e!s}\"\n",
        "                        logger.error(error_msg)\n",
        "                        return error_msg\n",
        "\n",
        "                return wrapper_func\n",
        "\n",
        "            # Create LangChain tool with wrapper\n",
        "            langchain_tool = Tool(\n",
        "                name=tool_name,\n",
        "                description=f\"Tool for {tool_name.replace('_', ' ')}\",\n",
        "                func=create_tool_wrapper(catalog_tool, tool_name),\n",
        "            )\n",
        "            tools.append(langchain_tool)\n",
        "\n",
        "        # Use the Agent Catalog prompt content directly\n",
        "        if isinstance(prompt_resource, list):\n",
        "            prompt_resource = prompt_resource[0]\n",
        "\n",
        "        prompt_content = getattr(prompt_resource, \"content\", \"\")\n",
        "        if not prompt_content:\n",
        "            prompt_content = \"You are a helpful flight search assistant. Use the available tools to help users with their flight queries.\"\n",
        "\n",
        "        # Inject current date into the prompt content\n",
        "        import datetime\n",
        "        current_date = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
        "        prompt_content = prompt_content.replace(\"{current_date}\", current_date)\n",
        "\n",
        "        # Create ReAct agent with tools and prompt\n",
        "        react_prompt = PromptTemplate.from_template(str(prompt_content))\n",
        "        agent = create_react_agent(self.chat_model, tools, react_prompt)\n",
        "\n",
        "        # Create agent executor\n",
        "        agent_executor = AgentExecutor(\n",
        "            agent=agent, tools=tools, verbose=True, handle_parsing_errors=True, max_iterations=10\n",
        "        )\n",
        "\n",
        "        # Execute the agent\n",
        "        response = agent_executor.invoke({\"input\": state[\"query\"]})\n",
        "\n",
        "        # Add response to conversation\n",
        "        assistant_msg = langchain_core.messages.AIMessage(content=response[\"output\"])\n",
        "        state[\"messages\"].append(assistant_msg)\n",
        "        state[\"resolved\"] = True\n",
        "\n",
        "        return state\n",
        "\n",
        "\n",
        "class FlightSearchGraph(agentc_langgraph.graph.GraphRunnable):\n",
        "    \"\"\"Flight search conversation graph using Agent Catalog.\"\"\"\n",
        "\n",
        "    def __init__(self, catalog, span, chat_model=None):\n",
        "        \"\"\"Initialize the flight search graph with optional chat model.\"\"\"\n",
        "        super().__init__(catalog=catalog, span=span)\n",
        "        self.chat_model = chat_model\n",
        "\n",
        "    @staticmethod\n",
        "    def build_starting_state(query: str) -> FlightSearchState:\n",
        "        \"\"\"Build the initial state for the flight search - single user system.\"\"\"\n",
        "        return FlightSearchState(\n",
        "            messages=[],\n",
        "            query=query,\n",
        "            resolved=False,\n",
        "            search_results=[],\n",
        "        )\n",
        "\n",
        "    def compile(self):\n",
        "        \"\"\"Compile the LangGraph workflow.\"\"\"\n",
        "\n",
        "        # Build the flight search agent with catalog integration\n",
        "        search_agent = FlightSearchAgent(catalog=self.catalog, span=self.span, chat_model=self.chat_model)\n",
        "\n",
        "        # Create a wrapper function for the ReActAgent\n",
        "        def flight_search_node(state: FlightSearchState) -> FlightSearchState:\n",
        "            \"\"\"Wrapper function for the flight search ReActAgent.\"\"\"\n",
        "            return search_agent._invoke(\n",
        "                span=self.span,\n",
        "                state=state,\n",
        "                config={},\n",
        "            )\n",
        "\n",
        "        # Create a simple workflow graph for flight search\n",
        "        workflow = langgraph.graph.StateGraph(FlightSearchState)\n",
        "        workflow.add_node(\"flight_search\", flight_search_node)\n",
        "        workflow.set_entry_point(\"flight_search\")\n",
        "        workflow.add_edge(\"flight_search\", langgraph.graph.END)\n",
        "\n",
        "        return workflow.compile()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Clear Existing Data\n",
        "\n",
        "Clear existing bookings and reviews for clean test run.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clear_bookings_and_reviews():\n",
        "    \"\"\"Clear existing flight bookings to start fresh for demo.\"\"\"\n",
        "    try:\n",
        "        client = CouchbaseClient(\n",
        "            conn_string=os.environ[\"CB_CONN_STRING\"],\n",
        "            username=os.environ[\"CB_USERNAME\"],\n",
        "            password=os.environ[\"CB_PASSWORD\"],\n",
        "            bucket_name=os.environ[\"CB_BUCKET\"],\n",
        "        )\n",
        "        client.connect()\n",
        "\n",
        "        # Clear bookings scope\n",
        "        bookings_scope = \"agentc_bookings\"\n",
        "        client.clear_scope(bookings_scope)\n",
        "        logger.info(\n",
        "            f\"✅ Cleared existing flight bookings for fresh test run: {os.environ['CB_BUCKET']}.{bookings_scope}\"\n",
        "        )\n",
        "\n",
        "        # Check if airline reviews collection needs clearing\n",
        "        try:\n",
        "            sys.path.append(os.path.join(os.getcwd(), \"data\"))\n",
        "            from airline_reviews_data import _data_manager\n",
        "\n",
        "            # Get expected document count\n",
        "            expected_docs = _data_manager.process_to_texts()\n",
        "            expected_count = len(expected_docs)\n",
        "\n",
        "            # Check current document count in collection\n",
        "            try:\n",
        "                count_query = f\"SELECT COUNT(*) as count FROM `{os.environ['CB_BUCKET']}`.`{os.environ['CB_SCOPE']}`.`{os.environ['CB_COLLECTION']}`\"\n",
        "                count_result = client.cluster.query(count_query)\n",
        "                count_row = next(iter(count_result))\n",
        "                existing_count = count_row[\"count\"]\n",
        "\n",
        "                logger.info(\n",
        "                    f\"📊 Airline reviews collection: {existing_count} existing, {expected_count} expected\"\n",
        "                )\n",
        "\n",
        "                if existing_count == expected_count:\n",
        "                    logger.info(\n",
        "                        f\"✅ Collection already has correct document count ({existing_count}), skipping clear\"\n",
        "                    )\n",
        "                else:\n",
        "                    logger.info(\n",
        "                        f\"🗑️  Clearing airline reviews collection: {os.environ['CB_BUCKET']}.{os.environ['CB_SCOPE']}.{os.environ['CB_COLLECTION']}\"\n",
        "                    )\n",
        "                    client.clear_collection(os.environ[\"CB_SCOPE\"], os.environ[\"CB_COLLECTION\"])\n",
        "                    logger.info(\n",
        "                        f\"✅ Cleared existing airline reviews for fresh data load: {os.environ['CB_BUCKET']}.{os.environ['CB_SCOPE']}.{os.environ['CB_COLLECTION']}\"\n",
        "                    )\n",
        "\n",
        "            except Exception as count_error:\n",
        "                logger.info(\n",
        "                    f\"📊 Collection doesn't exist or query failed, will clear and reload: {count_error}\"\n",
        "                )\n",
        "                client.clear_collection(os.environ[\"CB_SCOPE\"], os.environ[\"CB_COLLECTION\"])\n",
        "                logger.info(\n",
        "                    f\"✅ Cleared existing airline reviews for fresh data load: {os.environ['CB_BUCKET']}.{os.environ['CB_SCOPE']}.{os.environ['CB_COLLECTION']}\"\n",
        "                )\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"⚠️  Could not check collection count, clearing anyway: {e}\")\n",
        "            client.clear_collection(os.environ[\"CB_SCOPE\"], os.environ[\"CB_COLLECTION\"])\n",
        "            logger.info(\n",
        "                f\"✅ Cleared existing airline reviews for fresh data load: {os.environ['CB_BUCKET']}.{os.environ['CB_SCOPE']}.{os.environ['CB_COLLECTION']}\"\n",
        "            )\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"❌ Could not clear bookings: {e}\")\n",
        "\n",
        "\n",
        "# Clear existing data\n",
        "clear_bookings_and_reviews()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Setup Flight Search Agent\n",
        "\n",
        "Initialize the complete flight search agent setup.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def setup_flight_search_agent():\n",
        "    \"\"\"Setup flight search agent with all components.\"\"\"\n",
        "    try:\n",
        "        # Initialize Agent Catalog\n",
        "        catalog = agentc.Catalog(\n",
        "            conn_string=os.environ[\"AGENT_CATALOG_CONN_STRING\"],\n",
        "            username=os.environ[\"AGENT_CATALOG_USERNAME\"],\n",
        "            password=SecretStr(os.environ[\"AGENT_CATALOG_PASSWORD\"]),\n",
        "            bucket=os.environ[\"AGENT_CATALOG_BUCKET\"],\n",
        "        )\n",
        "        application_span = catalog.Span(name=\"Flight Search Agent\")\n",
        "\n",
        "        # Test Capella AI connectivity\n",
        "        if os.getenv(\"CAPELLA_API_ENDPOINT\"):\n",
        "            if not test_capella_connectivity():\n",
        "                logger.warning(\n",
        "                    \"❌ Capella AI connectivity test failed. Will use OpenAI fallback.\"\n",
        "                )\n",
        "        else:\n",
        "            logger.info(\"ℹ️ Capella API not configured - will use OpenAI models\")\n",
        "\n",
        "        # Create CouchbaseClient for all operations\n",
        "        client = CouchbaseClient(\n",
        "            conn_string=os.environ[\"CB_CONN_STRING\"],\n",
        "            username=os.environ[\"CB_USERNAME\"],\n",
        "            password=os.environ[\"CB_PASSWORD\"],\n",
        "            bucket_name=os.environ[\"CB_BUCKET\"],\n",
        "        )\n",
        "\n",
        "        # Setup collection\n",
        "        client.setup_collection(\n",
        "            scope_name=os.environ[\"CB_SCOPE\"], collection_name=os.environ[\"CB_COLLECTION\"]\n",
        "        )\n",
        "\n",
        "        # Setup vector search index\n",
        "        try:\n",
        "            with open(\"agentcatalog_index.json\") as file:\n",
        "                index_definition = json.load(file)\n",
        "            logger.info(\"Loaded vector search index definition from agentcatalog_index.json\")\n",
        "            client.setup_vector_search_index(index_definition, os.environ[\"CB_SCOPE\"])\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Error loading index definition: {e!s}\")\n",
        "            logger.info(\"Continuing without vector search index...\")\n",
        "\n",
        "        # Setup embeddings and vector store\n",
        "        # Use Capella AI embeddings if available, fallback to OpenAI\n",
        "        try:\n",
        "            if (\n",
        "                os.getenv(\"CB_USERNAME\")\n",
        "                and os.getenv(\"CB_PASSWORD\")\n",
        "                and os.getenv(\"CAPELLA_API_ENDPOINT\")\n",
        "                and os.getenv(\"CAPELLA_API_EMBEDDING_MODEL\")\n",
        "            ):\n",
        "                # Create API key for Capella AI\n",
        "                api_key = base64.b64encode(\n",
        "                    f\"{os.getenv('CB_USERNAME')}:{os.getenv('CB_PASSWORD')}\".encode()\n",
        "                ).decode()\n",
        "\n",
        "                # Use OpenAI embeddings client with Capella endpoint\n",
        "                embeddings = OpenAIEmbeddings(\n",
        "                    model=os.getenv(\"CAPELLA_API_EMBEDDING_MODEL\"),\n",
        "                    api_key=api_key,\n",
        "                    base_url=os.getenv(\"CAPELLA_API_ENDPOINT\"),\n",
        "                )\n",
        "                logger.info(\"✅ Using Capella AI for embeddings\")\n",
        "            else:\n",
        "                raise ValueError(\"Capella AI credentials not available\")\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"⚠️ Capella AI embeddings failed: {e}\")\n",
        "            logger.info(\"🔄 Falling back to OpenAI embeddings...\")\n",
        "            _set_if_undefined(\"OPENAI_API_KEY\")\n",
        "            embeddings = OpenAIEmbeddings(\n",
        "                api_key=SecretStr(os.environ[\"OPENAI_API_KEY\"]), model=\"text-embedding-3-small\"\n",
        "            )\n",
        "            logger.info(\"✅ Using OpenAI embeddings as fallback\")\n",
        "\n",
        "        client.setup_vector_store(\n",
        "            scope_name=os.environ[\"CB_SCOPE\"],\n",
        "            collection_name=os.environ[\"CB_COLLECTION\"],\n",
        "            index_name=os.environ[\"CB_INDEX\"],\n",
        "            embeddings=embeddings,\n",
        "        )\n",
        "\n",
        "        # Setup LLM - try Capella AI first, fallback to OpenAI\n",
        "        try:\n",
        "            # Create API key for Capella AI using same pattern as embeddings\n",
        "            api_key = base64.b64encode(\n",
        "                f\"{os.getenv('CB_USERNAME')}:{os.getenv('CB_PASSWORD')}\".encode()\n",
        "            ).decode()\n",
        "\n",
        "            chat_model = ChatOpenAI(\n",
        "                api_key=api_key,\n",
        "                base_url=os.getenv(\"CAPELLA_API_ENDPOINT\"),\n",
        "                model=os.getenv(\"CAPELLA_API_LLM_MODEL\"),\n",
        "                temperature=0.1,\n",
        "            )\n",
        "            # Test the LLM works\n",
        "            chat_model.invoke(\"Hello\")\n",
        "            logger.info(\"✅ Using Capella AI LLM\")\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"⚠️ Capella AI LLM failed: {e}\")\n",
        "            logger.info(\"🔄 Falling back to OpenAI LLM...\")\n",
        "            _set_if_undefined(\"OPENAI_API_KEY\")\n",
        "            chat_model = ChatOpenAI(\n",
        "                api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
        "                model=\"gpt-4o\",\n",
        "                temperature=0.1,\n",
        "            )\n",
        "            logger.info(\"✅ Using OpenAI LLM as fallback\")\n",
        "\n",
        "        # Create the flight search graph with the chat model\n",
        "        flight_graph = FlightSearchGraph(catalog=catalog, span=application_span, chat_model=chat_model)\n",
        "        compiled_graph = flight_graph.compile()\n",
        "\n",
        "        logger.info(\"Agent Catalog integration successful\")\n",
        "        return compiled_graph, application_span\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.exception(f\"Setup error: {e}\")\n",
        "        logger.info(\"Ensure Agent Catalog is published: agentc index . && agentc publish\")\n",
        "        raise\n",
        "\n",
        "\n",
        "def run_test_query(test_number: int, query: str, compiled_graph, application_span):\n",
        "    \"\"\"Run a single test query with error handling.\"\"\"\n",
        "    with application_span.new(f\"Test {test_number}: {query}\") as query_span:\n",
        "        logger.info(f\"\\\\n🔍 Test {test_number}: {query}\")\n",
        "        try:\n",
        "            query_span[\"query\"] = query\n",
        "            state = FlightSearchGraph.build_starting_state(query=query)\n",
        "            result = compiled_graph.invoke(state)\n",
        "            query_span[\"result\"] = result\n",
        "\n",
        "            if result.get(\"search_results\"):\n",
        "                logger.info(f\"Found {len(result['search_results'])} flight options\")\n",
        "            logger.info(f\"Test {test_number} completed: {result.get('resolved', False)}\")\n",
        "\n",
        "            return result\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.exception(f\"❌ Test {test_number} failed: {e}\")\n",
        "            query_span[\"error\"] = str(e)\n",
        "            return None\n",
        "\n",
        "\n",
        "# Setup the agent\n",
        "compiled_graph, application_span = setup_flight_search_agent()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Test 1: Flight Search\n",
        "\n",
        "Find flights from JFK to LAX for tomorrow.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result1 = run_test_query(1, \"Find flights from JFK to LAX for tomorrow\", compiled_graph, application_span)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Test 2: Flight Booking (Business Class)\n",
        "\n",
        "Book a flight with business class.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result2 = run_test_query(\n",
        "    2, \"Book a flight from LAX to JFK for tomorrow, 2 passengers, business class\", compiled_graph, application_span\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Test 3: Flight Booking (Economy Class)\n",
        "\n",
        "Book an economy flight.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result3 = run_test_query(3, \"Book an economy flight from JFK to MIA for next week, 1 passenger\", compiled_graph, application_span)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Test 4: Retrieve Current Bookings\n",
        "\n",
        "Show current flight bookings.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result4 = run_test_query(4, \"Show me my current flight bookings\", compiled_graph, application_span)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Test 5: Airline Reviews Search\n",
        "\n",
        "Search airline reviews for service quality.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result5 = run_test_query(5, \"What do passengers say about SpiceJet's service quality?\", compiled_graph, application_span)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Arize Phoenix Evaluation\n",
        "\n",
        "This section demonstrates how to evaluate the flight search agent using Arize Phoenix observability platform. The evaluation includes:\n",
        "\n",
        "- **Relevance Scoring**: Using Phoenix RelevanceEvaluator to score how relevant responses are to queries\n",
        "- **QA Scoring**: Using Phoenix QAEvaluator to score answer quality\n",
        "- **Hallucination Detection**: Using Phoenix HallucinationEvaluator to detect fabricated information  \n",
        "- **Toxicity Detection**: Using Phoenix ToxicityEvaluator to detect harmful content\n",
        "- **Phoenix UI**: Real-time observability dashboard at `http://localhost:6006/`\n",
        "\n",
        "We'll run two simple flight search queries and evaluate the responses for quality and safety.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import Phoenix evaluation components\n",
        "try:\n",
        "    import phoenix as px\n",
        "    from phoenix.evals import (\n",
        "        HALLUCINATION_PROMPT_RAILS_MAP,\n",
        "        HALLUCINATION_PROMPT_TEMPLATE,\n",
        "        QA_PROMPT_RAILS_MAP,\n",
        "        QA_PROMPT_TEMPLATE,\n",
        "        RAG_RELEVANCY_PROMPT_RAILS_MAP,\n",
        "        RAG_RELEVANCY_PROMPT_TEMPLATE,\n",
        "        TOXICITY_PROMPT_RAILS_MAP,\n",
        "        TOXICITY_PROMPT_TEMPLATE,\n",
        "        OpenAIModel,\n",
        "        llm_classify,\n",
        "    )\n",
        "    import pandas as pd\n",
        "    \n",
        "    ARIZE_AVAILABLE = True\n",
        "    logger.info(\"✅ Arize Phoenix evaluation components available\")\n",
        "except ImportError as e:\n",
        "    logger.warning(f\"Arize dependencies not available: {e}\")\n",
        "    logger.warning(\"Skipping evaluation section...\")\n",
        "    ARIZE_AVAILABLE = False\n",
        "\n",
        "if ARIZE_AVAILABLE:\n",
        "    # Start Phoenix session for observability\n",
        "    try:\n",
        "        px.launch_app(port=6006)\n",
        "        logger.info(\"🚀 Phoenix UI available at http://localhost:6006/\")\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"Could not start Phoenix UI: {e}\")\n",
        "\n",
        "    # Demo queries for evaluation\n",
        "    flight_demo_queries = [\n",
        "        \"Find flights from JFK to LAX\",\n",
        "        \"What do passengers say about SpiceJet's service quality?\"\n",
        "    ]\n",
        "    \n",
        "    # Run demo queries and collect responses for evaluation\n",
        "    flight_demo_results = []\n",
        "    \n",
        "    for i, query in enumerate(flight_demo_queries, 1):\n",
        "        try:\n",
        "            logger.info(f\"🔍 Running evaluation query {i}: {query}\")\n",
        "            \n",
        "            # Create initial state and run the compiled graph\n",
        "            state = FlightSearchGraph.build_starting_state(query=query)\n",
        "            result = compiled_graph.invoke(state)\n",
        "    \n",
        "            # Extract the response from the final message\n",
        "            if result.get(\"messages\") and len(result[\"messages\"]) > 1:\n",
        "                output = result[\"messages\"][-1].content\n",
        "            else:\n",
        "                output = \"No response generated\"\n",
        "    \n",
        "            flight_demo_results.append({\n",
        "                \"query\": query,\n",
        "                \"response\": output,\n",
        "                \"query_type\": f\"flight_demo_{i}\",\n",
        "                \"success\": result.get(\"resolved\", False)\n",
        "            })\n",
        "            \n",
        "            logger.info(f\"✅ Query {i} completed successfully\")\n",
        "    \n",
        "        except Exception as e:\n",
        "            logger.exception(f\"❌ Query {i} failed: {e}\")\n",
        "            flight_demo_results.append({\n",
        "                \"query\": query,\n",
        "                \"response\": f\"Error: {e!s}\",\n",
        "                \"query_type\": f\"flight_demo_{i}\",\n",
        "                \"success\": False\n",
        "            })\n",
        "    \n",
        "    # Convert to DataFrame for evaluation\n",
        "    flight_results_df = pd.DataFrame(flight_demo_results)\n",
        "    logger.info(f\"📊 Collected {len(flight_results_df)} responses for evaluation\")\n",
        "    \n",
        "    # Display results summary\n",
        "    for _, row in flight_results_df.iterrows():\n",
        "        logger.info(f\"Query: {row['query']}\")\n",
        "        logger.info(f\"Response: {row['response'][:200]}...\")\n",
        "        logger.info(f\"Success: {row['success']}\")\n",
        "        logger.info(\"-\" * 50)\n",
        "    \n",
        "    logger.info(\"💡 Visit Phoenix UI at http://localhost:6006/ to see detailed traces and evaluations\")\n",
        "    logger.info(\"💡 Use the evaluation script at evals/eval_arize.py for comprehensive evaluation\")\n",
        "\n",
        "else:\n",
        "    logger.info(\"Arize evaluation not available - install phoenix-evals to enable evaluation\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if ARIZE_AVAILABLE and len(flight_demo_results) > 0:\n",
        "    logger.info(\"🔍 Running comprehensive Phoenix evaluations...\")\n",
        "    \n",
        "    # Setup evaluator LLM (using OpenAI for consistency)\n",
        "    evaluator_llm = OpenAIModel(model=\"gpt-4o\", temperature=0.1)\n",
        "    \n",
        "    # Prepare evaluation data with proper column names for Phoenix evaluators\n",
        "    flight_eval_data = []\n",
        "    for _, row in flight_results_df.iterrows():\n",
        "        flight_eval_data.append({\n",
        "            \"input\": row[\"query\"],\n",
        "            \"output\": row[\"response\"],\n",
        "            \"reference\": \"A helpful and accurate response about flights with specific flight information or airline reviews\",\n",
        "            \"text\": row[\"response\"]  # For toxicity evaluation\n",
        "        })\n",
        "    \n",
        "    flight_eval_df = pd.DataFrame(flight_eval_data)\n",
        "    \n",
        "    try:\n",
        "        # 1. Relevance Evaluation\n",
        "        logger.info(\"🔍 Running Relevance Evaluation...\")\n",
        "        flight_relevance_results = llm_classify(\n",
        "            dataframe=flight_eval_df[[\"input\", \"reference\"]],\n",
        "            model=evaluator_llm,\n",
        "            template=RAG_RELEVANCY_PROMPT_TEMPLATE,\n",
        "            rails=list(RAG_RELEVANCY_PROMPT_RAILS_MAP.values())\n",
        "        )\n",
        "        \n",
        "        logger.info(\"✅ Relevance Evaluation Results:\")\n",
        "        for i, result in enumerate(flight_relevance_results):\n",
        "            query = flight_eval_data[i][\"input\"]\n",
        "            logger.info(f\"   Query: {query}\")\n",
        "            logger.info(f\"   Relevance: {result.label}\")\n",
        "            if hasattr(result, 'explanation') and result.explanation:\n",
        "                logger.info(f\"   Explanation: {result.explanation}\")\n",
        "            logger.info(\"   \" + \"-\"*30)\n",
        "        \n",
        "        # 2. QA Evaluation\n",
        "        logger.info(\"🔍 Running QA Evaluation...\")\n",
        "        flight_qa_results = llm_classify(\n",
        "            dataframe=flight_eval_df[[\"input\", \"output\", \"reference\"]],\n",
        "            model=evaluator_llm,\n",
        "            template=QA_PROMPT_TEMPLATE,\n",
        "            rails=list(QA_PROMPT_RAILS_MAP.values())\n",
        "        )\n",
        "        \n",
        "        logger.info(\"✅ QA Evaluation Results:\")\n",
        "        for i, result in enumerate(flight_qa_results):\n",
        "            query = flight_eval_data[i][\"input\"]\n",
        "            logger.info(f\"   Query: {query}\")\n",
        "            logger.info(f\"   QA Score: {result.label}\")\n",
        "            if hasattr(result, 'explanation') and result.explanation:\n",
        "                logger.info(f\"   Explanation: {result.explanation}\")\n",
        "            logger.info(\"   \" + \"-\"*30)\n",
        "        \n",
        "        # 3. Hallucination Evaluation\n",
        "        logger.info(\"🔍 Running Hallucination Evaluation...\")\n",
        "        flight_hallucination_results = llm_classify(\n",
        "            dataframe=flight_eval_df[[\"input\", \"reference\", \"output\"]],\n",
        "            model=evaluator_llm,\n",
        "            template=HALLUCINATION_PROMPT_TEMPLATE,\n",
        "            rails=list(HALLUCINATION_PROMPT_RAILS_MAP.values())\n",
        "        )\n",
        "        \n",
        "        logger.info(\"✅ Hallucination Evaluation Results:\")\n",
        "        for i, result in enumerate(flight_hallucination_results):\n",
        "            query = flight_eval_data[i][\"input\"]\n",
        "            logger.info(f\"   Query: {query}\")\n",
        "            logger.info(f\"   Hallucination: {result.label}\")\n",
        "            if hasattr(result, 'explanation') and result.explanation:\n",
        "                logger.info(f\"   Explanation: {result.explanation}\")\n",
        "            logger.info(\"   \" + \"-\"*30)\n",
        "        \n",
        "        # 4. Toxicity Evaluation\n",
        "        logger.info(\"🔍 Running Toxicity Evaluation...\")\n",
        "        flight_toxicity_results = llm_classify(\n",
        "            dataframe=flight_eval_df[[\"text\"]],\n",
        "            model=evaluator_llm,\n",
        "            template=TOXICITY_PROMPT_TEMPLATE,\n",
        "            rails=list(TOXICITY_PROMPT_RAILS_MAP.values())\n",
        "        )\n",
        "        \n",
        "        logger.info(\"✅ Toxicity Evaluation Results:\")\n",
        "        for i, result in enumerate(flight_toxicity_results):\n",
        "            query = flight_eval_data[i][\"input\"]\n",
        "            logger.info(f\"   Query: {query}\")\n",
        "            logger.info(f\"   Toxicity: {result.label}\")\n",
        "            if hasattr(result, 'explanation') and result.explanation:\n",
        "                logger.info(f\"   Explanation: {result.explanation}\")\n",
        "            logger.info(\"   \" + \"-\"*30)\n",
        "        \n",
        "        # Summary of all evaluations\n",
        "        logger.info(\"📊 EVALUATION SUMMARY\")\n",
        "        logger.info(\"=\" * 50)\n",
        "        \n",
        "        for i, query in enumerate([item[\"input\"] for item in flight_eval_data]):\n",
        "            logger.info(f\"Query {i+1}: {query}\")\n",
        "            logger.info(f\"  Relevance: {flight_relevance_results[i].label}\")\n",
        "            logger.info(f\"  QA Score: {flight_qa_results[i].label}\")\n",
        "            logger.info(f\"  Hallucination: {flight_hallucination_results[i].label}\")\n",
        "            logger.info(f\"  Toxicity: {flight_toxicity_results[i].label}\")\n",
        "            logger.info(\"  \" + \"-\"*40)\n",
        "        \n",
        "        logger.info(\"✅ All Phoenix evaluations completed successfully!\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.exception(f\"❌ Phoenix evaluation failed: {e}\")\n",
        "        logger.info(\"💡 This might be due to API rate limits or model availability\")\n",
        "        \n",
        "else:\n",
        "    if not ARIZE_AVAILABLE:\n",
        "        logger.info(\"❌ Phoenix evaluations skipped - Arize dependencies not available\")\n",
        "    else:\n",
        "        logger.info(\"❌ Phoenix evaluations skipped - No demo results to evaluate\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrates a complete flight search agent implementation using:\n",
        "\n",
        "1. **Agent Catalog Integration**: Using agentc to find tools and prompts\n",
        "2. **LangGraph Framework**: ReAct agent pattern with proper state management\n",
        "3. **Couchbase Vector Store**: Storing and searching airline reviews data\n",
        "4. **Tool Integration**: Flight lookup, booking, and review search capabilities\n",
        "5. **Comprehensive Evaluation**: Phoenix-based evaluation with multiple metrics\n",
        "\n",
        "The agent can handle various flight-related queries including:\n",
        "- Flight search and lookup\n",
        "- Flight booking with different classes\n",
        "- Retrieving existing bookings\n",
        "- Searching airline reviews for service quality\n",
        "\n",
        "## Phoenix Evaluation Metrics\n",
        "\n",
        "The notebook demonstrates all four key Phoenix evaluation types:\n",
        "\n",
        "1. **Relevance Evaluation**: Measures how relevant responses are to user queries\n",
        "2. **QA Evaluation**: Assesses the quality and accuracy of answers\n",
        "3. **Hallucination Detection**: Identifies fabricated or incorrect information\n",
        "4. **Toxicity Detection**: Screens for harmful or inappropriate content\n",
        "\n",
        "Each evaluation provides:\n",
        "- Binary or categorical labels (e.g., \"relevant\"/\"irrelevant\", \"correct\"/\"incorrect\")\n",
        "- Detailed explanations of the evaluation reasoning\n",
        "- Confidence scores for the assessments\n",
        "\n",
        "For production use, consider:\n",
        "- Setting up proper monitoring with Arize Phoenix\n",
        "- Implementing comprehensive evaluation pipelines\n",
        "- Adding error handling and retry logic\n",
        "- Scaling the vector store for larger datasets\n",
        "\n",
        "## Key Changes from Previous Version\n",
        "\n",
        "This updated notebook:\n",
        "- **Removed** the ParameterMapper class (no longer needed)\n",
        "- **Updated** to use current airline reviews data instead of flight policies\n",
        "- **Simplified** tool parameter handling with direct input parsing\n",
        "- **Added** comprehensive Phoenix evaluation with all four evaluation types\n",
        "- **Maintained** CouchbaseClient class for database operations\n",
        "- **Updated** imports and dependencies to match current implementation\n",
        "\n",
        "To run this notebook:\n",
        "1. Set up the required environment variables\n",
        "2. Install dependencies: `pip install -r requirements.txt`\n",
        "3. Publish your agent catalog: `agentc index . && agentc publish`\n",
        "4. Run the notebook cells sequentially\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
