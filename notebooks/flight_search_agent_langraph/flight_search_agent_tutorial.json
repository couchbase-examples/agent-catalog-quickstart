{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDGGbfZlmTCq",
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Flight Search Agent Tutorial - Priority 1 Implementation\n",
        "\n",
        "This notebook demonstrates the Agent Catalog flight search agent using LangGraph with Couchbase vector store and Arize evaluation. Uses Priority 1 AI services with standard OpenAI wrappers and Capella (simple & fast).\n",
        "\n",
        "The agent provides comprehensive flight search capabilities including:\n",
        "- Flight lookup and search\n",
        "- Flight booking management\n",
        "- Airline review search\n",
        "- Booking retrieval and management\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XmfZF6U8gH12",
        "outputId": "f3445c63-2fe8-48e6-b228-b0f3476db0e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "print(os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWQCmIIngH12",
        "outputId": "fde4cc46-731d-4c3b-bdef-52d5451a93b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-10-22 21:59:49--  https://raw.githubusercontent.com/couchbase-examples/agent-catalog-quickstart/refs/heads/main/notebooks/flight_search_agent_langraph/prompts/flight_search_assistant.yaml\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2630 (2.6K) [text/plain]\n",
            "Saving to: ‘prompts/flight_search_assistant.yaml’\n",
            "\n",
            "\r          prompts/f   0%[                    ]       0  --.-KB/s               \rprompts/flight_sear 100%[===================>]   2.57K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-10-22 21:59:49 (36.9 MB/s) - ‘prompts/flight_search_assistant.yaml’ saved [2630/2630]\n",
            "\n",
            "--2025-10-22 21:59:50--  https://raw.githubusercontent.com/couchbase-examples/agent-catalog-quickstart/refs/heads/main/notebooks/flight_search_agent_langraph/tools/lookup_flight_info.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3709 (3.6K) [text/plain]\n",
            "Saving to: ‘tools/lookup_flight_info.py’\n",
            "\n",
            "tools/lookup_flight 100%[===================>]   3.62K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-10-22 21:59:50 (29.4 MB/s) - ‘tools/lookup_flight_info.py’ saved [3709/3709]\n",
            "\n",
            "--2025-10-22 21:59:50--  https://raw.githubusercontent.com/couchbase-examples/agent-catalog-quickstart/refs/heads/main/notebooks/flight_search_agent_langraph/tools/retrieve_flight_bookings.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6509 (6.4K) [text/plain]\n",
            "Saving to: ‘tools/retrieve_flight_bookings.py’\n",
            "\n",
            "tools/retrieve_flig 100%[===================>]   6.36K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-10-22 21:59:50 (55.9 MB/s) - ‘tools/retrieve_flight_bookings.py’ saved [6509/6509]\n",
            "\n",
            "--2025-10-22 21:59:50--  https://raw.githubusercontent.com/couchbase-examples/agent-catalog-quickstart/refs/heads/main/notebooks/flight_search_agent_langraph/tools/save_flight_booking.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 16553 (16K) [text/plain]\n",
            "Saving to: ‘tools/save_flight_booking.py’\n",
            "\n",
            "tools/save_flight_b 100%[===================>]  16.17K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2025-10-22 21:59:50 (10.6 MB/s) - ‘tools/save_flight_booking.py’ saved [16553/16553]\n",
            "\n",
            "--2025-10-22 21:59:50--  https://raw.githubusercontent.com/couchbase-examples/agent-catalog-quickstart/refs/heads/main/notebooks/flight_search_agent_langraph/tools/search_airline_reviews.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5171 (5.0K) [text/plain]\n",
            "Saving to: ‘tools/search_airline_reviews.py’\n",
            "\n",
            "tools/search_airlin 100%[===================>]   5.05K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-10-22 21:59:50 (46.3 MB/s) - ‘tools/search_airline_reviews.py’ saved [5171/5171]\n",
            "\n",
            "--2025-10-22 21:59:50--  https://raw.githubusercontent.com/couchbase-examples/agent-catalog-quickstart/refs/heads/main/notebooks/flight_search_agent_langraph/tools/schemas.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2025-10-22 21:59:51 ERROR 404: Not Found.\n",
            "\n",
            "--2025-10-22 21:59:51--  https://raw.githubusercontent.com/couchbase-examples/agent-catalog-quickstart/refs/heads/main/notebooks/flight_search_agent_langraph/agentcatalog_index.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1956 (1.9K) [text/plain]\n",
            "Saving to: ‘agentcatalog_index.json’\n",
            "\n",
            "agentcatalog_index. 100%[===================>]   1.91K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-10-22 21:59:51 (24.5 MB/s) - ‘agentcatalog_index.json’ saved [1956/1956]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Download required resources for the flight search agent\n",
        "!mkdir -p prompts\n",
        "!wget -O prompts/flight_search_assistant.yaml https://raw.githubusercontent.com/couchbase-examples/agent-catalog-quickstart/refs/heads/main/notebooks/flight_search_agent_langraph/prompts/flight_search_assistant.yaml\n",
        "!mkdir -p tools\n",
        "!wget -O tools/lookup_flight_info.py https://raw.githubusercontent.com/couchbase-examples/agent-catalog-quickstart/refs/heads/main/notebooks/flight_search_agent_langraph/tools/lookup_flight_info.py\n",
        "!wget -O tools/retrieve_flight_bookings.py https://raw.githubusercontent.com/couchbase-examples/agent-catalog-quickstart/refs/heads/main/notebooks/flight_search_agent_langraph/tools/retrieve_flight_bookings.py\n",
        "!wget -O tools/save_flight_booking.py https://raw.githubusercontent.com/couchbase-examples/agent-catalog-quickstart/refs/heads/main/notebooks/flight_search_agent_langraph/tools/save_flight_booking.py\n",
        "!wget -O tools/search_airline_reviews.py https://raw.githubusercontent.com/couchbase-examples/agent-catalog-quickstart/refs/heads/main/notebooks/flight_search_agent_langraph/tools/search_airline_reviews.py\n",
        "!wget -O tools/schemas.py https://raw.githubusercontent.com/couchbase-examples/agent-catalog-quickstart/refs/heads/main/notebooks/flight_search_agent_langraph/tools/schemas.py\n",
        "!wget -O agentcatalog_index.json https://raw.githubusercontent.com/couchbase-examples/agent-catalog-quickstart/refs/heads/main/notebooks/flight_search_agent_langraph/agentcatalog_index.json\n",
        "!wget -O .agentcignore https://raw.githubusercontent.com/couchbase-examples/agent-catalog-quickstart/refs/heads/main/notebooks/flight_search_agent_langraph/.agentcignore\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ld1O6CqSgH13",
        "outputId": "e0b2d27e-049d-42d7-cc0a-d64967bba317"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "agentc-langchain 0.2.5a2 requires langchain_couchbase<0.3.0,>=0.2.5, but you have langchain-couchbase 0.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "%pip install -q \\\n",
        "    \"pydantic>=2.0.0,<3.0.0\" \\\n",
        "    \"pydantic-settings>=2.10.1,<3.0.0\" \\\n",
        "    \"python-dotenv>=1.0.0,<2.0.0\" \\\n",
        "    \"pandas>=2.0.0,<3.0.0\" \\\n",
        "    \"nest-asyncio>=1.6.0,<2.0.0\" \\\n",
        "    \"uvicorn>=0.29.0,<0.30.0\" \\\n",
        "    \"kagglehub>=0.2.0,<1.0.0\" \\\n",
        "    \"langchain-couchbase>=0.4.0,<0.5.0\" \\\n",
        "    \"langchain-openai>=0.3.11,<0.4.0\" \\\n",
        "    \"langchain-nvidia-ai-endpoints>=0.3.13,<0.4.0\" \\\n",
        "    \"langgraph>=0.5.1,<0.6.0\" \\\n",
        "    \"arize>=7.51.0,<8.0.0\" \\\n",
        "    \"arize-phoenix>=11.37.0,<12.0.0\" \\\n",
        "    \"arize-phoenix-evals>=2.2.0,<3.0.0\" \\\n",
        "    \"openinference-instrumentation-langchain>=0.1.29,<0.2.0\" \\\n",
        "    \"openinference-instrumentation-openai>=0.1.18,<0.2.0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPxLBXu4gH13",
        "outputId": "dd580eca-8c9e-4715-8526-0adbfa422011"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/98.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m92.2/98.5 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.5/98.5 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "agentc-langchain 0.2.5a2 requires langchain_couchbase<0.3.0,>=0.2.5, but you have langchain-couchbase 0.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "%pip install -q https://github.com/couchbaselabs/agent-catalog/releases/download/v0.2.5a3/agentc_core-0.2.5a3-py3-none-any.whl\n",
        "%pip install -q https://github.com/couchbaselabs/agent-catalog/releases/download/v0.2.5a3/agentc_cli-0.2.5a3-py3-none-any.whl\n",
        "%pip install -q https://github.com/couchbaselabs/agent-catalog/releases/download/v0.2.5a3/agentc-0.2.5a3-py3-none-any.whl\n",
        "%pip install -q https://github.com/couchbaselabs/agent-catalog/releases/download/v0.2.5a3/agentc_langchain-0.2.5a3-py3-none-any.whl # Explicitly install agentc_langchain a3\n",
        "%pip install -q https://github.com/couchbaselabs/agent-catalog/releases/download/v0.2.5a3/agentc_langgraph-0.2.5a3-py3-none-any.whl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "94jTF0e6gH13"
      },
      "outputs": [],
      "source": [
        "# Install the couchbase-infrastructure package\n",
        "%pip install -q couchbase-infrastructure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QaglXlSgH13"
      },
      "source": [
        "## 🚀 Educational Infrastructure Setup\n",
        "\n",
        "**This cell sets up your Couchbase Capella infrastructure step-by-step using the `couchbase-infrastructure` package.**\n",
        "\n",
        "### What It Does:\n",
        "1. **Prompts for Credentials** - Securely collects your API key (no .env file needed for Colab!)\n",
        "2. **Creates Capella Project & Cluster** - Sets up your cloud database infrastructure\n",
        "3. **Loads travel-sample Data** - Imports the sample dataset for the tutorial\n",
        "4. **Deploys AI Models** - Provisions embedding (Mistral 7B) and LLM (Llama 3 8B) models\n",
        "5. **Configures Network Access** - Sets up CIDR allowlists for connectivity\n",
        "6. **Creates Database User** - Generates credentials with appropriate permissions\n",
        "7. **Sets Environment Variables** - Configures all required variables for the tutorial\n",
        "\n",
        "### You'll Be Prompted For:\n",
        "- `MANAGEMENT_API_KEY` (required) - Get from [Capella Console](https://cloud.couchbase.com) → Settings → API Keys\n",
        "- Optional: `ORGANIZATION_ID`, `PROJECT_NAME`, `CLUSTER_NAME` (defaults provided)\n",
        "\n",
        "### Process Time:\n",
        "⏳ This will take **10-15 minutes** for cluster and AI model deployment. You'll see step-by-step progress!\n",
        "\n",
        "### After Running:\n",
        "All subsequent cells will automatically use the provisioned infrastructure. No manual configuration needed!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yhwo9axFgH13",
        "outputId": "8e6cc345-185e-4bb2-9a70-09e1d852fc5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "🚀 Couchbase Capella Infrastructure Setup\n",
            "======================================================================\n",
            "\n",
            "This educational setup shows you how to provision Capella infrastructure\n",
            "step-by-step using the couchbase-infrastructure package.\n",
            "\n",
            "\n",
            "📋 Step 1: Collecting Credentials\n",
            "----------------------------------------------------------------------\n",
            "✅ Found .env file. Loading configuration...\n",
            "\n",
            "Get your credentials from: https://cloud.couchbase.com → Settings → API Keys\n",
            "\n",
            "✅ Using MANAGEMENT_API_KEY from environment\n",
            "✅ Using ORGANIZATION_ID from environment: 23086345-371f-4650-8dc4-c61733dd27a0\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"🚀 Couchbase Capella Infrastructure Setup\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nThis educational setup shows you how to provision Capella infrastructure\")\n",
        "print(\"step-by-step using the couchbase-infrastructure package.\\n\")\n",
        "\n",
        "# Import the infrastructure package\n",
        "from couchbase_infrastructure import CapellaConfig, CapellaClient\n",
        "from couchbase_infrastructure.resources import (\n",
        "    create_project,\n",
        "    create_developer_pro_cluster,\n",
        "    add_allowed_cidr,\n",
        "    load_sample_data,\n",
        "    create_database_user,\n",
        "    deploy_ai_model,\n",
        "    create_ai_api_key,\n",
        ")\n",
        "\n",
        "# Step 1: Load from .env file if available, then collect any missing credentials\n",
        "print(\"\\n📋 Step 1: Collecting Credentials\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "# Try to load .env file\n",
        "env_file = Path('.env')\n",
        "if env_file.exists():\n",
        "    print(\"✅ Found .env file. Loading configuration...\\n\")\n",
        "    from dotenv import load_dotenv\n",
        "    load_dotenv('.env')\n",
        "else:\n",
        "    print(\"ℹ️  No .env file found. Will prompt for credentials.\\n\")\n",
        "\n",
        "print(\"Get your credentials from: https://cloud.couchbase.com → Settings → API Keys\\n\")\n",
        "\n",
        "# Required: MANAGEMENT_API_KEY\n",
        "management_api_key = os.getenv('MANAGEMENT_API_KEY')\n",
        "if management_api_key:\n",
        "    print(\"✅ Using MANAGEMENT_API_KEY from environment\")\n",
        "else:\n",
        "    management_api_key = getpass(\"Enter your MANAGEMENT_API_KEY (hidden): \")\n",
        "    if not management_api_key:\n",
        "        raise ValueError(\"MANAGEMENT_API_KEY is required!\")\n",
        "\n",
        "# Required: ORGANIZATION_ID\n",
        "organization_id = os.getenv('ORGANIZATION_ID')\n",
        "if organization_id:\n",
        "    print(f\"✅ Using ORGANIZATION_ID from environment: {organization_id}\")\n",
        "else:\n",
        "    organization_id = input(\"Enter your ORGANIZATION_ID (required): \").strip()\n",
        "    if not organization_id:\n",
        "        raise ValueError(\"ORGANIZATION_ID is required! Find it in Capella Console under Settings.\")\n",
        "\n",
        "# Optional configuration (use env vars if available, otherwise prompt with defaults)\n",
        "api_base_url = os.getenv('API_BASE_URL') or input(\"Enter API_BASE_URL (default: 'cloudapi.cloud.couchbase.com'): \").strip() or \"cloudapi.cloud.couchbase.com\"\n",
        "project_name = os.getenv('PROJECT_NAME') or input(\"Enter PROJECT_NAME (default: 'agent-app'): \").strip() or \"agent-app\"\n",
        "cluster_name = os.getenv('CLUSTER_NAME') or input(\"Enter CLUSTER_NAME (default: 'agent-app-cluster'): \").strip() or \"agent-app-cluster\"\n",
        "db_username = os.getenv('DB_USERNAME') or input(\"Enter DB_USERNAME (default: 'agent_app_user'): \").strip() or \"agent_app_user\"\n",
        "sample_bucket = os.getenv('SAMPLE_BUCKET') or input(\"Enter BUCKET_NAME (default: 'travel-sample'): \").strip() or \"travel-sample\"\n",
        "embedding_model = os.getenv('EMBEDDING_MODEL_NAME') or input(\"Enter EMBEDDING_MODEL (default: 'nvidia/llama-3.2-nv-embedqa-1b-v2'): \").strip() or \"nvidia/llama-3.2-nv-embedqa-1b-v2\"\n",
        "llm_model = os.getenv('LLM_MODEL_NAME') or input(\"Enter LLM_MODEL (default: 'meta/llama3-8b-instruct'): \").strip() or \"meta/llama3-8b-instruct\"\n",
        "\n",
        "print(\"\\n✅ Configuration collected successfully!\\n\")\n",
        "\n",
        "# Step 2: Initialize configuration\n",
        "print(\"\\n🔧 Step 2: Initializing Configuration\")\n",
        "print(\"-\"*70)\n",
        "config = CapellaConfig(\n",
        "    management_api_key=management_api_key,\n",
        "    organization_id=organization_id,\n",
        "    api_base_url=api_base_url,\n",
        "    project_name=project_name,\n",
        "    cluster_name=cluster_name,\n",
        "    db_username=db_username,\n",
        "    sample_bucket=sample_bucket,\n",
        "    embedding_model_name=embedding_model,\n",
        "    llm_model_name=llm_model,\n",
        ")\n",
        "print(\"✅ Configuration initialized\\n\")\n",
        "\n",
        "# Step 3: Initialize client and get organization ID\n",
        "print(\"\\n🔌 Step 3: Initializing Client\")\n",
        "print(\"-\"*70)\n",
        "client = CapellaClient(config)\n",
        "org_id = client.get_organization_id()\n",
        "print(f\"✅ Using Organization ID: {org_id}\\n\")\n",
        "\n",
        "# Step 4: Test API connection\n",
        "print(\"\\n🔍 Step 4: Testing API Connection\")\n",
        "print(\"-\"*70)\n",
        "if not client.test_connection(org_id):\n",
        "    raise ConnectionError(\"Failed to connect to Capella API\")\n",
        "print(\"✅ API connection successful\\n\")\n",
        "\n",
        "# Step 5: Create Capella Project\n",
        "print(\"\\n📁 Step 5: Creating Capella Project\")\n",
        "print(\"-\"*70)\n",
        "project_id = create_project(client, org_id, config.project_name)\n",
        "print(f\"✅ Project ready: {config.project_name} (ID: {project_id})\\n\")\n",
        "\n",
        "# Step 6: Create Developer Pro cluster with Analytics\n",
        "print(\"\\n☁️ Step 6: Creating Developer Pro Cluster with Analytics\")\n",
        "print(\"-\"*70)\n",
        "print(\"⏳ This will take 10-15 minutes for cluster deployment...\\n\")\n",
        "cluster_id = create_developer_pro_cluster(\n",
        "    client, org_id, project_id, config.cluster_name,\n",
        "    cloud_provider=config.cluster_cloud_provider,\n",
        "    region=config.cluster_region,\n",
        "    cidr=config.cluster_cidr\n",
        ")\n",
        "# Wait for cluster to be ready\n",
        "cluster_check_url = f\"/v4/organizations/{org_id}/projects/{project_id}/clusters/{cluster_id}\"\n",
        "cluster_details = client.wait_for_resource(cluster_check_url, \"Cluster\", None)\n",
        "cluster_conn_string = cluster_details.get(\"connectionString\")\n",
        "print(f\"✅ Cluster ready: {config.cluster_name} (ID: {cluster_id})\\n\")\n",
        "\n",
        "# Step 7: Configure network access\n",
        "print(\"\\n🌐 Step 7: Configuring Network Access\")\n",
        "print(\"-\"*70)\n",
        "add_allowed_cidr(client, org_id, project_id, cluster_id, config.allowed_cidr)\n",
        "print(\"✅ Network access configured (0.0.0.0/0 allowed)\\n\")\n",
        "\n",
        "# Step 8: Load travel-sample bucket\n",
        "print(\"\\n📦 Step 8: Loading travel-sample Bucket\")\n",
        "print(\"-\"*70)\n",
        "load_sample_data(client, org_id, project_id, cluster_id, config.sample_bucket)\n",
        "print(f\"✅ Sample data loaded: {config.sample_bucket}\\n\")\n",
        "\n",
        "# Step 9: Create database user (password auto-generated)\n",
        "print(\"\\n👤 Step 9: Creating Database User\")\n",
        "print(\"-\"*70)\n",
        "db_password = create_database_user(\n",
        "    client,\n",
        "    org_id,\n",
        "    project_id,\n",
        "    cluster_id,\n",
        "    config.db_username,\n",
        "    config.sample_bucket,\n",
        "    recreate_if_exists=True,  # Delete and recreate if exists to get fresh password\n",
        ")\n",
        "print(f\"✅ Database user created: {config.db_username}\\n\")\n",
        "if db_password and db_password != \"existing_user_password_not_retrievable\":\n",
        "    print(f\"   Auto-generated password: {db_password[:4]}...{db_password[-4:]}\\n\")\n",
        "\n",
        "# Step 10: Deploy AI models\n",
        "print(\"\\n🤖 Step 10: Deploying AI Models\")\n",
        "print(\"-\"*70)\n",
        "print(\"⏳ Deploying embedding and LLM models (5-10 minutes)...\\n\")\n",
        "\n",
        "# Deploy Embedding Model\n",
        "print(\"   Deploying embedding model...\")\n",
        "embedding_model_id = deploy_ai_model(\n",
        "    client,\n",
        "    org_id,\n",
        "    config.embedding_model_name,\n",
        "    \"agent-hub-embedding-model\",\n",
        "    \"embedding\",\n",
        "    config,\n",
        ")\n",
        "embedding_check_url = f\"/v4/organizations/{org_id}/aiServices/models/{embedding_model_id}\"\n",
        "embedding_details = client.wait_for_resource(embedding_check_url, \"Embedding Model\", None)\n",
        "embedding_endpoint = embedding_details.get(\"connectionString\", \"\")\n",
        "print(f\"✅ Embedding model deployed: {config.embedding_model_name}\\n\")\n",
        "\n",
        "# Deploy LLM Model\n",
        "print(\"   Deploying LLM model...\")\n",
        "llm_model_id = deploy_ai_model(\n",
        "    client,\n",
        "    org_id,\n",
        "    config.llm_model_name,\n",
        "    \"agent-hub-llm-model\",\n",
        "    \"llm\",\n",
        "    config,\n",
        ")\n",
        "llm_check_url = f\"/v4/organizations/{org_id}/aiServices/models/{llm_model_id}\"\n",
        "llm_details = client.wait_for_resource(llm_check_url, \"LLM Model\", None)\n",
        "llm_endpoint = llm_details.get(\"connectionString\", \"\")\n",
        "print(f\"✅ LLM model deployed: {config.llm_model_name}\\n\")\n",
        "\n",
        "# Step 11: Create API Key for AI models\n",
        "print(\"\\n🔑 Step 11: Creating API Key for AI Models\")\n",
        "print(\"-\"*70)\n",
        "api_key = create_ai_api_key(client, org_id, config.ai_model_region)\n",
        "print(f\"✅ AI API key created\\n\")\n",
        "\n",
        "# Step 12: Set environment variables\n",
        "print(\"\\n⚙️ Step 12: Setting Environment Variables\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "# Set all environment variables for subsequent cells\n",
        "# Ensure connection string has proper protocol\n",
        "if not cluster_conn_string.startswith(\"couchbase://\") and not cluster_conn_string.startswith(\"couchbases://\"):\n",
        "    cluster_conn_string = f\"couchbases://{cluster_conn_string}\"\n",
        "    print(f\"⚠️  Added protocol to connection string: {cluster_conn_string}\")\n",
        "\n",
        "os.environ[\"CB_CONN_STRING\"] = cluster_conn_string + \"?tls_verify=none\"\n",
        "os.environ[\"CB_USERNAME\"] = config.db_username\n",
        "os.environ[\"CB_PASSWORD\"] = db_password\n",
        "os.environ[\"CB_BUCKET\"] = config.sample_bucket\n",
        "os.environ[\"CAPELLA_API_ENDPOINT\"] = embedding_endpoint  # Use as base endpoint\n",
        "os.environ[\"CAPELLA_API_EMBEDDING_ENDPOINT\"] = embedding_endpoint\n",
        "os.environ[\"CAPELLA_API_LLM_ENDPOINT\"] = llm_endpoint\n",
        "os.environ[\"CAPELLA_API_EMBEDDINGS_KEY\"] = api_key\n",
        "os.environ[\"CAPELLA_API_LLM_KEY\"] = api_key\n",
        "os.environ[\"CAPELLA_API_EMBEDDING_MODEL\"] = config.embedding_model_name\n",
        "os.environ[\"CAPELLA_API_LLM_MODEL\"] = config.llm_model_name\n",
        "\n",
        "print(\"✅ Environment variables configured:\\n\")\n",
        "print(f\"   CB_CONN_STRING: {cluster_conn_string}\")\n",
        "print(f\"   CB_USERNAME: {config.db_username}\")\n",
        "print(f\"   CB_BUCKET: {config.sample_bucket}\")\n",
        "print(f\"   CAPELLA_API_EMBEDDING_ENDPOINT: {embedding_endpoint}\")\n",
        "print(f\"   CAPELLA_API_LLM_ENDPOINT: {llm_endpoint}\")\n",
        "print(f\"   CAPELLA_API_EMBEDDING_MODEL: {config.embedding_model_name}\")\n",
        "print(f\"   CAPELLA_API_LLM_MODEL: {config.llm_model_name}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"✅ Infrastructure Setup Complete!\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nYou can now run the flight search agent cells below.\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cNe4nUmgH14"
      },
      "outputs": [],
      "source": [
        "# Set Agent Catalog environment variables (required for agentc commands)\n",
        "# These use the same Couchbase connection created above\n",
        "import os\n",
        "\n",
        "# Strip the ?tls_verify=none from the connection string for agentc\n",
        "conn_string = os.environ[\"CB_CONN_STRING\"]\n",
        "if conn_string.endswith(\"?tls_verify=none\"):\n",
        "    conn_string_clean = conn_string[:-len(\"?tls_verify=none\")]\n",
        "else:\n",
        "    conn_string_clean = conn_string\n",
        "\n",
        "# Ensure connection string has proper protocol (agentc requires couchbase:// or couchbases://)\n",
        "if not conn_string_clean.startswith(\"couchbase://\") and not conn_string_clean.startswith(\"couchbases://\"):\n",
        "    # Add couchbases:// protocol for secure connections\n",
        "    conn_string_clean = f\"couchbases://{conn_string_clean}\"\n",
        "    print(f\"⚠️  Added protocol to connection string: {conn_string_clean}\")\n",
        "\n",
        "os.environ[\"AGENT_CATALOG_CONN_STRING\"] = conn_string_clean\n",
        "\n",
        "os.environ[\"AGENT_CATALOG_USERNAME\"] = os.environ[\"CB_USERNAME\"]\n",
        "os.environ[\"AGENT_CATALOG_PASSWORD\"] = os.environ[\"CB_PASSWORD\"]\n",
        "os.environ[\"AGENT_CATALOG_BUCKET\"] = os.environ[\"CB_BUCKET\"]\n",
        "\n",
        "print(\"✅ Agent Catalog environment variables set:\")\n",
        "print(f\"   AGENT_CATALOG_CONN_STRING: {os.environ['AGENT_CATALOG_CONN_STRING']}\")\n",
        "print(f\"   AGENT_CATALOG_USERNAME: {os.environ['AGENT_CATALOG_USERNAME']}\")\n",
        "print(f\"   AGENT_CATALOG_BUCKET: {os.environ['AGENT_CATALOG_BUCKET']}\")\n",
        "\n",
        "# Handle root certificate (required for secure connections)\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"📜 Root Certificate Setup\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\n⚠️  IMPORTANT: You need to download the root certificate from Capella UI\")\n",
        "print(\"\\nSteps:\")\n",
        "print(\"1. Go to Capella Console: https://cloud.couchbase.com\")\n",
        "print(\"2. Navigate to your cluster → Connect tab\")\n",
        "print(\"3. Download the 'Root Certificate' file\")\n",
        "print(\"4. Upload it using the file upload below\\n\")\n",
        "\n",
        "# Try to use Google Colab's file upload, fallback to manual input\n",
        "try:\n",
        "    from google.colab import files\n",
        "    print(\"📤 Please upload your root certificate file:\")\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    if uploaded:\n",
        "        cert_filename = list(uploaded.keys())[0]\n",
        "        # Validate it's actually a certificate file\n",
        "        if cert_filename.endswith(('.pem', '.crt', '.cer', '.txt')):\n",
        "            os.environ[\"AGENT_CATALOG_CONN_ROOT_CERTIFICATE\"] = cert_filename\n",
        "            print(f\"\\n✅ Root certificate uploaded: {cert_filename}\")\n",
        "            print(f\"   AGENT_CATALOG_CONN_ROOT_CERTIFICATE: {cert_filename}\")\n",
        "        else:\n",
        "            print(f\"\\n⚠️  Uploaded file '{cert_filename}' doesn't appear to be a certificate (.pem, .crt, .cer, .txt)\")\n",
        "            print(\"   Skipping certificate setup. You can configure it later if needed.\")\n",
        "            os.environ[\"AGENT_CATALOG_CONN_ROOT_CERTIFICATE\"] = \"\"\n",
        "    else:\n",
        "        print(\"\\n⚠️  No file uploaded. You can set it manually later if needed.\")\n",
        "        os.environ[\"AGENT_CATALOG_CONN_ROOT_CERTIFICATE\"] = \"\"\n",
        "except ImportError:\n",
        "    # Not in Colab - ask user to place file and provide filename\n",
        "    print(\"📝 Not running in Google Colab.\")\n",
        "    print(\"   Please place the root certificate file in the current directory.\\n\")\n",
        "    cert_filename = input(\"Enter the certificate filename (or press Enter to skip): \").strip()\n",
        "\n",
        "    if cert_filename:\n",
        "        os.environ[\"AGENT_CATALOG_CONN_ROOT_CERTIFICATE\"] = cert_filename\n",
        "        print(f\"\\n✅ Root certificate set: {cert_filename}\")\n",
        "    else:\n",
        "        print(\"\\n⚠️  Root certificate not set. You can add it manually later if needed.\")\n",
        "        os.environ[\"AGENT_CATALOG_CONN_ROOT_CERTIFICATE\"] = \"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"✅ Agent Catalog Configuration Complete\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Write environment variables to .env file for agentc commands\n",
        "# agentc CLI will load from .env file automatically\n",
        "import os.path\n",
        "with open('.env', 'w') as f:\n",
        "    # CB variables (needed for database operations - prevents wiping by dotenv.load_dotenv)\n",
        "    f.write(f\"CB_CONN_STRING={os.environ['CB_CONN_STRING']}\\n\")\n",
        "    f.write(f\"CB_USERNAME={os.environ['CB_USERNAME']}\\n\")\n",
        "    f.write(f\"CB_PASSWORD={os.environ['CB_PASSWORD']}\\n\")\n",
        "    f.write(f\"CB_BUCKET={os.environ['CB_BUCKET']}\\n\")\n",
        "    \n",
        "    # CAPELLA_API variables (needed for AI services - prevents wiping by dotenv.load_dotenv)\n",
        "    f.write(f\"CAPELLA_API_ENDPOINT={os.environ['CAPELLA_API_ENDPOINT']}\\n\")\n",
        "    f.write(f\"CAPELLA_API_EMBEDDING_ENDPOINT={os.environ['CAPELLA_API_EMBEDDING_ENDPOINT']}\\n\")\n",
        "    f.write(f\"CAPELLA_API_LLM_ENDPOINT={os.environ['CAPELLA_API_LLM_ENDPOINT']}\\n\")\n",
        "    f.write(f\"CAPELLA_API_EMBEDDINGS_KEY={os.environ['CAPELLA_API_EMBEDDINGS_KEY']}\\n\")\n",
        "    f.write(f\"CAPELLA_API_LLM_KEY={os.environ['CAPELLA_API_LLM_KEY']}\\n\")\n",
        "    f.write(f\"CAPELLA_API_EMBEDDING_MODEL={os.environ['CAPELLA_API_EMBEDDING_MODEL']}\\n\")\n",
        "    f.write(f\"CAPELLA_API_LLM_MODEL={os.environ['CAPELLA_API_LLM_MODEL']}\\n\")\n",
        "    \n",
        "    # AGENT_CATALOG variables (for agentc CLI)\n",
        "    f.write(f\"AGENT_CATALOG_CONN_STRING={os.environ['AGENT_CATALOG_CONN_STRING']}\\n\")\n",
        "    f.write(f\"AGENT_CATALOG_USERNAME={os.environ['AGENT_CATALOG_USERNAME']}\\n\")\n",
        "    f.write(f\"AGENT_CATALOG_PASSWORD={os.environ['AGENT_CATALOG_PASSWORD']}\\n\")\n",
        "    f.write(f\"AGENT_CATALOG_BUCKET={os.environ['AGENT_CATALOG_BUCKET']}\\n\")\n",
        "    \n",
        "    # Only write certificate if it exists and is a valid file\n",
        "    cert = os.environ.get('AGENT_CATALOG_CONN_ROOT_CERTIFICATE', '').strip()\n",
        "    if cert and os.path.isfile(cert):\n",
        "        f.write(f\"AGENT_CATALOG_CONN_ROOT_CERTIFICATE={cert}\\n\")\n",
        "    elif cert:\n",
        "        print(f\"⚠️  Warning: Certificate file '{cert}' not found, skipping from .env\")\n",
        "\n",
        "print(\"\\n✅ Environment variables written to .env file for agentc commands\")\n",
        "\n",
        "# Verify .env file was created correctly\n",
        "print(\"\\n🔍 Verifying .env file contents:\")\n",
        "!pwd\n",
        "!ls -la .env\n",
        "print(\"\\nFirst 5 lines of .env (passwords masked):\")\n",
        "with open('.env', 'r') as f:\n",
        "    for i, line in enumerate(f):\n",
        "        if i < 5:\n",
        "            if 'PASSWORD' in line:\n",
        "                print(f\"  {line.split('=')[0]}=***\")\n",
        "            else:\n",
        "                print(f\"  {line.strip()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Optional: Configure OpenAI and Arize (Observability)\n",
        "\n",
        "Provide optional API keys for:\n",
        "- **OpenAI**: Fallback LLM/embeddings if Capella AI is unavailable\n",
        "- **Arize Phoenix**: Observability and evaluation platform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"🔧 Optional API Keys Configuration\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# OpenAI Configuration (optional - for fallback)\n",
        "print(\"\\n📝 OpenAI API (Optional - for fallback LLM/embeddings)\")\n",
        "print(\"-\"*70)\n",
        "openai_api_key = input(\"Enter OpenAI API Key (or press Enter to skip): \").strip()\n",
        "\n",
        "if openai_api_key:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
        "    os.environ[\"OPENAI_MODEL\"] = \"gpt-4o\"  # Default model\n",
        "    print(\"✅ OpenAI API key configured\")\n",
        "    print(f\"   Model: gpt-4o\")\n",
        "else:\n",
        "    print(\"⏭️  Skipped OpenAI configuration\")\n",
        "    os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "    os.environ[\"OPENAI_MODEL\"] = \"gpt-4o\"\n",
        "\n",
        "# Arize Phoenix Configuration (optional - for observability)\n",
        "print(\"\\n📊 Arize Phoenix (Optional - for observability and evaluation)\")\n",
        "print(\"-\"*70)\n",
        "arize_space_id = input(\"Enter Arize Space ID (or press Enter to skip): \").strip()\n",
        "arize_api_key = input(\"Enter Arize API Key (or press Enter to skip): \").strip()\n",
        "\n",
        "if arize_space_id and arize_api_key:\n",
        "    os.environ[\"ARIZE_SPACE_ID\"] = arize_space_id\n",
        "    os.environ[\"ARIZE_API_KEY\"] = arize_api_key\n",
        "    print(\"✅ Arize Phoenix configured\")\n",
        "else:\n",
        "    print(\"⏭️  Skipped Arize configuration\")\n",
        "    os.environ[\"ARIZE_SPACE_ID\"] = \"\"\n",
        "    os.environ[\"ARIZE_API_KEY\"] = \"\"\n",
        "\n",
        "# Append optional variables to .env file\n",
        "with open('.env', 'a') as f:\n",
        "    f.write(\"\\n# Optional: OpenAI Configuration (fallback LLM/embeddings)\\n\")\n",
        "    f.write(f\"OPENAI_API_KEY={os.environ['OPENAI_API_KEY']}\\n\")\n",
        "    f.write(f\"OPENAI_MODEL={os.environ['OPENAI_MODEL']}\\n\")\n",
        "    \n",
        "    f.write(\"\\n# Optional: Arize Phoenix (observability and evaluation)\\n\")\n",
        "    f.write(f\"ARIZE_SPACE_ID={os.environ['ARIZE_SPACE_ID']}\\n\")\n",
        "    f.write(f\"ARIZE_API_KEY={os.environ['ARIZE_API_KEY']}\\n\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"✅ Optional Configuration Complete\")\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Czit8mxTgH14"
      },
      "outputs": [],
      "source": [
        "!git init\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-dza6HeygH14"
      },
      "outputs": [],
      "source": [
        "!git add .\n",
        "!git config --global user.email \"your.email@example.com\"\n",
        "!git config --global user.name \"Your Name\"\n",
        "!git commit -m \"initial commit\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TrqZAWxagH14"
      },
      "outputs": [],
      "source": [
        "!agentc init\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NefIjKW_gH14"
      },
      "outputs": [],
      "source": [
        "!agentc index .\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OgV2nmNpgH14"
      },
      "outputs": [],
      "source": [
        "!agentc publish\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abGx3vEkmTCr",
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Setup and Imports\n",
        "\n",
        "Import all necessary modules for the flight search agent using the latest code structure.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fp4GDdukmTCr"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import logging\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "from datetime import timedelta\n",
        "\n",
        "import agentc\n",
        "import agentc_langgraph.agent\n",
        "import agentc_langgraph.graph\n",
        "import dotenv\n",
        "import langchain_core.messages\n",
        "import langchain_core.runnables\n",
        "import langchain_openai.chat_models\n",
        "import langgraph.graph\n",
        "from couchbase.auth import PasswordAuthenticator\n",
        "from couchbase.cluster import Cluster\n",
        "from couchbase.exceptions import KeyspaceNotFoundException\n",
        "from couchbase.options import ClusterOptions\n",
        "from langchain.agents import AgentExecutor, create_react_agent\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.tools import Tool\n",
        "from pydantic import SecretStr\n",
        "\n",
        "# Setup logging for Colab (must use stdout instead of stderr)\n",
        "root_logger = logging.getLogger()\n",
        "if not root_logger.handlers:\n",
        "    handler = logging.StreamHandler(sys.stdout)\n",
        "    formatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n",
        "    handler.setFormatter(formatter)\n",
        "    root_logger.addHandler(handler)\n",
        "root_logger.setLevel(logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Suppress verbose logging from external libraries\n",
        "logging.getLogger(\"openai\").setLevel(logging.WARNING)\n",
        "logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n",
        "logging.getLogger(\"httpcore\").setLevel(logging.WARNING)\n",
        "logging.getLogger(\"agentc_core\").setLevel(logging.WARNING)\n",
        "\n",
        "# Load environment variables\n",
        "dotenv.load_dotenv(override=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7AQo_w5mTCs",
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Project Root Discovery and Shared Imports\n",
        "\n",
        "Essential project root discovery and shared module imports exactly as in working main.py.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2f3bgI_mTCs"
      },
      "outputs": [],
      "source": [
        "# INLINE IMPLEMENTATION - Universal AI Services and Couchbase Client\n",
        "import base64\n",
        "import time\n",
        "from typing import Tuple, Any, Optional, List\n",
        "from datetime import timedelta\n",
        "from couchbase.auth import PasswordAuthenticator\n",
        "from couchbase.cluster import Cluster\n",
        "from couchbase.exceptions import KeyspaceNotFoundException\n",
        "from couchbase.management.buckets import BucketType, CreateBucketSettings\n",
        "from couchbase.management.search import SearchIndex\n",
        "from couchbase.options import ClusterOptions\n",
        "\n",
        "# We'll add the function implementations in the next cell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umeyMLs0mTCs",
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Universal AI Services Setup\n",
        "\n",
        "5-case priority AI service setup implementation inline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3EuELHqmTCs"
      },
      "outputs": [],
      "source": [
        "def setup_ai_services(\n",
        "    framework: str = \"langchain\",\n",
        "    temperature: float = 0.0,\n",
        "    callbacks: Optional[List] = None,\n",
        "    application_span: Optional[Any] = None\n",
        ") -> Tuple[Any, Any]:\n",
        "    \"\"\"Priority 1 AI service setup - Capella with direct API keys + OpenAI wrappers only.\"\"\"\n",
        "    embeddings = None\n",
        "    llm = None\n",
        "\n",
        "    logger.info(f\"🔧 Setting up Priority 1 AI services for {framework} framework...\")\n",
        "\n",
        "    # PRIORITY 1: LATEST CAPELLA (OpenAI wrappers with direct API keys)\n",
        "    if (\n",
        "        not embeddings\n",
        "        and os.getenv(\"CAPELLA_API_ENDPOINT\")\n",
        "        and os.getenv(\"CAPELLA_API_EMBEDDINGS_KEY\")\n",
        "    ):\n",
        "        try:\n",
        "            if framework == \"llamaindex\":\n",
        "                from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "                embeddings = OpenAIEmbedding(\n",
        "                    api_key=os.getenv(\"CAPELLA_API_EMBEDDINGS_KEY\"),\n",
        "                    api_base=f\"{os.getenv('CAPELLA_API_ENDPOINT')}/v1\",\n",
        "                    model_name=os.getenv(\"CAPELLA_API_EMBEDDING_MODEL\"),\n",
        "                    embed_batch_size=30,\n",
        "                )\n",
        "            else:  # langchain, langgraph\n",
        "                from langchain_openai import OpenAIEmbeddings\n",
        "                embeddings = OpenAIEmbeddings(\n",
        "                    model=os.getenv(\"CAPELLA_API_EMBEDDING_MODEL\"),\n",
        "                    api_key=os.getenv(\"CAPELLA_API_EMBEDDINGS_KEY\"),\n",
        "                    base_url=f\"{os.getenv('CAPELLA_API_ENDPOINT')}/v1\",\n",
        "                    check_embedding_ctx_length=False,  # Fix for asymmetric models\n",
        "                )\n",
        "            logger.info(\"✅ Using latest Capella AI embeddings (direct API key + OpenAI wrapper)\")\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"⚠️ Latest Capella AI embeddings failed: {e}\")\n",
        "\n",
        "    if (\n",
        "        not llm\n",
        "        and os.getenv(\"CAPELLA_API_ENDPOINT\")\n",
        "        and os.getenv(\"CAPELLA_API_LLM_KEY\")\n",
        "    ):\n",
        "        try:\n",
        "            if framework == \"llamaindex\":\n",
        "                from llama_index.llms.openai_like import OpenAILike\n",
        "                llm = OpenAILike(\n",
        "                    model=os.getenv(\"CAPELLA_API_LLM_MODEL\"),\n",
        "                    api_base=f\"{os.getenv('CAPELLA_API_ENDPOINT')}/v1\",\n",
        "                    api_key=os.getenv(\"CAPELLA_API_LLM_KEY\"),\n",
        "                    is_chat_model=True,\n",
        "                    temperature=temperature,\n",
        "                )\n",
        "            else:  # langchain, langgraph\n",
        "                from langchain_openai import ChatOpenAI\n",
        "\n",
        "                chat_kwargs = {\n",
        "                    \"api_key\": os.getenv(\"CAPELLA_API_LLM_KEY\"),\n",
        "                    \"base_url\": f\"{os.getenv('CAPELLA_API_ENDPOINT')}/v1\",\n",
        "                    \"model\": os.getenv(\"CAPELLA_API_LLM_MODEL\"),\n",
        "                    \"temperature\": temperature,\n",
        "                }\n",
        "                if callbacks:\n",
        "                    chat_kwargs[\"callbacks\"] = callbacks\n",
        "\n",
        "                llm = ChatOpenAI(**chat_kwargs)\n",
        "\n",
        "            # Test the LLM works\n",
        "            if framework == \"llamaindex\":\n",
        "                llm.complete(\"Hello\")\n",
        "            else:\n",
        "                llm.invoke(\"Hello\")\n",
        "\n",
        "            logger.info(\"✅ Using latest Capella AI LLM (direct API key + OpenAI wrapper)\")\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"⚠️ Latest Capella AI LLM failed: {e}\")\n",
        "            llm = None\n",
        "\n",
        "    # VALIDATION\n",
        "    if not embeddings:\n",
        "        raise ValueError(\"❌ No embeddings service could be initialized\")\n",
        "    if not llm:\n",
        "        raise ValueError(\"❌ No LLM service could be initialized\")\n",
        "\n",
        "    logger.info(f\"✅ AI services setup completed for {framework}\")\n",
        "    return embeddings, llm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkfKX0OpmTCt",
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Helper Functions\n",
        "\n",
        "Environment setup and connectivity test functions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dU591dHNmTCt"
      },
      "outputs": [],
      "source": [
        "def setup_environment():\n",
        "    \"\"\"Setup default environment variables for agent operations.\"\"\"\n",
        "    defaults = {\n",
        "        \"CB_BUCKET\": \"travel-sample\",\n",
        "        \"CB_SCOPE\": \"agentc_data\",\n",
        "        \"CB_COLLECTION\": \"airline_reviews\",\n",
        "        \"CB_INDEX\": \"airline_reviews_index\",\n",
        "        \"CAPELLA_API_EMBEDDING_MODEL\": \"nvidia/nv-embedqa-e5-v5\",\n",
        "        \"CAPELLA_API_LLM_MODEL\": \"meta/llama-3-8b-instruct\",\n",
        "        \"CAPELLA_API_EMBEDDING_MAX_TOKENS\": \"512\",\n",
        "        \"NVIDIA_API_EMBEDDING_MODEL\": \"nvidia/nv-embedqa-e5-v5\",\n",
        "        \"NVIDIA_API_LLM_MODEL\": \"meta/llama-3.1-70b-instruct\",\n",
        "    }\n",
        "\n",
        "    for key, value in defaults.items():\n",
        "        if not os.getenv(key):\n",
        "            os.environ[key] = value\n",
        "\n",
        "    logger.info(\"✅ Environment variables configured\")\n",
        "\n",
        "\n",
        "def test_capella_connectivity(api_key: str = None, endpoint: str = None) -> bool:\n",
        "    \"\"\"Test connectivity to Capella AI services.\"\"\"\n",
        "    try:\n",
        "        import httpx\n",
        "\n",
        "        test_key = api_key or os.getenv(\"CAPELLA_API_EMBEDDINGS_KEY\") or os.getenv(\"CAPELLA_API_LLM_KEY\")\n",
        "        test_endpoint = endpoint or os.getenv(\"CAPELLA_API_ENDPOINT\")\n",
        "\n",
        "        if not test_key or not test_endpoint:\n",
        "            return False\n",
        "\n",
        "        headers = {\"Authorization\": f\"Bearer {test_key}\"}\n",
        "\n",
        "        with httpx.Client(timeout=10.0) as client:\n",
        "            response = client.get(f\"{test_endpoint.rstrip('/')}/v1/models\", headers=headers)\n",
        "            return response.status_code < 500\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"⚠️ Capella connectivity test failed: {e}\")\n",
        "        return False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-8xkvOnmTCt",
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Universal Couchbase Client\n",
        "\n",
        "Complete Couchbase client implementation with database operations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cz7oscM9mTCt"
      },
      "outputs": [],
      "source": [
        "class CouchbaseClient:\n",
        "    \"\"\"Universal Couchbase client for all database operations across agent frameworks.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        conn_string: str,\n",
        "        username: str,\n",
        "        password: str,\n",
        "        bucket_name: str,\n",
        "        wan_profile: bool = True,\n",
        "        timeout_seconds: int = 20,\n",
        "    ):\n",
        "        \"\"\"Initialize Couchbase client with connection details.\"\"\"\n",
        "        self.conn_string = conn_string\n",
        "        self.username = username\n",
        "        self.password = password\n",
        "        self.bucket_name = bucket_name\n",
        "        self.wan_profile = wan_profile\n",
        "        self.timeout_seconds = timeout_seconds\n",
        "        self.cluster = None\n",
        "        self.bucket = None\n",
        "        self._collections = {}\n",
        "\n",
        "    def connect(self):\n",
        "        \"\"\"Establish connection to Couchbase cluster.\"\"\"\n",
        "        try:\n",
        "            auth = PasswordAuthenticator(self.username, self.password)\n",
        "            options = ClusterOptions(auth)\n",
        "\n",
        "            if self.wan_profile:\n",
        "                options.apply_profile(\"wan_development\")\n",
        "\n",
        "            self.cluster = Cluster(self.conn_string, options)\n",
        "            self.cluster.wait_until_ready(timedelta(seconds=self.timeout_seconds))\n",
        "            logger.info(\"✅ Successfully connected to Couchbase\")\n",
        "            return self.cluster\n",
        "        except Exception as e:\n",
        "            raise ConnectionError(f\"❌ Failed to connect to Couchbase: {e!s}\")\n",
        "\n",
        "    def setup_bucket(self, create_if_missing: bool = True):\n",
        "        \"\"\"Setup bucket - connect to existing or create if missing.\"\"\"\n",
        "        try:\n",
        "            if not self.cluster:\n",
        "                self.connect()\n",
        "\n",
        "            try:\n",
        "                self.bucket = self.cluster.bucket(self.bucket_name)\n",
        "                logger.info(f\"✅ Connected to existing bucket '{self.bucket_name}'\")\n",
        "                return self.bucket\n",
        "            except Exception as e:\n",
        "                logger.info(f\"⚠️ Bucket '{self.bucket_name}' not accessible: {e}\")\n",
        "\n",
        "            if create_if_missing:\n",
        "                logger.info(f\"🔧 Creating bucket '{self.bucket_name}'...\")\n",
        "                bucket_settings = CreateBucketSettings(\n",
        "                    name=self.bucket_name,\n",
        "                    bucket_type=BucketType.COUCHBASE,\n",
        "                    ram_quota_mb=1024,\n",
        "                    flush_enabled=True,\n",
        "                    num_replicas=0,\n",
        "                )\n",
        "                self.cluster.buckets().create_bucket(bucket_settings)\n",
        "                time.sleep(5)\n",
        "                self.bucket = self.cluster.bucket(self.bucket_name)\n",
        "                logger.info(f\"✅ Bucket '{self.bucket_name}' created successfully\")\n",
        "                return self.bucket\n",
        "            else:\n",
        "                raise RuntimeError(f\"❌ Bucket '{self.bucket_name}' not found\")\n",
        "\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"❌ Error setting up bucket: {e!s}\")\n",
        "\n",
        "    def setup_collection(self, scope_name: str, collection_name: str, clear_existing_data: bool = True, create_primary_index: bool = True):\n",
        "        \"\"\"Setup collection with comprehensive options.\"\"\"\n",
        "        try:\n",
        "            if not self.bucket:\n",
        "                self.setup_bucket()\n",
        "\n",
        "            bucket_manager = self.bucket.collections()\n",
        "            scopes = bucket_manager.get_all_scopes()\n",
        "            scope_exists = any(scope.name == scope_name for scope in scopes)\n",
        "\n",
        "            if not scope_exists and scope_name != \"_default\":\n",
        "                logger.info(f\"🔧 Creating scope '{scope_name}'...\")\n",
        "                bucket_manager.create_scope(scope_name)\n",
        "                logger.info(f\"✅ Scope '{scope_name}' created successfully\")\n",
        "\n",
        "            collections = bucket_manager.get_all_scopes()\n",
        "            collection_exists = any(\n",
        "                scope.name == scope_name\n",
        "                and collection_name in [col.name for col in scope.collections]\n",
        "                for scope in collections\n",
        "            )\n",
        "\n",
        "            if collection_exists:\n",
        "                if clear_existing_data:\n",
        "                    logger.info(f\"🗑️ Collection '{collection_name}' exists, clearing data...\")\n",
        "                    self.clear_collection_data(scope_name, collection_name)\n",
        "                else:\n",
        "                    logger.info(f\"ℹ️ Collection '{collection_name}' exists, keeping existing data\")\n",
        "            else:\n",
        "                logger.info(f\"🔧 Creating collection '{collection_name}'...\")\n",
        "                bucket_manager.create_collection(scope_name, collection_name)\n",
        "                logger.info(f\"✅ Collection '{collection_name}' created successfully\")\n",
        "\n",
        "            time.sleep(3)\n",
        "\n",
        "            if create_primary_index:\n",
        "                try:\n",
        "                    self.cluster.query(\n",
        "                        f\"CREATE PRIMARY INDEX IF NOT EXISTS ON `{self.bucket_name}`.`{scope_name}`.`{collection_name}`\"\n",
        "                    ).execute()\n",
        "                    logger.info(\"✅ Primary index created successfully\")\n",
        "                except Exception as e:\n",
        "                    logger.warning(f\"⚠️ Error creating primary index: {e}\")\n",
        "\n",
        "            collection_key = f\"{scope_name}.{collection_name}\"\n",
        "            collection = self.bucket.scope(scope_name).collection(collection_name)\n",
        "            self._collections[collection_key] = collection\n",
        "\n",
        "            logger.info(f\"✅ Collection setup complete: {scope_name}.{collection_name}\")\n",
        "            return collection\n",
        "\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"❌ Error setting up collection: {e!s}\")\n",
        "\n",
        "    def clear_collection_data(self, scope_name: str, collection_name: str, verify_cleared: bool = True):\n",
        "        \"\"\"Clear all data from a collection.\"\"\"\n",
        "        try:\n",
        "            logger.info(f\"🗑️ Clearing data from {self.bucket_name}.{scope_name}.{collection_name}...\")\n",
        "\n",
        "            delete_query = f\"DELETE FROM `{self.bucket_name}`.`{scope_name}`.`{collection_name}`\"\n",
        "            result = self.cluster.query(delete_query)\n",
        "            list(result)\n",
        "\n",
        "            time.sleep(2)\n",
        "\n",
        "            if verify_cleared:\n",
        "                count_query = f\"SELECT COUNT(*) as count FROM `{self.bucket_name}`.`{scope_name}`.`{collection_name}`\"\n",
        "                count_result = self.cluster.query(count_query)\n",
        "                count_row = list(count_result)[0]\n",
        "                remaining_count = count_row[\"count\"]\n",
        "\n",
        "                if remaining_count == 0:\n",
        "                    logger.info(f\"✅ Collection cleared successfully\")\n",
        "                else:\n",
        "                    logger.warning(f\"⚠️ Collection clear incomplete, {remaining_count} documents remaining\")\n",
        "\n",
        "        except KeyspaceNotFoundException:\n",
        "            logger.info(f\"ℹ️ Collection doesn't exist, nothing to clear\")\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"⚠️ Error clearing collection data: {e}\")\n",
        "\n",
        "    def clear_scope(self, scope_name: str):\n",
        "        \"\"\"Clear all collections in the specified scope.\"\"\"\n",
        "        try:\n",
        "            if not self.bucket:\n",
        "                self.setup_bucket()\n",
        "\n",
        "            logger.info(f\"🗑️ Clearing scope: {self.bucket_name}.{scope_name}\")\n",
        "            bucket_manager = self.bucket.collections()\n",
        "            scopes = bucket_manager.get_all_scopes()\n",
        "\n",
        "            target_scope = None\n",
        "            for scope in scopes:\n",
        "                if scope.name == scope_name:\n",
        "                    target_scope = scope\n",
        "                    break\n",
        "\n",
        "            if not target_scope:\n",
        "                logger.info(f\"ℹ️ Scope '{self.bucket_name}.{scope_name}' does not exist\")\n",
        "                return\n",
        "\n",
        "            for collection in target_scope.collections:\n",
        "                try:\n",
        "                    self.clear_collection_data(scope_name, collection.name, verify_cleared=False)\n",
        "                    logger.info(f\"✅ Cleared collection: {collection.name}\")\n",
        "                except Exception as e:\n",
        "                    logger.warning(f\"⚠️ Could not clear collection {collection.name}: {e}\")\n",
        "\n",
        "            logger.info(f\"✅ Completed clearing scope: {self.bucket_name}.{scope_name}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"❌ Could not clear scope: {e}\")\n",
        "\n",
        "    def setup_vector_search_index(self, index_definition: dict, scope_name: str):\n",
        "        \"\"\"Setup vector search index for the specified scope.\"\"\"\n",
        "        try:\n",
        "            if not self.bucket:\n",
        "                raise RuntimeError(\"❌ Bucket not initialized. Call setup_bucket first.\")\n",
        "\n",
        "            scope_index_manager = self.bucket.scope(scope_name).search_indexes()\n",
        "            existing_indexes = scope_index_manager.get_all_indexes()\n",
        "            index_name = index_definition[\"name\"]\n",
        "\n",
        "            if index_name not in [index.name for index in existing_indexes]:\n",
        "                logger.info(f\"🔧 Creating vector search index '{index_name}'...\")\n",
        "                search_index = SearchIndex.from_json(index_definition)\n",
        "                scope_index_manager.upsert_index(search_index)\n",
        "                logger.info(f\"✅ Vector search index '{index_name}' created successfully\")\n",
        "            else:\n",
        "                logger.info(f\"ℹ️ Vector search index '{index_name}' already exists\")\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"❌ Error setting up vector search index: {e!s}\")\n",
        "\n",
        "    def load_index_definition(self, index_file_path: str = \"agentcatalog_index.json\"):\n",
        "        \"\"\"Load vector search index definition from JSON file.\"\"\"\n",
        "        try:\n",
        "            with open(index_file_path) as file:\n",
        "                index_definition = json.load(file)\n",
        "            logger.info(f\"✅ Loaded index definition from {index_file_path}\")\n",
        "            return index_definition\n",
        "        except FileNotFoundError:\n",
        "            logger.warning(f\"⚠️ {index_file_path} not found\")\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"⚠️ Error loading index definition: {e!s}\")\n",
        "            return None\n",
        "\n",
        "    def setup_vector_store_langchain(self, scope_name: str, collection_name: str, index_name: str, embeddings, data_loader_func=None, **loader_kwargs):\n",
        "        \"\"\"Setup LangChain CouchbaseSearchVectorStore with optional data loading.\"\"\"\n",
        "        try:\n",
        "            from langchain_couchbase.vectorstores import CouchbaseSearchVectorStore\n",
        "\n",
        "            if data_loader_func:\n",
        "                logger.info(\"🔄 Loading data into vector store...\")\n",
        "                data_loader_func(\n",
        "                    cluster=self.cluster,\n",
        "                    bucket_name=self.bucket_name,\n",
        "                    scope_name=scope_name,\n",
        "                    collection_name=collection_name,\n",
        "                    embeddings=embeddings,\n",
        "                    index_name=index_name,\n",
        "                    **loader_kwargs,\n",
        "                )\n",
        "                logger.info(\"✅ Data loaded into vector store successfully\")\n",
        "\n",
        "            vector_store = CouchbaseSearchVectorStore(\n",
        "                cluster=self.cluster,\n",
        "                bucket_name=self.bucket_name,\n",
        "                scope_name=scope_name,\n",
        "                collection_name=collection_name,\n",
        "                embedding=embeddings,\n",
        "                index_name=index_name,\n",
        "            )\n",
        "\n",
        "            logger.info(f\"✅ LangChain vector store setup complete\")\n",
        "            return vector_store\n",
        "\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"❌ Error setting up LangChain vector store: {e!s}\")\n",
        "\n",
        "\n",
        "def create_couchbase_client(\n",
        "    conn_string: str = None,\n",
        "    username: str = None,\n",
        "    password: str = None,\n",
        "    bucket_name: str = None,\n",
        "    wan_profile: bool = True,\n",
        "    timeout_seconds: int = 20,\n",
        ") -> CouchbaseClient:\n",
        "    \"\"\"Factory function to create CouchbaseClient with environment variable defaults.\"\"\"\n",
        "    return CouchbaseClient(\n",
        "        conn_string=conn_string or os.getenv(\"CB_CONN_STRING\", \"couchbase://localhost\"),\n",
        "        username=username or os.getenv(\"CB_USERNAME\", \"Administrator\"),\n",
        "        password=password or os.getenv(\"CB_PASSWORD\", \"password\"),\n",
        "        bucket_name=bucket_name or os.getenv(\"CB_BUCKET\", \"travel-sample\"),\n",
        "        wan_profile=wan_profile,\n",
        "        timeout_seconds=timeout_seconds,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-F17qGvmTCu",
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Shared Flight Search Queries\n",
        "\n",
        "Exact code from queries.py - flight search queries and reference answers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78O608HUmTCu"
      },
      "outputs": [],
      "source": [
        "# Flight search queries (for evaluation and testing)\n",
        "FLIGHT_SEARCH_QUERIES = [\n",
        "    \"Find flights from JFK to LAX\",\n",
        "    \"Book a flight from LAX to JFK for tomorrow, 2 passengers, business class\",\n",
        "    \"Book an economy flight from JFK to MIA for next week, 1 passenger\",\n",
        "    \"Show me my current flight bookings\",\n",
        "    \"What do passengers say about SpiceJet's service quality?\",\n",
        "]\n",
        "\n",
        "# Comprehensive reference answers based on actual system responses\n",
        "FLIGHT_REFERENCE_ANSWERS = [\n",
        "    # Query 1: Flight search JFK to LAX\n",
        "    \"\"\"Available flights from JFK to LAX:\n",
        "\n",
        "1. AS flight from JFK to LAX using 321 762\n",
        "2. B6 flight from JFK to LAX using 320\n",
        "3. DL flight from JFK to LAX using 76W 752\n",
        "4. QF flight from JFK to LAX using 744\n",
        "5. AA flight from JFK to LAX using 32B 762\n",
        "6. UA flight from JFK to LAX using 757\n",
        "7. US flight from JFK to LAX using 32B 762\n",
        "8. VX flight from JFK to LAX using 320\"\"\",\n",
        "\n",
        "    # Query 2: Flight booking LAX to JFK for tomorrow, 2 passengers, business class\n",
        "    \"\"\"Flight Booking Confirmed!\n",
        "\n",
        "Booking ID: FL08061563CACD\n",
        "Route: LAX → JFK\n",
        "Departure Date: 2025-08-06\n",
        "Passengers: 2\n",
        "Class: business\n",
        "Total Price: $1500.00\n",
        "\n",
        "Next Steps:\n",
        "1. Check-in opens 24 hours before departure\n",
        "2. Arrive at airport 2 hours early for domestic flights\n",
        "3. Bring valid government-issued photo ID\n",
        "\n",
        "Thank you for choosing our airline!\"\"\",\n",
        "\n",
        "    # Query 3: Flight booking JFK to MIA for next week\n",
        "    \"\"\"Flight Booking Confirmed!\n",
        "\n",
        "Booking ID: FL08124E7B9C2A\n",
        "Route: JFK → MIA\n",
        "Departure Date: 2025-08-12\n",
        "Passengers: 1\n",
        "Class: economy\n",
        "Total Price: $250.00\n",
        "\n",
        "Next Steps:\n",
        "1. Check-in opens 24 hours before departure\n",
        "2. Arrive at airport 2 hours early for domestic flights\n",
        "3. Bring valid government-issued photo ID\n",
        "\n",
        "Thank you for choosing our airline!\"\"\",\n",
        "\n",
        "    # Query 4: Show current flight bookings\n",
        "    \"\"\"Your Current Bookings (2 found):\n",
        "\n",
        "Booking 1:\n",
        "  Booking ID: FL08061563CACD\n",
        "  Route: LAX → JFK\n",
        "  Date: 2025-08-06\n",
        "  Passengers: 2\n",
        "  Class: business\n",
        "  Total: $1500.00\n",
        "  Status: confirmed\n",
        "  Booked: 2025-08-05\n",
        "\n",
        "Booking 2:\n",
        "  Booking ID: FL08124E7B9C2A\n",
        "  Route: JFK → MIA\n",
        "  Date: 2025-08-12\n",
        "  Passengers: 1\n",
        "  Class: economy\n",
        "  Total: $250.00\n",
        "  Status: confirmed\n",
        "  Booked: 2025-08-05\"\"\",\n",
        "\n",
        "    # Query 5: SpiceJet service quality reviews\n",
        "    \"\"\"Found 5 relevant airline reviews for 'SpiceJet service':\n",
        "\n",
        "Review 1:\n",
        "Airline: SpiceJet. Title: \"Service is impeccable\". Review: ✅ Trip Verified | Much better than airbus models. Even the basic economy class has ambient lighting. Better personal air vents and better spotlights. Even overhead storage bins are good. Service is impeccable with proper care taken of guests...\n",
        "\n",
        "Review 2:\n",
        "Airline: SpiceJet. Title: \"good service by the crew\". Review: ✅ Trip Verified | I have had good service by the crew. It was amazing, the crew was very enthusiastic and warm welcome. It was one of the best services in my experience.. Rating: 10.0/10. Reviewer: K Mansour. Date: 10th August 2024. Recom...\n",
        "\n",
        "Review 3:\n",
        "Airline: SpiceJet. Title: \"outstanding service I experienced\". Review: Not Verified |  I wanted to take a moment to express my sincere thanks for the outstanding service I experienced on my recent flight from Pune to Delhi. SG-8937. From the moment I boarded, the warmth and friendliness of the air h...\n",
        "\n",
        "Review 4:\n",
        "Airline: SpiceJet. Title: \"efficient and warm onboard service\". Review: ✅ Trip Verified |  New Delhi to Kolkata. Delighted with the prompt, efficient and warm onboard service provided by the crew. Appreciate their efforts towards customer centricity.. Rating: 10.0/10. Reviewer: Debashis Roy. Date: 2...\n",
        "\n",
        "Review 5:\n",
        "Airline: SpiceJet. Title: \"Service is very good\". Review: Service is very good,  I am impressed with Miss Renu  who gave the best services ever. Thanks to Renu who is very sweet by her nature as well as her service. Rating: 9.0/10. Reviewer: Sanjay Patnaik. Date: 21st September 2023. Recommended: ye...\"\"\",\n",
        "]\n",
        "\n",
        "# Create dictionary for backward compatibility\n",
        "QUERY_REFERENCE_ANSWERS = {\n",
        "    query: answer for query, answer in zip(FLIGHT_SEARCH_QUERIES, FLIGHT_REFERENCE_ANSWERS)\n",
        "}\n",
        "\n",
        "def get_test_queries():\n",
        "    \"\"\"Return test queries for evaluation.\"\"\"\n",
        "    return FLIGHT_SEARCH_QUERIES\n",
        "\n",
        "def get_evaluation_queries():\n",
        "    \"\"\"Get queries for evaluation\"\"\"\n",
        "    return FLIGHT_SEARCH_QUERIES\n",
        "\n",
        "def get_all_queries():\n",
        "    \"\"\"Get all available queries\"\"\"\n",
        "    return FLIGHT_SEARCH_QUERIES\n",
        "\n",
        "def get_simple_queries():\n",
        "    \"\"\"Get simple queries for basic testing\"\"\"\n",
        "    return FLIGHT_SEARCH_QUERIES\n",
        "\n",
        "def get_flight_policy_queries():\n",
        "    \"\"\"Return flight policy queries (for backward compatibility).\"\"\"\n",
        "    return FLIGHT_SEARCH_QUERIES\n",
        "\n",
        "def get_reference_answer(query: str) -> str:\n",
        "    \"\"\"Get the correct reference answer for a given query\"\"\"\n",
        "    return QUERY_REFERENCE_ANSWERS.get(query, f\"No reference answer available for: {query}\")\n",
        "\n",
        "def get_all_query_references():\n",
        "    \"\"\"Get all query-reference pairs\"\"\"\n",
        "    return QUERY_REFERENCE_ANSWERS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyAUlqTImTCv",
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Airline Reviews Data Module\n",
        "\n",
        "Exact code from airline_reviews_data.py - data loading and processing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BzF7cnZTmTCv"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from langchain_couchbase.vectorstores import CouchbaseSearchVectorStore\n",
        "\n",
        "# Import kagglehub only when needed\n",
        "try:\n",
        "    import kagglehub\n",
        "except ImportError:\n",
        "    kagglehub = None\n",
        "\n",
        "class AirlineReviewsDataManager:\n",
        "    \"\"\"Manages airline reviews data loading, processing, and embedding.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self._raw_data_cache = None\n",
        "        self._processed_texts_cache = None\n",
        "\n",
        "    def load_raw_data(self):\n",
        "        \"\"\"Load raw airline reviews data from Kaggle dataset (with caching).\"\"\"\n",
        "        if self._raw_data_cache is not None:\n",
        "            return self._raw_data_cache\n",
        "\n",
        "        try:\n",
        "            if kagglehub is None:\n",
        "                raise ImportError(\"kagglehub is not available\")\n",
        "\n",
        "            logger.info(\"Downloading Indian Airlines Customer Reviews dataset from Kaggle...\")\n",
        "            path = kagglehub.dataset_download(\"jagathratchakan/indian-airlines-customer-reviews\")\n",
        "\n",
        "            csv_file = None\n",
        "            for file in os.listdir(path):\n",
        "                if file.endswith(\".csv\"):\n",
        "                    csv_file = os.path.join(path, file)\n",
        "                    break\n",
        "\n",
        "            if not csv_file:\n",
        "                msg = \"No CSV file found in downloaded dataset\"\n",
        "                raise FileNotFoundError(msg)\n",
        "\n",
        "            logger.info(f\"Loading reviews from {csv_file}\")\n",
        "            df = pd.read_csv(csv_file)\n",
        "\n",
        "            self._raw_data_cache = df.to_dict(\"records\")\n",
        "            logger.info(f\"Loaded {len(self._raw_data_cache)} airline reviews from Kaggle dataset\")\n",
        "            return self._raw_data_cache\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.exception(f\"Error loading airline reviews from Kaggle: {e!s}\")\n",
        "            raise\n",
        "\n",
        "    def process_to_texts(self):\n",
        "        \"\"\"Process raw data into formatted text strings for embedding (with caching).\"\"\"\n",
        "        if self._processed_texts_cache is not None:\n",
        "            return self._processed_texts_cache\n",
        "\n",
        "        reviews = self.load_raw_data()\n",
        "        review_texts = []\n",
        "\n",
        "        for review in reviews:\n",
        "            text_parts = []\n",
        "\n",
        "            if review.get(\"AirLine_Name\"):\n",
        "                text_parts.append(f\"Airline: {review['AirLine_Name']}\")\n",
        "\n",
        "            if review.get(\"Title\"):\n",
        "                text_parts.append(f\"Title: {review['Title']}\")\n",
        "\n",
        "            if review.get(\"Review\"):\n",
        "                text_parts.append(f\"Review: {review['Review']}\")\n",
        "\n",
        "            if review.get(\"Rating - 10\"):\n",
        "                text_parts.append(f\"Rating: {review['Rating - 10']}/10\")\n",
        "\n",
        "            if review.get(\"Name\"):\n",
        "                text_parts.append(f\"Reviewer: {review['Name']}\")\n",
        "\n",
        "            if review.get(\"Date\"):\n",
        "                text_parts.append(f\"Date: {review['Date']}\")\n",
        "\n",
        "            if review.get(\"Recommond\"):\n",
        "                text_parts.append(f\"Recommended: {review['Recommond']}\")\n",
        "\n",
        "            text = \". \".join(text_parts)\n",
        "            review_texts.append(text)\n",
        "\n",
        "        self._processed_texts_cache = review_texts\n",
        "        logger.info(f\"Processed {len(review_texts)} airline reviews into text format\")\n",
        "        return review_texts\n",
        "\n",
        "    def load_to_vector_store(\n",
        "        self,\n",
        "        cluster,\n",
        "        bucket_name: str,\n",
        "        scope_name: str,\n",
        "        collection_name: str,\n",
        "        embeddings,\n",
        "        index_name: str,\n",
        "    ):\n",
        "        \"\"\"Load airline reviews into Couchbase vector store with embeddings.\"\"\"\n",
        "        try:\n",
        "            count_query = (\n",
        "                f\"SELECT COUNT(*) as count FROM `{bucket_name}`.`{scope_name}`.`{collection_name}`\"\n",
        "            )\n",
        "            count_result = cluster.query(count_query)\n",
        "            count_row = next(iter(count_result))\n",
        "            existing_count = count_row[\"count\"]\n",
        "\n",
        "            if existing_count > 0:\n",
        "                logger.info(\n",
        "                    f\"Found {existing_count} existing documents in collection, skipping data load\"\n",
        "                )\n",
        "                return\n",
        "\n",
        "            review_texts = self.process_to_texts()\n",
        "\n",
        "            vector_store = CouchbaseSearchVectorStore(\n",
        "                cluster=cluster,\n",
        "                bucket_name=bucket_name,\n",
        "                scope_name=scope_name,\n",
        "                collection_name=collection_name,\n",
        "                embedding=embeddings,\n",
        "                index_name=index_name,\n",
        "            )\n",
        "\n",
        "            logger.info(\n",
        "                f\"Loading {len(review_texts)} airline review embeddings to {bucket_name}.{scope_name}.{collection_name}\"\n",
        "            )\n",
        "\n",
        "            batch_size = 10  # Conservative batch size for stability\n",
        "            total_batches = (len(review_texts) + batch_size - 1) // batch_size\n",
        "\n",
        "            with tqdm(\n",
        "                total=len(review_texts), desc=\"Loading airline reviews\", unit=\"reviews\"\n",
        "            ) as pbar:\n",
        "                for i in range(0, len(review_texts), batch_size):\n",
        "                    batch_num = i // batch_size + 1\n",
        "                    batch = review_texts[i : i + batch_size]\n",
        "\n",
        "                    vector_store.add_texts(texts=batch, batch_size=len(batch))\n",
        "\n",
        "                    pbar.update(len(batch))\n",
        "                    pbar.set_postfix(batch=f\"{batch_num}/{total_batches}\")\n",
        "\n",
        "            logger.info(\n",
        "                f\"Successfully loaded {len(review_texts)} airline review embeddings to vector store\"\n",
        "            )\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.exception(f\"Error loading airline reviews to Couchbase: {e!s}\")\n",
        "            raise\n",
        "\n",
        "\n",
        "# Global instance for reuse\n",
        "_data_manager = AirlineReviewsDataManager()\n",
        "\n",
        "\n",
        "def get_airline_review_texts():\n",
        "    \"\"\"Get processed airline review texts (uses global cached instance).\"\"\"\n",
        "    return _data_manager.process_to_texts()\n",
        "\n",
        "\n",
        "def load_airline_reviews_from_kaggle():\n",
        "    \"\"\"Load raw airline reviews data from Kaggle (uses global cached instance).\"\"\"\n",
        "    return _data_manager.load_raw_data()\n",
        "\n",
        "\n",
        "def load_airline_reviews_to_couchbase(\n",
        "    cluster, bucket_name: str, scope_name: str, collection_name: str, embeddings, index_name: str\n",
        "):\n",
        "    \"\"\"Load airline reviews into Couchbase vector store (uses global cached instance).\"\"\"\n",
        "    return _data_manager.load_to_vector_store(\n",
        "        cluster, bucket_name, scope_name, collection_name, embeddings, index_name\n",
        "    )\n",
        "\n",
        "\n",
        "def load_airline_reviews():\n",
        "    \"\"\"Simple function to load airline reviews - called by main.py.\"\"\"\n",
        "    try:\n",
        "        # Just return the processed texts for embedding\n",
        "        # This eliminates the need for separate cluster connection here\n",
        "        logger.info(\"Loading airline reviews data...\")\n",
        "        reviews = _data_manager.process_to_texts()\n",
        "        logger.info(f\"Successfully loaded {len(reviews)} airline reviews\")\n",
        "        return reviews\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.exception(f\"Error in load_airline_reviews: {e!s}\")\n",
        "        raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70ajWVAemTCv",
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Agent Classes and Core Implementation\n",
        "\n",
        "Core agent classes and implementation from working main.py script.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P2tjFmkimTCv"
      },
      "outputs": [],
      "source": [
        "# Agent classes are implemented using the inline AI services and Couchbase client above\n",
        "print(\"All setup functions and client implementations are now available inline.\")\n",
        "def setup_ai_services(\n",
        "    framework: str = \"langchain\",\n",
        "    temperature: float = 0.0,\n",
        "    callbacks: Optional[List] = None,\n",
        "    application_span: Optional[Any] = None\n",
        ") -> Tuple[Any, Any]:\n",
        "    \"\"\"Priority 1 AI service setup - Capella with direct API keys + OpenAI wrappers only.\"\"\"\n",
        "    embeddings = None\n",
        "    llm = None\n",
        "\n",
        "    logger.info(f\"🔧 Setting up Priority 1 AI services for {framework} framework...\")\n",
        "\n",
        "    # PRIORITY 1: LATEST CAPELLA (OpenAI wrappers with direct API keys)\n",
        "    if (\n",
        "        not embeddings\n",
        "        and os.getenv(\"CAPELLA_API_ENDPOINT\")\n",
        "        and os.getenv(\"CAPELLA_API_EMBEDDINGS_KEY\")\n",
        "    ):\n",
        "        try:\n",
        "            from langchain_openai import OpenAIEmbeddings\n",
        "            endpoint = os.getenv(\"CAPELLA_API_ENDPOINT\")\n",
        "            api_key = os.getenv(\"CAPELLA_API_EMBEDDINGS_KEY\")\n",
        "            model = os.getenv(\"CAPELLA_API_EMBEDDING_MODEL\")\n",
        "\n",
        "            # Handle endpoint that may or may not already have /v1 suffix\n",
        "            if endpoint.endswith('/v1'):\n",
        "                base_url = endpoint\n",
        "            else:\n",
        "                base_url = f\"{endpoint}/v1\"\n",
        "\n",
        "            # Debug logging - same pattern as working test\n",
        "            logger.info(f\"🔧 Endpoint: {endpoint}\")\n",
        "            logger.info(f\"🔧 Model: {model}\")\n",
        "            logger.info(f\"🔧 Base URL: {base_url}\")\n",
        "\n",
        "            embeddings = OpenAIEmbeddings(\n",
        "                model=model,\n",
        "                api_key=api_key,\n",
        "                base_url=base_url,\n",
        "                check_embedding_ctx_length=False,  # KEY FIX for asymmetric models in LangChain/LangGraph\n",
        "            )\n",
        "            logger.info(\"✅ Using Priority 1: Capella AI embeddings (OpenAI wrapper)\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"❌ Priority 1 Capella AI embeddings failed: {type(e).__name__}: {e}\")\n",
        "\n",
        "    if not llm and os.getenv(\"CAPELLA_API_ENDPOINT\") and os.getenv(\"CAPELLA_API_LLM_KEY\"):\n",
        "        try:\n",
        "            from langchain_openai import ChatOpenAI\n",
        "\n",
        "            endpoint = os.getenv(\"CAPELLA_API_ENDPOINT\")\n",
        "            llm_key = os.getenv(\"CAPELLA_API_LLM_KEY\")\n",
        "            llm_model = os.getenv(\"CAPELLA_API_LLM_MODEL\")\n",
        "\n",
        "            # Handle endpoint that may or may not already have /v1 suffix\n",
        "            if endpoint.endswith('/v1'):\n",
        "                base_url = endpoint\n",
        "            else:\n",
        "                base_url = f\"{endpoint}/v1\"\n",
        "\n",
        "            # Debug logging\n",
        "            logger.info(f\"🔧 LLM Endpoint: {endpoint}\")\n",
        "            logger.info(f\"🔧 LLM Model: {llm_model}\")\n",
        "            logger.info(f\"🔧 LLM Base URL: {base_url}\")\n",
        "\n",
        "            llm = ChatOpenAI(\n",
        "                api_key=llm_key,\n",
        "                base_url=base_url,\n",
        "                model=llm_model,\n",
        "                temperature=temperature,\n",
        "            )\n",
        "            # Test the LLM works\n",
        "            from langchain_core.messages import HumanMessage\n",
        "            test_response = llm.invoke([HumanMessage(content=\"Hello\")])\n",
        "            logger.info(\"✅ Using Priority 1: Capella AI LLM (OpenAI wrapper)\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"❌ Priority 1 Capella AI LLM failed: {type(e).__name__}: {e}\")\n",
        "            llm = None\n",
        "\n",
        "    # Fallback: OpenAI\n",
        "    if not embeddings and os.getenv(\"OPENAI_API_KEY\"):\n",
        "        try:\n",
        "            from langchain_openai import OpenAIEmbeddings\n",
        "            embeddings = OpenAIEmbeddings(\n",
        "                model=\"text-embedding-3-small\",\n",
        "                api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
        "            )\n",
        "            logger.info(\"✅ Using OpenAI embeddings fallback\")\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"⚠️ OpenAI embeddings failed: {e}\")\n",
        "\n",
        "    if not llm and os.getenv(\"OPENAI_API_KEY\"):\n",
        "        try:\n",
        "            from langchain_openai import ChatOpenAI\n",
        "            llm = ChatOpenAI(\n",
        "                api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
        "                model=\"gpt-4o\",\n",
        "                temperature=temperature,\n",
        "            )\n",
        "            logger.info(\"✅ Using OpenAI LLM fallback\")\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"⚠️ OpenAI LLM failed: {e}\")\n",
        "\n",
        "    if not embeddings:\n",
        "        raise ValueError(\"❌ No embeddings service could be initialized\")\n",
        "    if not llm:\n",
        "        raise ValueError(\"❌ No LLM service could be initialized\")\n",
        "\n",
        "    logger.info(f\"✅ Priority 1 AI services setup completed for {framework}\")\n",
        "    return embeddings, llm\n",
        "\n",
        "\n",
        "# Setup environment\n",
        "setup_environment()\n",
        "\n",
        "# Test Capella AI connectivity if configured\n",
        "if os.getenv(\"CAPELLA_API_ENDPOINT\"):\n",
        "    if not test_capella_connectivity():\n",
        "        logger.warning(\"❌ Capella AI connectivity test failed. Will use fallback models.\")\n",
        "else:\n",
        "    logger.info(\"ℹ️ Capella API not configured - will use fallback models\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBnkb9fUmTCv",
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## CouchbaseClient Class\n",
        "\n",
        "Define the CouchbaseClient for all database operations inline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ZpmJfgVmTCv"
      },
      "outputs": [],
      "source": [
        "class CouchbaseClient:\n",
        "    \"\"\"Centralized Couchbase client for all database operations.\"\"\"\n",
        "\n",
        "    def __init__(self, conn_string: str, username: str, password: str, bucket_name: str):\n",
        "        self.conn_string = conn_string\n",
        "        self.username = username\n",
        "        self.password = password\n",
        "        self.bucket_name = bucket_name\n",
        "        self.cluster = None\n",
        "        self.bucket = None\n",
        "        self._collections = {}\n",
        "\n",
        "    def connect(self):\n",
        "        try:\n",
        "            auth = PasswordAuthenticator(self.username, self.password)\n",
        "            options = ClusterOptions(auth)\n",
        "            options.apply_profile(\"wan_development\")\n",
        "            self.cluster = Cluster(self.conn_string, options)\n",
        "            self.cluster.wait_until_ready(timedelta(seconds=15))\n",
        "            logger.info(\"✅ Successfully connected to Couchbase\")\n",
        "            return self.cluster\n",
        "        except Exception as e:\n",
        "            raise ConnectionError(f\"❌ Failed to connect to Couchbase: {e!s}\")\n",
        "\n",
        "    def setup_collection(self, scope_name: str, collection_name: str, clear_existing_data: bool = False):\n",
        "        try:\n",
        "            if not self.cluster:\n",
        "                self.connect()\n",
        "\n",
        "            if not self.bucket:\n",
        "                try:\n",
        "                    self.bucket = self.cluster.bucket(self.bucket_name)\n",
        "                except Exception:\n",
        "                    logger.info(f\"Creating bucket '{self.bucket_name}'...\")\n",
        "                    bucket_settings = CreateBucketSettings(\n",
        "                        name=self.bucket_name, bucket_type=BucketType.COUCHBASE,\n",
        "                        ram_quota_mb=1024, flush_enabled=True, num_replicas=0\n",
        "                    )\n",
        "                    self.cluster.buckets().create_bucket(bucket_settings)\n",
        "                    time.sleep(5)\n",
        "                    self.bucket = self.cluster.bucket(self.bucket_name)\n",
        "\n",
        "            bucket_manager = self.bucket.collections()\n",
        "            scopes = bucket_manager.get_all_scopes()\n",
        "            scope_exists = any(scope.name == scope_name for scope in scopes)\n",
        "\n",
        "            if not scope_exists and scope_name != \"_default\":\n",
        "                bucket_manager.create_scope(scope_name)\n",
        "\n",
        "            collections = bucket_manager.get_all_scopes()\n",
        "            collection_exists = any(\n",
        "                scope.name == scope_name and collection_name in [col.name for col in scope.collections]\n",
        "                for scope in collections\n",
        "            )\n",
        "\n",
        "            if collection_exists and clear_existing_data:\n",
        "                self.clear_collection_data(scope_name, collection_name)\n",
        "            elif not collection_exists:\n",
        "                bucket_manager.create_collection(scope_name, collection_name)\n",
        "\n",
        "            time.sleep(3)\n",
        "            try:\n",
        "                self.cluster.query(\n",
        "                    f\"CREATE PRIMARY INDEX IF NOT EXISTS ON `{self.bucket_name}`.`{scope_name}`.`{collection_name}`\"\n",
        "                ).execute()\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "            collection = self.bucket.scope(scope_name).collection(collection_name)\n",
        "            self._collections[f\"{scope_name}.{collection_name}\"] = collection\n",
        "            logger.info(f\"✅ Collection setup complete: {scope_name}.{collection_name}\")\n",
        "            return collection\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"❌ Error setting up collection: {e!s}\")\n",
        "\n",
        "    def clear_collection_data(self, scope_name: str, collection_name: str):\n",
        "        try:\n",
        "            logger.info(f\"🗑️ Clearing data from {self.bucket_name}.{scope_name}.{collection_name}...\")\n",
        "            delete_query = f\"DELETE FROM `{self.bucket_name}`.`{scope_name}`.`{collection_name}`\"\n",
        "            result = self.cluster.query(delete_query)\n",
        "            list(result)\n",
        "            time.sleep(2)\n",
        "        except KeyspaceNotFoundException:\n",
        "            logger.info(f\"ℹ️ Collection {self.bucket_name}.{scope_name}.{collection_name} doesn't exist, nothing to clear\")\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"⚠️ Error clearing collection data: {e}\")\n",
        "\n",
        "    def clear_scope(self, scope_name: str):\n",
        "        try:\n",
        "            if not self.bucket:\n",
        "                if not self.cluster:\n",
        "                    self.connect()\n",
        "                self.bucket = self.cluster.bucket(self.bucket_name)\n",
        "\n",
        "            bucket_manager = self.bucket.collections()\n",
        "            scopes = bucket_manager.get_all_scopes()\n",
        "            target_scope = next((s for s in scopes if s.name == scope_name), None)\n",
        "\n",
        "            if target_scope:\n",
        "                for collection in target_scope.collections:\n",
        "                    try:\n",
        "                        self.clear_collection_data(scope_name, collection.name)\n",
        "                    except Exception:\n",
        "                        pass\n",
        "                logger.info(f\"✅ Completed clearing scope: {self.bucket_name}.{scope_name}\")\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"❌ Could not clear scope: {e}\")\n",
        "\n",
        "    def setup_vector_search_index(self, index_definition: dict, scope_name: str):\n",
        "        try:\n",
        "            scope_index_manager = self.bucket.scope(scope_name).search_indexes()\n",
        "            existing_indexes = scope_index_manager.get_all_indexes()\n",
        "            index_name = index_definition[\"name\"]\n",
        "\n",
        "            if index_name not in [index.name for index in existing_indexes]:\n",
        "                search_index = SearchIndex.from_json(index_definition)\n",
        "                scope_index_manager.upsert_index(search_index)\n",
        "                logger.info(f\"✅ Vector search index '{index_name}' created\")\n",
        "            else:\n",
        "                logger.info(f\"ℹ️ Vector search index '{index_name}' already exists\")\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"⚠️ Error setting up vector search index: {e}\")\n",
        "\n",
        "    def setup_vector_store_langchain(self, scope_name: str, collection_name: str, index_name: str, embeddings, data_loader_func=None, **loader_kwargs):\n",
        "        try:\n",
        "            if data_loader_func:\n",
        "                logger.info(\"🔄 Loading data into vector store...\")\n",
        "                data_loader_func(\n",
        "                    cluster=self.cluster, bucket_name=self.bucket_name,\n",
        "                    scope_name=scope_name, collection_name=collection_name,\n",
        "                    embeddings=embeddings, index_name=index_name, **loader_kwargs\n",
        "                )\n",
        "\n",
        "            vector_store = CouchbaseSearchVectorStore(\n",
        "                cluster=self.cluster, bucket_name=self.bucket_name,\n",
        "                scope_name=scope_name, collection_name=collection_name,\n",
        "                embedding=embeddings, index_name=index_name\n",
        "            )\n",
        "            logger.info(f\"✅ Vector store setup complete: {self.bucket_name}.{scope_name}.{collection_name}\")\n",
        "            return vector_store\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"❌ Error setting up vector store: {e!s}\")\n",
        "\n",
        "\n",
        "def create_couchbase_client():\n",
        "    \"\"\"Factory function to create CouchbaseClient with environment defaults.\"\"\"\n",
        "    return CouchbaseClient(\n",
        "        conn_string=os.getenv(\"CB_CONN_STRING\", \"couchbase://localhost\"),\n",
        "        username=os.getenv(\"CB_USERNAME\", \"Administrator\"),\n",
        "        password=os.getenv(\"CB_PASSWORD\", \"password\"),\n",
        "        bucket_name=os.getenv(\"CB_BUCKET\", \"travel-sample\"),\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPIVbrffmTCw"
      },
      "source": [
        "## Flight Search Agent Classes\n",
        "\n",
        "This cell contains the classes for the flight search agent.\n",
        "\n",
        "### FlightSearchGraph\n",
        "\n",
        "The `FlightSearchGraph` class is the main class for the flight search agent. It is a subclass of `langgraph.graph.StateGraph` and is used to define the graph of the flight search agent.\n",
        "\n",
        "### FlightSearchGraphState\n",
        "\n",
        "The `FlightSearchGraphState` class is the state of the flight search agent. It is a subclass of `langgraph.graph.State` and is used to define the state of the flight search agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gH84eu2ImTCw"
      },
      "outputs": [],
      "source": [
        "\n",
        "## Agent Classes\n",
        "\n",
        "class FlightSearchState(agentc_langgraph.agent.State):\n",
        "    \"\"\"State for flight search conversations - single user system.\"\"\"\n",
        "\n",
        "    query: str\n",
        "    resolved: bool\n",
        "    search_results: list[dict]\n",
        "\n",
        "\n",
        "class FlightSearchAgent(agentc_langgraph.agent.ReActAgent):\n",
        "    \"\"\"Flight search agent using Agent Catalog tools and ReActAgent framework.\"\"\"\n",
        "\n",
        "    def __init__(self, catalog: agentc.Catalog, span: agentc.Span, chat_model=None):\n",
        "        \"\"\"Initialize the flight search agent.\"\"\"\n",
        "\n",
        "        if chat_model is None:\n",
        "            # Fallback to OpenAI if no chat model provided\n",
        "            model_name = os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\")\n",
        "            chat_model = langchain_openai.chat_models.ChatOpenAI(model=model_name, temperature=0.1)\n",
        "\n",
        "        super().__init__(\n",
        "            chat_model=chat_model, catalog=catalog, span=span, prompt_name=\"flight_search_assistant\"\n",
        "        )\n",
        "\n",
        "    def _invoke(\n",
        "        self,\n",
        "        span: agentc.Span,\n",
        "        state: FlightSearchState,\n",
        "        config: langchain_core.runnables.RunnableConfig,\n",
        "    ) -> FlightSearchState:\n",
        "        \"\"\"Handle flight search conversation using ReActAgent.\"\"\"\n",
        "\n",
        "        # Initialize conversation if this is the first message\n",
        "        if not state[\"messages\"]:\n",
        "            initial_msg = langchain_core.messages.HumanMessage(content=state[\"query\"])\n",
        "            state[\"messages\"].append(initial_msg)\n",
        "            logger.info(f\"Flight Query: {state['query']}\")\n",
        "\n",
        "        # Get prompt resource first - we'll need it for the ReAct agent\n",
        "        prompt_resource = self.catalog.find(\"prompt\", name=\"flight_search_assistant\")\n",
        "\n",
        "        # Get tools from Agent Catalog with simplified discovery\n",
        "        tools = []\n",
        "        tool_names = [\n",
        "            \"lookup_flight_info\",\n",
        "            \"save_flight_booking\",\n",
        "            \"retrieve_flight_bookings\",\n",
        "            \"search_airline_reviews\",\n",
        "        ]\n",
        "\n",
        "        for tool_name in tool_names:\n",
        "            try:\n",
        "                # Find tool using Agent Catalog\n",
        "                catalog_tool = self.catalog.find(\"tool\", name=tool_name)\n",
        "                if catalog_tool:\n",
        "                    logger.info(f\"✅ Found tool: {tool_name}\")\n",
        "                else:\n",
        "                    logger.error(f\"❌ Tool not found: {tool_name}\")\n",
        "                    continue\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"❌ Failed to find tool {tool_name}: {e}\")\n",
        "                continue\n",
        "\n",
        "            # JSON-first architecture with Pydantic validation\n",
        "            # Import validation schemas\n",
        "            import json\n",
        "            from pydantic import ValidationError\n",
        "            from tools.schemas import FlightSearchInput, BookingInput\n",
        "\n",
        "            def clean_react_artifacts(raw_input: str) -> str:\n",
        "                \"\"\"Remove ReAct format artifacts that contaminate tool inputs.\"\"\"\n",
        "                if not raw_input:\n",
        "                    return \"\"\n",
        "\n",
        "                cleaned = raw_input.strip()\n",
        "\n",
        "                # Remove ReAct artifacts - order matters, check longer patterns first\n",
        "                artifacts_to_remove = [\n",
        "                    '\\nObservation:', '\\nObservation', 'Observation:', 'Observation',\n",
        "                    '\\nThought:', 'Thought:',\n",
        "                    '\\nAction:', 'Action:',\n",
        "                    '\\nAction Input:', 'Action Input:',\n",
        "                    '\\nFinal Answer:', 'Final Answer:',\n",
        "                    'Observ'  # Handle incomplete artifact\n",
        "                ]\n",
        "\n",
        "                for artifact in artifacts_to_remove:\n",
        "                    if artifact in cleaned:\n",
        "                        # Split and take only the part before the artifact\n",
        "                        cleaned = cleaned.split(artifact)[0].strip()\n",
        "\n",
        "                # Clean up quotes and extra whitespace\n",
        "                cleaned = cleaned.strip().strip(\"\\\"'\").strip()\n",
        "\n",
        "                # Normalize whitespace\n",
        "                cleaned = \" \".join(cleaned.split())\n",
        "\n",
        "                return cleaned\n",
        "\n",
        "            def parse_tool_input(tool_name: str, tool_input: str) -> dict:\n",
        "                \"\"\"Parse tool input: JSON with Pydantic validation for structured tools, plain text for simple tools.\"\"\"\n",
        "                # Clean ReAct artifacts first\n",
        "                cleaned = clean_react_artifacts(tool_input)\n",
        "\n",
        "                # Structured tools MUST use JSON\n",
        "                if tool_name == \"lookup_flight_info\":\n",
        "                    data = json.loads(cleaned)  # Will raise JSONDecodeError if invalid\n",
        "                    validated = FlightSearchInput(**data)  # Will raise ValidationError if invalid\n",
        "                    logger.info(f\"✅ Parsed {tool_name}: {validated.source_airport} → {validated.destination_airport}\")\n",
        "                    return {\n",
        "                        \"source_airport\": validated.source_airport,\n",
        "                        \"destination_airport\": validated.destination_airport\n",
        "                    }\n",
        "\n",
        "                elif tool_name == \"save_flight_booking\":\n",
        "                    data = json.loads(cleaned)  # Will raise JSONDecodeError if invalid\n",
        "                    validated = BookingInput(**data)  # Will raise ValidationError if invalid\n",
        "                    logger.info(f\"✅ Parsed {tool_name}: {validated.source_airport}→{validated.destination_airport}, {validated.passengers} pax, {validated.flight_class}\")\n",
        "                    return {\n",
        "                        \"source_airport\": validated.source_airport,\n",
        "                        \"destination_airport\": validated.destination_airport,\n",
        "                        \"departure_date\": validated.departure_date,\n",
        "                        \"passengers\": validated.passengers,\n",
        "                        \"flight_class\": validated.flight_class\n",
        "                    }\n",
        "\n",
        "                # Simple tools use plain text\n",
        "                elif tool_name == \"retrieve_flight_bookings\":\n",
        "                    return {\"booking_query\": cleaned}\n",
        "\n",
        "                elif tool_name == \"search_airline_reviews\":\n",
        "                    return {\"query\": cleaned}\n",
        "\n",
        "                raise ValueError(f\"Unknown tool: {tool_name}\")\n",
        "\n",
        "            # Create clean wrapper function for this tool\n",
        "            def create_tool_func(catalog_tool_ref, tool_name_ref):\n",
        "                \"\"\"Create a wrapper that parses JSON and calls catalog tool with structured params.\"\"\"\n",
        "                def tool_func(tool_input: str) -> str:\n",
        "                    try:\n",
        "                        # Parse input with Pydantic validation (JSON for structured tools)\n",
        "                        params = parse_tool_input(tool_name_ref, tool_input)\n",
        "\n",
        "                        # Call the Agent Catalog tool with parsed parameters\n",
        "                        result = catalog_tool_ref.func(**params)\n",
        "\n",
        "                        return str(result) if result is not None else \"No results found\"\n",
        "\n",
        "                    except Exception as e:\n",
        "                        logger.error(f\"❌ Error in tool {tool_name_ref}: {e}\")\n",
        "                        import traceback\n",
        "                        logger.debug(traceback.format_exc())\n",
        "                        return f\"Error: {str(e)}\"\n",
        "                return tool_func\n",
        "\n",
        "            # Tool descriptions for the LLM (JSON required for structured tools)\n",
        "            tool_descriptions = {\n",
        "                \"lookup_flight_info\": \"Find flights between airports. REQUIRES JSON: {\\\"source_airport\\\": \\\"JFK\\\", \\\"destination_airport\\\": \\\"LAX\\\"}\",\n",
        "                \"save_flight_booking\": \"Book a flight. REQUIRES JSON: {\\\"source_airport\\\": \\\"LAX\\\", \\\"destination_airport\\\": \\\"JFK\\\", \\\"departure_date\\\": \\\"2025-12-25\\\", \\\"passengers\\\": 2, \\\"flight_class\\\": \\\"business\\\"}\",\n",
        "                \"retrieve_flight_bookings\": \"View all flight bookings or search by criteria. Leave input empty for all bookings.\",\n",
        "                \"search_airline_reviews\": \"Search airline customer reviews. Input: plain text query (e.g., 'SpiceJet service quality')\"\n",
        "            }\n",
        "\n",
        "            langchain_tool = Tool(\n",
        "                name=tool_name,\n",
        "                description=tool_descriptions.get(tool_name, f\"Tool for {tool_name.replace('_', ' ')}\"),\n",
        "                func=create_tool_func(catalog_tool, tool_name),\n",
        "            )\n",
        "            tools.append(langchain_tool)\n",
        "\n",
        "        # Use the Agent Catalog prompt content directly - get first result if it's a list\n",
        "        if isinstance(prompt_resource, list):\n",
        "            prompt_resource = prompt_resource[0]\n",
        "\n",
        "        # Safely get the content from the prompt resource\n",
        "        prompt_content = getattr(prompt_resource, \"content\", \"\")\n",
        "        if not prompt_content:\n",
        "            prompt_content = \"You are a helpful flight search assistant. Use the available tools to help users with their flight queries.\"\n",
        "\n",
        "        # Inject current date into the prompt content\n",
        "        import datetime\n",
        "\n",
        "        current_date = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
        "        prompt_content = prompt_content.replace(\"{current_date}\", current_date)\n",
        "\n",
        "        # Use the Agent Catalog prompt content directly - it already has ReAct format\n",
        "        react_prompt = PromptTemplate.from_template(str(prompt_content))\n",
        "\n",
        "        # Create ReAct agent with tools and prompt\n",
        "        agent = create_react_agent(self.chat_model, tools, react_prompt)\n",
        "\n",
        "        # Custom parsing error handler - force stopping on parsing errors\n",
        "        def handle_parsing_errors(error):\n",
        "            \"\"\"Custom handler for parsing errors - force early termination.\"\"\"\n",
        "            error_msg = str(error)\n",
        "            if \"both a final answer and a parse-able action\" in error_msg:\n",
        "                # Force early termination - return a reasonable response\n",
        "                return \"Final Answer: I encountered a parsing error. Please reformulate your request.\"\n",
        "            elif \"Missing 'Action:'\" in error_msg:\n",
        "                return \"I need to use the correct format with Action: and Action Input:\"\n",
        "            else:\n",
        "                return f\"Final Answer: I encountered an error processing your request. Please try again.\"\n",
        "\n",
        "        # Create agent executor - very strict: only 2 iterations max\n",
        "        agent_executor = AgentExecutor(\n",
        "            agent=agent,\n",
        "            tools=tools,\n",
        "            verbose=True,\n",
        "            handle_parsing_errors=handle_parsing_errors,\n",
        "            max_iterations=2,  # STRICT: 1 tool call + 1 Final Answer only\n",
        "            early_stopping_method=\"force\",  # Force stop\n",
        "            return_intermediate_steps=True,\n",
        "        )\n",
        "\n",
        "        # Execute the agent\n",
        "        response = agent_executor.invoke({\"input\": state[\"query\"]})\n",
        "\n",
        "        # Extract tool outputs from intermediate_steps and store in search_results\n",
        "        if \"intermediate_steps\" in response and response[\"intermediate_steps\"]:\n",
        "            tool_outputs = []\n",
        "            for step in response[\"intermediate_steps\"]:\n",
        "                if isinstance(step, tuple) and len(step) >= 2:\n",
        "                    # step[0] is the action, step[1] is the tool output/observation\n",
        "                    tool_output = str(step[1])\n",
        "                    if tool_output and tool_output.strip():\n",
        "                        tool_outputs.append(tool_output)\n",
        "            state[\"search_results\"] = tool_outputs\n",
        "\n",
        "        # Add response to conversation\n",
        "        assistant_msg = langchain_core.messages.AIMessage(content=response[\"output\"])\n",
        "        state[\"messages\"].append(assistant_msg)\n",
        "        state[\"resolved\"] = True\n",
        "\n",
        "        return state\n",
        "\n",
        "\n",
        "class FlightSearchGraph(agentc_langgraph.graph.GraphRunnable):\n",
        "    \"\"\"Flight search conversation graph using Agent Catalog.\"\"\"\n",
        "\n",
        "    def __init__(self, catalog, span, chat_model=None):\n",
        "        \"\"\"Initialize the flight search graph with optional chat model.\"\"\"\n",
        "        super().__init__(catalog=catalog, span=span)\n",
        "        self.chat_model = chat_model\n",
        "\n",
        "    @staticmethod\n",
        "    def build_starting_state(query: str) -> FlightSearchState:\n",
        "        \"\"\"Build the initial state for the flight search - single user system.\"\"\"\n",
        "        return FlightSearchState(\n",
        "            messages=[],\n",
        "            query=query,\n",
        "            resolved=False,\n",
        "            search_results=[],\n",
        "        )\n",
        "\n",
        "    def compile(self):\n",
        "        \"\"\"Compile the LangGraph workflow.\"\"\"\n",
        "\n",
        "        # Build the flight search agent with catalog integration\n",
        "        search_agent = FlightSearchAgent(\n",
        "            catalog=self.catalog, span=self.span, chat_model=self.chat_model\n",
        "        )\n",
        "\n",
        "        # Create a wrapper function for the ReActAgent\n",
        "        def flight_search_node(state: FlightSearchState) -> FlightSearchState:\n",
        "            \"\"\"Wrapper function for the flight search ReActAgent.\"\"\"\n",
        "            return search_agent._invoke(\n",
        "                span=self.span,\n",
        "                state=state,\n",
        "                config={},  # Empty config for now\n",
        "            )\n",
        "\n",
        "        # Create a simple workflow graph for flight search\n",
        "        workflow = langgraph.graph.StateGraph(FlightSearchState)\n",
        "\n",
        "        # Add the flight search agent node using the wrapper function\n",
        "        workflow.add_node(\"flight_search\", flight_search_node)\n",
        "\n",
        "        # Set entry point and simple flow\n",
        "        workflow.set_entry_point(\"flight_search\")\n",
        "        workflow.add_edge(\"flight_search\", langgraph.graph.END)\n",
        "\n",
        "        return workflow.compile()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaohuackmTCw",
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Data Loading Components\n",
        "\n",
        "Complete data loading and query definition components embedded inline for standalone operation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kVwXayEymTCw"
      },
      "outputs": [],
      "source": [
        "# Flight Search Queries and Reference Answers - Complete implementation from data/queries.py\n",
        "\n",
        "# Flight search queries (for evaluation and testing)\n",
        "FLIGHT_SEARCH_QUERIES = [\n",
        "    \"Find flights from JFK to LAX\",\n",
        "    \"Book a flight from LAX to JFK for tomorrow, 2 passengers, business class\",\n",
        "    \"Book an economy flight from JFK to MIA for next week, 1 passenger\",\n",
        "    \"Show me my current flight bookings\",\n",
        "    \"What do passengers say about SpiceJet's service quality?\",\n",
        "]\n",
        "\n",
        "# Comprehensive reference answers based on actual system responses\n",
        "FLIGHT_REFERENCE_ANSWERS = [\n",
        "    # Query 1: Flight search JFK to LAX\n",
        "    \"\"\"Available flights from JFK to LAX:\n",
        "\n",
        "1. AS flight from JFK to LAX using 321 762\n",
        "2. B6 flight from JFK to LAX using 320\n",
        "3. DL flight from JFK to LAX using 76W 752\n",
        "4. QF flight from JFK to LAX using 744\n",
        "5. AA flight from JFK to LAX using 32B 762\n",
        "6. UA flight from JFK to LAX using 757\n",
        "7. US flight from JFK to LAX using 32B 762\n",
        "8. VX flight from JFK to LAX using 320\"\"\",\n",
        "\n",
        "    # Query 2: Flight booking LAX to JFK for tomorrow, 2 passengers, business class\n",
        "    \"\"\"Flight Booking Confirmed!\n",
        "\n",
        "Booking ID: FL08061563CACD\n",
        "Route: LAX → JFK\n",
        "Departure Date: 2025-08-06\n",
        "Passengers: 2\n",
        "Class: business\n",
        "Total Price: $1500.00\n",
        "\n",
        "Next Steps:\n",
        "1. Check-in opens 24 hours before departure\n",
        "2. Arrive at airport 2 hours early for domestic flights\n",
        "3. Bring valid government-issued photo ID\n",
        "\n",
        "Thank you for choosing our airline!\"\"\",\n",
        "\n",
        "    # Query 3: Flight booking JFK to MIA for next week\n",
        "    \"\"\"Flight Booking Confirmed!\n",
        "\n",
        "Booking ID: FL08124E7B9C2A\n",
        "Route: JFK → MIA\n",
        "Departure Date: 2025-08-12\n",
        "Passengers: 1\n",
        "Class: economy\n",
        "Total Price: $250.00\n",
        "\n",
        "Next Steps:\n",
        "1. Check-in opens 24 hours before departure\n",
        "2. Arrive at airport 2 hours early for domestic flights\n",
        "3. Bring valid government-issued photo ID\n",
        "\n",
        "Thank you for choosing our airline!\"\"\",\n",
        "\n",
        "    # Query 4: Show current flight bookings\n",
        "    \"\"\"Your Current Bookings (2 found):\n",
        "\n",
        "Booking 1:\n",
        "  Booking ID: FL08061563CACD\n",
        "  Route: LAX → JFK\n",
        "  Date: 2025-08-06\n",
        "  Passengers: 2\n",
        "  Class: business\n",
        "  Total: $1500.00\n",
        "  Status: confirmed\n",
        "  Booked: 2025-08-05\n",
        "\n",
        "Booking 2:\n",
        "  Booking ID: FL08124E7B9C2A\n",
        "  Route: JFK → MIA\n",
        "  Date: 2025-08-12\n",
        "  Passengers: 1\n",
        "  Class: economy\n",
        "  Total: $250.00\n",
        "  Status: confirmed\n",
        "  Booked: 2025-08-05\"\"\",\n",
        "\n",
        "    # Query 5: SpiceJet service quality reviews\n",
        "    \"\"\"Found 5 relevant airline reviews for 'SpiceJet service':\n",
        "\n",
        "Review 1:\n",
        "Airline: SpiceJet. Title: \"Service is impeccable\". Review: ✅ Trip Verified | Much better than airbus models. Even the basic economy class has ambient lighting. Better personal air vents and better spotlights. Even overhead storage bins are good. Service is impeccable with proper care taken of guests...\n",
        "\n",
        "Review 2:\n",
        "Airline: SpiceJet. Title: \"good service by the crew\". Review: ✅ Trip Verified | I have had good service by the crew. It was amazing, the crew was very enthusiastic and warm welcome. It was one of the best services in my experience.. Rating: 10.0/10. Reviewer: K Mansour. Date: 10th August 2024. Recom...\n",
        "\n",
        "Review 3:\n",
        "Airline: SpiceJet. Title: \"outstanding service I experienced\". Review: Not Verified |  I wanted to take a moment to express my sincere thanks for the outstanding service I experienced on my recent flight from Pune to Delhi. SG-8937. From the moment I boarded, the warmth and friendliness of the air h...\n",
        "\n",
        "Review 4:\n",
        "Airline: SpiceJet. Title: \"efficient and warm onboard service\". Review: ✅ Trip Verified |  New Delhi to Kolkata. Delighted with the prompt, efficient and warm onboard service provided by the crew. Appreciate their efforts towards customer centricity.. Rating: 10.0/10. Reviewer: Debashis Roy. Date: 2...\n",
        "\n",
        "Review 5:\n",
        "Airline: SpiceJet. Title: \"Service is very good\". Review: Service is very good,  I am impressed with Miss Renu  who gave the best services ever. Thanks to Renu who is very sweet by her nature as well as her service. Rating: 9.0/10. Reviewer: Sanjay Patnaik. Date: 21st September 2023. Recommended: ye...\"\"\",\n",
        "]\n",
        "\n",
        "# Create dictionary for backward compatibility\n",
        "QUERY_REFERENCE_ANSWERS = {\n",
        "    query: answer for query, answer in zip(FLIGHT_SEARCH_QUERIES, FLIGHT_REFERENCE_ANSWERS)\n",
        "}\n",
        "\n",
        "def get_test_queries():\n",
        "    \"\"\"Return test queries for evaluation.\"\"\"\n",
        "    return FLIGHT_SEARCH_QUERIES\n",
        "\n",
        "def get_evaluation_queries():\n",
        "    \"\"\"Get queries for evaluation\"\"\"\n",
        "    return FLIGHT_SEARCH_QUERIES\n",
        "\n",
        "def get_all_queries():\n",
        "    \"\"\"Get all available queries\"\"\"\n",
        "    return FLIGHT_SEARCH_QUERIES\n",
        "\n",
        "def get_simple_queries():\n",
        "    \"\"\"Get simple queries for basic testing\"\"\"\n",
        "    return FLIGHT_SEARCH_QUERIES\n",
        "\n",
        "def get_flight_policy_queries():\n",
        "    \"\"\"Return flight policy queries (for backward compatibility).\"\"\"\n",
        "    return FLIGHT_SEARCH_QUERIES\n",
        "\n",
        "def get_reference_answer(query: str) -> str:\n",
        "    \"\"\"Get the correct reference answer for a given query\"\"\"\n",
        "    return QUERY_REFERENCE_ANSWERS.get(query, f\"No reference answer available for: {query}\")\n",
        "\n",
        "def get_all_query_references():\n",
        "    \"\"\"Get all query-reference pairs\"\"\"\n",
        "    return QUERY_REFERENCE_ANSWERS\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Akv3vV84mTCw",
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Clear Existing Data\n",
        "\n",
        "Clear existing bookings and reviews for clean test run.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBSREadbmTCw"
      },
      "outputs": [],
      "source": [
        "def clear_bookings_and_reviews():\n",
        "    \"\"\"Clear existing flight bookings to start fresh for demo.\"\"\"\n",
        "    try:\n",
        "        client = create_couchbase_client()\n",
        "        client.connect()\n",
        "\n",
        "        # Clear bookings scope using environment variables\n",
        "        bookings_scope = \"agentc_bookings\"\n",
        "        client.clear_scope(bookings_scope)\n",
        "        logger.info(\n",
        "            f\"✅ Cleared existing flight bookings for fresh test run: {os.environ['CB_BUCKET']}.{bookings_scope}\"\n",
        "        )\n",
        "\n",
        "        # Check if airline reviews collection needs clearing by comparing expected vs actual document count\n",
        "        try:\n",
        "            # Get expected document count (this uses cached data if available)\n",
        "            expected_docs = _data_manager.process_to_texts()\n",
        "            expected_count = len(expected_docs)\n",
        "\n",
        "            # Check current document count in collection\n",
        "            try:\n",
        "                count_query = f\"SELECT COUNT(*) as count FROM `{os.environ['CB_BUCKET']}`.`{os.environ['CB_SCOPE']}`.`{os.environ['CB_COLLECTION']}`\"\n",
        "                count_result = client.cluster.query(count_query)\n",
        "                count_row = next(iter(count_result))\n",
        "                existing_count = count_row[\"count\"]\n",
        "\n",
        "                logger.info(\n",
        "                    f\"📊 Airline reviews collection: {existing_count} existing, {expected_count} expected\"\n",
        "                )\n",
        "\n",
        "                if existing_count == expected_count:\n",
        "                    logger.info(\n",
        "                        f\"✅ Collection already has correct document count ({existing_count}), skipping clear\"\n",
        "                    )\n",
        "                else:\n",
        "                    logger.info(\n",
        "                        f\"🗑️  Clearing airline reviews collection: {os.environ['CB_BUCKET']}.{os.environ['CB_SCOPE']}.{os.environ['CB_COLLECTION']}\"\n",
        "                    )\n",
        "                    client.clear_collection_data(os.environ[\"CB_SCOPE\"], os.environ[\"CB_COLLECTION\"])\n",
        "                    logger.info(\n",
        "                        f\"✅ Cleared existing airline reviews for fresh data load: {os.environ['CB_BUCKET']}.{os.environ['CB_SCOPE']}.{os.environ['CB_COLLECTION']}\"\n",
        "                    )\n",
        "\n",
        "            except KeyspaceNotFoundException:\n",
        "                # Collection doesn't exist yet - this is expected for fresh setup\n",
        "                logger.info(\n",
        "                    f\"📊 Collection doesn't exist yet, will create and load fresh data\"\n",
        "                )\n",
        "            except Exception as count_error:\n",
        "                # Other query errors - clear anyway to ensure fresh start\n",
        "                logger.info(\n",
        "                    f\"📊 Collection query failed, will clear and reload: {count_error}\"\n",
        "                )\n",
        "                client.clear_collection_data(os.environ[\"CB_SCOPE\"], os.environ[\"CB_COLLECTION\"])\n",
        "                logger.info(\n",
        "                    f\"✅ Cleared existing airline reviews for fresh data load: {os.environ['CB_BUCKET']}.{os.environ['CB_SCOPE']}.{os.environ['CB_COLLECTION']}\"\n",
        "                )\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"⚠️  Could not check collection count, clearing anyway: {e}\")\n",
        "            client.clear_collection_data(os.environ[\"CB_SCOPE\"], os.environ[\"CB_COLLECTION\"])\n",
        "            logger.info(\n",
        "                f\"✅ Cleared existing airline reviews for fresh data load: {os.environ['CB_BUCKET']}.{os.environ['CB_SCOPE']}.{os.environ['CB_COLLECTION']}\"\n",
        "            )\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"❌ Could not clear bookings: {e}\")\n",
        "\n",
        "\n",
        "# Clear existing data for fresh test run\n",
        "clear_bookings_and_reviews()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZzJmJm4mTCx",
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Setup Flight Search Agent\n",
        "\n",
        "Initialize the complete flight search agent setup using the refactored approach.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7pFbwTkmTCx"
      },
      "outputs": [],
      "source": [
        "def setup_flight_search_agent():\n",
        "    \"\"\"Common setup function for flight search agent - returns all necessary components.\"\"\"\n",
        "    try:\n",
        "        # Setup environment first\n",
        "        setup_environment()\n",
        "\n",
        "        # Initialize Agent Catalog (uses .env or defaults if AGENT_CATALOG_* vars not set)\n",
        "        catalog = agentc.Catalog()\n",
        "        application_span = catalog.Span(name=\"Flight Search Agent\", blacklist=set())\n",
        "\n",
        "        # Test Capella AI connectivity\n",
        "        if os.getenv(\"CAPELLA_API_ENDPOINT\"):\n",
        "            if not test_capella_connectivity():\n",
        "                logger.warning(\"❌ Capella AI connectivity test failed. Will use fallback models.\")\n",
        "        else:\n",
        "            logger.info(\"ℹ️ Capella API not configured - will use fallback models\")\n",
        "\n",
        "        # Create CouchbaseClient for all operations\n",
        "        client = create_couchbase_client()\n",
        "\n",
        "        # Setup everything in one call - bucket, scope, collection\n",
        "        client.setup_collection(\n",
        "            scope_name=os.environ[\"CB_SCOPE\"],\n",
        "            collection_name=os.environ[\"CB_COLLECTION\"],\n",
        "            clear_existing_data=False,  # Let data loader decide based on count check\n",
        "        )\n",
        "\n",
        "        # Setup vector search index\n",
        "        try:\n",
        "            with open(\"agentcatalog_index.json\") as file:\n",
        "                index_definition = json.load(file)\n",
        "            logger.info(\"Loaded vector search index definition from agentcatalog_index.json\")\n",
        "            client.setup_vector_search_index(index_definition, os.environ[\"CB_SCOPE\"])\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Error loading index definition: {e!s}\")\n",
        "            logger.info(\"Continuing without vector search index...\")\n",
        "\n",
        "\n",
        "        # Setup AI services using Priority 1: Capella AI + OpenAI wrappers\n",
        "        embeddings, _ = setup_ai_services(framework=\"langgraph\")\n",
        "\n",
        "        # Setup vector store with airline reviews data\n",
        "        vector_store = client.setup_vector_store_langchain(\n",
        "            scope_name=os.environ[\"CB_SCOPE\"],\n",
        "            collection_name=os.environ[\"CB_COLLECTION\"],\n",
        "            index_name=os.environ[\"CB_INDEX\"],\n",
        "            embeddings=embeddings,\n",
        "            data_loader_func=load_airline_reviews_to_couchbase,\n",
        "        )\n",
        "\n",
        "        # Setup LLM using Priority 1: Capella AI + OpenAI wrappers\n",
        "        _, chat_model = setup_ai_services(framework=\"langgraph\", temperature=0.1)\n",
        "\n",
        "        # Create the flight search graph with the chat model\n",
        "        flight_graph = FlightSearchGraph(\n",
        "            catalog=catalog, span=application_span, chat_model=chat_model\n",
        "        )\n",
        "        # Compile the graph\n",
        "        compiled_graph = flight_graph.compile()\n",
        "\n",
        "        logger.info(\"Agent Catalog integration successful\")\n",
        "\n",
        "        return compiled_graph, application_span\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.exception(f\"Setup error: {e}\")\n",
        "        logger.info(\"Ensure Agent Catalog is published: agentc index . && agentc publish\")\n",
        "        raise\n",
        "\n",
        "\n",
        "def run_test_query(test_number: int, query: str, compiled_graph, application_span):\n",
        "    \"\"\"Run a single test query with error handling.\"\"\"\n",
        "    logger.info(f\"\\n🔍 Test {test_number}: {query}\")\n",
        "    try:\n",
        "        state = FlightSearchGraph.build_starting_state(query=query)\n",
        "        result = compiled_graph.invoke(state)\n",
        "\n",
        "        if result.get(\"search_results\"):\n",
        "            logger.info(f\"Found {len(result['search_results'])} flight options\")\n",
        "        logger.info(f\"✅ Test {test_number} completed: {result.get('resolved', False)}\")\n",
        "\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.exception(f\"❌ Test {test_number} failed: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# Setup the agent\n",
        "compiled_graph, application_span = setup_flight_search_agent()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puhVz-pvmTCx",
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Test 1: Flight Search\n",
        "\n",
        "Find flights from JFK to LAX for tomorrow.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k8Ba2EgVmTCx"
      },
      "outputs": [],
      "source": [
        "result1 = run_test_query(\n",
        "    1, \"Find flights from JFK to LAX for tomorrow\", compiled_graph, application_span\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YubHWNhvmTCx",
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Test 2: Flight Booking (Business Class)\n",
        "\n",
        "Book a flight with business class.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CbKwZgFSmTCx"
      },
      "outputs": [],
      "source": [
        "result2 = run_test_query(\n",
        "    2,\n",
        "    \"Book a flight from LAX to JFK for tomorrow, 2 passengers, business class\",\n",
        "    compiled_graph,\n",
        "    application_span,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDBEUzYImTCx",
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Test 3: Flight Booking (Economy Class)\n",
        "\n",
        "Book an economy flight.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "crBUQX-FmTC7"
      },
      "outputs": [],
      "source": [
        "result3 = run_test_query(\n",
        "    3,\n",
        "    \"Book an economy flight from JFK to MIA for next week, 1 passenger\",\n",
        "    compiled_graph,\n",
        "    application_span,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9M4J-KDmTC7",
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Test 4: Retrieve Current Bookings\n",
        "\n",
        "Show current flight bookings.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTa0vaxjmTC8"
      },
      "outputs": [],
      "source": [
        "result4 = run_test_query(4, \"Show me my current flight bookings\", compiled_graph, application_span)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e5zJao2mTC8",
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Test 5: Airline Reviews Search\n",
        "\n",
        "Search airline reviews for service quality.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8PJhI7e7mTC8"
      },
      "outputs": [],
      "source": [
        "result5 = run_test_query(\n",
        "    5, \"What do passengers say about SpiceJet's service quality?\", compiled_graph, application_span\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWQzh0MrmTC8",
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Arize Phoenix Evaluation\n",
        "\n",
        "This section demonstrates how to evaluate the flight search agent using Arize Phoenix observability platform. The evaluation includes:\n",
        "\n",
        "- **Relevance Scoring**: Using Phoenix RelevanceEvaluator to score how relevant responses are to queries\n",
        "- **QA Scoring**: Using Phoenix QAEvaluator with lenient evaluation templates for better accuracy\n",
        "- **Hallucination Detection**: Using Phoenix HallucinationEvaluator with lenient templates to detect fabricated information\n",
        "- **Toxicity Detection**: Using Phoenix ToxicityEvaluator to detect harmful content\n",
        "- **Phoenix UI**: Real-time observability dashboard\n",
        "\n",
        "We'll run evaluation queries and assess the responses for quality and safety using the latest evaluation approach.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qUXXU77hmTC8"
      },
      "outputs": [],
      "source": [
        "# Import Phoenix evaluation components and nest_asyncio for better notebook performance\n",
        "try:\n",
        "    import nest_asyncio\n",
        "    import pandas as pd\n",
        "    import phoenix as px\n",
        "    from phoenix.evals import (\n",
        "        RAG_RELEVANCY_PROMPT_RAILS_MAP,\n",
        "        RAG_RELEVANCY_PROMPT_TEMPLATE,\n",
        "        TOXICITY_PROMPT_RAILS_MAP,\n",
        "        TOXICITY_PROMPT_TEMPLATE,\n",
        "        OpenAIModel,\n",
        "        llm_classify,\n",
        "    )\n",
        "\n",
        "    # Apply the patch to allow nested asyncio event loops\n",
        "    nest_asyncio.apply()\n",
        "\n",
        "    # Define lenient evaluation templates inline for self-contained notebook\n",
        "    LENIENT_QA_PROMPT_TEMPLATE = \"\"\"\n",
        "You are evaluating whether an AI agent's response correctly addresses a user's question.\n",
        "\n",
        "FOCUS ON FUNCTIONAL SUCCESS, NOT EXACT MATCHING:\n",
        "1. Did the agent provide the requested information (flights, bookings, reviews)?\n",
        "2. Is the core information accurate and helpful to the user?\n",
        "3. Would the user be satisfied with what they received?\n",
        "\n",
        "DYNAMIC DATA IS EXPECTED AND CORRECT:\n",
        "- Booking IDs will be DIFFERENT each time (dynamically generated - this is correct!)\n",
        "- Dates like \"tomorrow\" are calculated dynamically (may differ from reference)\n",
        "- Booking lists reflect ACTUAL session bookings (may differ from reference)\n",
        "- Route sequences depend on actual booking order in this session\n",
        "\n",
        "IGNORE THESE DIFFERENCES:\n",
        "- Different booking IDs, dates, or sequences (these are dynamic!)\n",
        "- Format differences, duplicate calls, system messages\n",
        "- Reference mismatches due to dynamic data\n",
        "\n",
        "MARK AS CORRECT IF:\n",
        "- Agent successfully completed the action (found flights, made booking, retrieved bookings, got reviews)\n",
        "- User received useful, accurate information\n",
        "- Core functionality worked as expected\n",
        "\n",
        "Question: {input}\n",
        "Reference Answer: {reference}\n",
        "Agent Response: {output}\n",
        "\n",
        "Did the agent successfully provide what the user requested, regardless of exact reference matching?\n",
        "Respond with just \"correct\" or \"incorrect\".\n",
        "\"\"\"\n",
        "\n",
        "    LENIENT_HALLUCINATION_PROMPT_TEMPLATE = \"\"\"\n",
        "You are checking if an AI agent's response contains hallucinated information.\n",
        "\n",
        "DYNAMIC DATA IS EXPECTED AND FACTUAL:\n",
        "- Booking IDs are dynamically generated (will ALWAYS be different from reference - this is correct!)\n",
        "- Dates are calculated dynamically (\"tomorrow\", \"next week\" based on current date)\n",
        "- Booking sequences reflect actual session bookings (not static reference data)\n",
        "- Tool outputs contain real system data\n",
        "\n",
        "MARK AS FACTUAL IF:\n",
        "- Response contains \"iteration limit\" or \"time limit\" (system issue, not hallucination)\n",
        "- Dynamic data differs from reference (booking IDs, dates, booking sequences)\n",
        "- Agent provides plausible flight data, booking confirmations, or reviews\n",
        "- Information is consistent with system capabilities\n",
        "\n",
        "ONLY MARK AS HALLUCINATED IF:\n",
        "- Response contains clearly impossible information (fake airlines, impossible routes)\n",
        "- Agent makes up data it cannot access\n",
        "- Response contradicts fundamental system facts\n",
        "\n",
        "REMEMBER: Different booking IDs, dates, and sequences are EXPECTED dynamic behavior!\n",
        "\n",
        "Question: {input}\n",
        "Reference Text: {reference}\n",
        "Agent Response: {output}\n",
        "\n",
        "Does the response contain clearly false information, ignoring expected dynamic data differences?\n",
        "Respond with just \"factual\" or \"hallucinated\".\n",
        "\"\"\"\n",
        "\n",
        "    # Custom Rails\n",
        "    LENIENT_QA_RAILS = [\"correct\", \"incorrect\"]\n",
        "    LENIENT_HALLUCINATION_RAILS = [\"factual\", \"hallucinated\"]\n",
        "\n",
        "    ARIZE_AVAILABLE = True\n",
        "    logger.info(\"✅ Arize Phoenix evaluation components available\")\n",
        "except ImportError as e:\n",
        "    logger.warning(f\"Arize dependencies not available: {e}\")\n",
        "    logger.warning(\"Skipping evaluation section...\")\n",
        "    ARIZE_AVAILABLE = False\n",
        "\n",
        "if ARIZE_AVAILABLE:\n",
        "    # Start Phoenix session for observability\n",
        "    try:\n",
        "        session = px.launch_app()\n",
        "        if session:\n",
        "            logger.info(f\"🚀 Phoenix UI available at {session.url}\")\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"Could not start Phoenix UI: {e}\")\n",
        "\n",
        "    # Demo queries for evaluation\n",
        "    flight_demo_queries = [\n",
        "        \"Find flights from JFK to LAX\",\n",
        "        \"What do passengers say about SpiceJet's service quality?\",\n",
        "    ]\n",
        "\n",
        "    # Run demo queries and collect responses for evaluation\n",
        "    flight_demo_results = []\n",
        "\n",
        "    for i, query in enumerate(flight_demo_queries, 1):\n",
        "        try:\n",
        "            logger.info(f\"🔍 Running evaluation query {i}: {query}\")\n",
        "\n",
        "            # Create initial state and run the compiled graph\n",
        "            state = FlightSearchGraph.build_starting_state(query=query)\n",
        "            result = compiled_graph.invoke(state)\n",
        "\n",
        "            # Extract the response content including tool results\n",
        "            response_parts = []\n",
        "\n",
        "            # Critical Fix: Extract tool outputs from search_results first\n",
        "            if isinstance(result, dict) and \"search_results\" in result:\n",
        "                search_results = result[\"search_results\"]\n",
        "                if search_results:\n",
        "                    response_parts.append(str(search_results))\n",
        "\n",
        "            # Check for messages from final response\n",
        "            if result.get(\"messages\") and len(result[\"messages\"]) > 1:\n",
        "                final_response = result[\"messages\"][-1].content\n",
        "                if final_response:\n",
        "                    response_parts.append(final_response)\n",
        "\n",
        "            # Join all response parts\n",
        "            output = \"\\n\\n\".join(response_parts) if response_parts else \"No response generated\"\n",
        "\n",
        "            flight_demo_results.append(\n",
        "                {\n",
        "                    \"query\": query,\n",
        "                    \"response\": output,\n",
        "                    \"success\": result.get(\"resolved\", False),\n",
        "                }\n",
        "            )\n",
        "\n",
        "            logger.info(f\"✅ Query {i} completed successfully\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.exception(f\"❌ Query {i} failed: {e}\")\n",
        "            flight_demo_results.append(\n",
        "                {\n",
        "                    \"query\": query,\n",
        "                    \"response\": f\"Error: {e!s}\",\n",
        "                    \"success\": False,\n",
        "                }\n",
        "            )\n",
        "\n",
        "    # Convert to DataFrame for evaluation\n",
        "    flight_results_df = pd.DataFrame(flight_demo_results)\n",
        "    logger.info(f\"📊 Collected {len(flight_results_df)} responses for evaluation\")\n",
        "\n",
        "    # Display results summary\n",
        "    for _, row in flight_results_df.iterrows():\n",
        "        logger.info(f\"Query: {row['query']}\")\n",
        "        logger.info(f\"Response: {row['response'][:200]}...\")\n",
        "        logger.info(f\"Success: {row['success']}\")\n",
        "        logger.info(\"-\" * 50)\n",
        "\n",
        "    logger.info(\"💡 Visit Phoenix UI to see detailed traces and evaluations\")\n",
        "    logger.info(\"💡 Use the evaluation script at evals/eval_arize.py for comprehensive evaluation\")\n",
        "\n",
        "else:\n",
        "    logger.info(\"Arize evaluation not available - install phoenix-evals to enable evaluation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bEgPxq8PmTC8"
      },
      "outputs": [],
      "source": [
        "if ARIZE_AVAILABLE and len(flight_demo_results) > 0:\n",
        "    logger.info(\"🔍 Running comprehensive Phoenix evaluations with lenient templates...\")\n",
        "\n",
        "    # Setup evaluator LLM (using OpenAI for consistency)\n",
        "    evaluator_llm = OpenAIModel(model=\"gpt-4o\", temperature=0.1)\n",
        "\n",
        "    # Reference answers copied from data/queries.py (proper copy-paste as requested)\n",
        "    FLIGHT_REFERENCE_ANSWERS = [\n",
        "        # Query 1: Flight search JFK to LAX\n",
        "        \"\"\"Available flights from JFK to LAX:\n",
        "\n",
        "1. AS flight from JFK to LAX using 321 762\n",
        "2. B6 flight from JFK to LAX using 320\n",
        "3. DL flight from JFK to LAX using 76W 752\n",
        "4. QF flight from JFK to LAX using 744\n",
        "5. AA flight from JFK to LAX using 32B 762\n",
        "6. UA flight from JFK to LAX using 757\n",
        "7. US flight from JFK to LAX using 32B 762\n",
        "8. VX flight from JFK to LAX using 320\"\"\",\n",
        "\n",
        "        # Query 2: Flight booking LAX to JFK for tomorrow, 2 passengers, business class\n",
        "        \"\"\"Flight Booking Confirmed!\n",
        "\n",
        "Booking ID: FL08061563CACD\n",
        "Route: LAX → JFK\n",
        "Departure Date: 2025-08-06\n",
        "Passengers: 2\n",
        "Class: business\n",
        "Total Price: $1500.00\n",
        "\n",
        "Next Steps:\n",
        "1. Check-in opens 24 hours before departure\n",
        "2. Arrive at airport 2 hours early for domestic flights\n",
        "3. Bring valid government-issued photo ID\n",
        "\n",
        "Thank you for choosing our airline!\"\"\",\n",
        "\n",
        "        # Query 3: Flight booking JFK to MIA for next week\n",
        "        \"\"\"Flight Booking Confirmed!\n",
        "\n",
        "Booking ID: FL08124E7B9C2A\n",
        "Route: JFK → MIA\n",
        "Departure Date: 2025-08-12\n",
        "Passengers: 1\n",
        "Class: economy\n",
        "Total Price: $250.00\n",
        "\n",
        "Next Steps:\n",
        "1. Check-in opens 24 hours before departure\n",
        "2. Arrive at airport 2 hours early for domestic flights\n",
        "3. Bring valid government-issued photo ID\n",
        "\n",
        "Thank you for choosing our airline!\"\"\",\n",
        "\n",
        "        # Query 4: Show current flight bookings\n",
        "        \"\"\"Your Current Bookings (2 found):\n",
        "\n",
        "Booking 1:\n",
        "  Booking ID: FL08061563CACD\n",
        "  Route: LAX → JFK\n",
        "  Date: 2025-08-06\n",
        "  Passengers: 2\n",
        "  Class: business\n",
        "  Total: $1500.00\n",
        "  Status: confirmed\n",
        "  Booked: 2025-08-05\n",
        "\n",
        "Booking 2:\n",
        "  Booking ID: FL08124E7B9C2A\n",
        "  Route: JFK → MIA\n",
        "  Date: 2025-08-12\n",
        "  Passengers: 1\n",
        "  Class: economy\n",
        "  Total: $250.00\n",
        "  Status: confirmed\n",
        "  Booked: 2025-08-05\"\"\",\n",
        "\n",
        "        # Query 5: SpiceJet service quality reviews\n",
        "        \"\"\"Found 5 relevant airline reviews for 'SpiceJet service':\n",
        "\n",
        "Review 1:\n",
        "Airline: SpiceJet. Title: \"Service is impeccable\". Review: ✅ Trip Verified | Much better than airbus models. Even the basic economy class has ambient lighting. Better personal air vents and better spotlights. Even overhead storage bins are good. Service is impeccable with proper care taken of guests...\n",
        "\n",
        "Review 2:\n",
        "Airline: SpiceJet. Title: \"good service by the crew\". Review: ✅ Trip Verified | I have had good service by the crew. It was amazing, the crew was very enthusiastic and warm welcome. It was one of the best services in my experience.. Rating: 10.0/10. Reviewer: K Mansour. Date: 10th August 2024. Recom...\n",
        "\n",
        "Review 3:\n",
        "Airline: SpiceJet. Title: \"outstanding service I experienced\". Review: Not Verified |  I wanted to take a moment to express my sincere thanks for the outstanding service I experienced on my recent flight from Pune to Delhi. SG-8937. From the moment I boarded, the warmth and friendliness of the air h...\n",
        "\n",
        "Review 4:\n",
        "Airline: SpiceJet. Title: \"efficient and warm onboard service\". Review: ✅ Trip Verified |  New Delhi to Kolkata. Delighted with the prompt, efficient and warm onboard service provided by the crew. Appreciate their efforts towards customer centricity.. Rating: 10.0/10. Reviewer: Debashis Roy. Date: 2...\n",
        "\n",
        "Review 5:\n",
        "Airline: SpiceJet. Title: \"Outstanding service from SpiceJet\". Review: ✅ Trip Verified | I recently flew with SpiceJet from Mumbai to Delhi and was thoroughly impressed with the level of service provided. The check-in process was smooth and efficient, and the staff at the counter were courteous and helpful. The aircraft was clean and well-maintained, and the seats were comfortable for the duration of the flight. The in-flight service was exceptional, with the cabin crew being attentive and professional throughout the journey. The meal served was tasty and well-presented, and the entertainment system kept me engaged during the flight. Overall, I had a fantastic experience with SpiceJet and would definitely recommend them to other travelers. Rating: 9.5/10. Reviewer: Priya Sharma. Date: 15th July 2024.\"\"\",\n",
        "    ]\n",
        "\n",
        "    # Queries copied from data/queries.py\n",
        "    FLIGHT_SEARCH_QUERIES = [\n",
        "        \"Find flights from JFK to LAX\",\n",
        "        \"Book a flight from LAX to JFK for tomorrow, 2 passengers, business class\",\n",
        "        \"Book an economy flight from JFK to MIA for next week, 1 passenger\",\n",
        "        \"Show me my current flight bookings\",\n",
        "        \"What do passengers say about SpiceJet's service quality?\",\n",
        "    ]\n",
        "\n",
        "    # Create mapping dictionary like the working source files\n",
        "    QUERY_REFERENCE_ANSWERS = {\n",
        "        query: answer for query, answer in zip(FLIGHT_SEARCH_QUERIES, FLIGHT_REFERENCE_ANSWERS)\n",
        "    }\n",
        "\n",
        "    # Prepare evaluation data with proper column names for Phoenix evaluators\n",
        "    flight_eval_data = []\n",
        "    for _, row in flight_results_df.iterrows():\n",
        "        flight_eval_data.append(\n",
        "            {\n",
        "                \"input\": row[\"query\"],\n",
        "                \"output\": row[\"response\"],\n",
        "                \"reference\": QUERY_REFERENCE_ANSWERS.get(row[\"query\"], \"Reference answer not found\"),\n",
        "                \"text\": row[\"response\"],  # For toxicity evaluation\n",
        "            }\n",
        "        )\n",
        "\n",
        "    flight_eval_df = pd.DataFrame(flight_eval_data)\n",
        "\n",
        "    try:\n",
        "        # 1. Relevance Evaluation\n",
        "        logger.info(\"🔍 Running Relevance Evaluation...\")\n",
        "        flight_relevance_results = llm_classify(\n",
        "            data=flight_eval_df[[\"input\", \"reference\"]],\n",
        "            model=evaluator_llm,\n",
        "            template=RAG_RELEVANCY_PROMPT_TEMPLATE,\n",
        "            rails=list(RAG_RELEVANCY_PROMPT_RAILS_MAP.values()),\n",
        "            provide_explanation=True,\n",
        "        )\n",
        "\n",
        "        logger.info(\"✅ Relevance Evaluation Results:\")\n",
        "        for i, row in flight_relevance_results.iterrows():\n",
        "            query = flight_eval_data[i][\"input\"]\n",
        "            logger.info(f\"   Query: {query}\")\n",
        "            logger.info(f\"   Relevance: {row.get('label', row.get('classification', 'unknown'))}\")\n",
        "            logger.info(f\"   Explanation: {row.get('explanation', 'No explanation')}\")\n",
        "            logger.info(\"   \" + \"-\" * 30)\n",
        "\n",
        "        # 2. QA Evaluation with Lenient Templates\n",
        "        logger.info(\"🔍 Running QA Evaluation with Lenient Templates...\")\n",
        "        flight_qa_results = llm_classify(\n",
        "            data=flight_eval_df[[\"input\", \"output\", \"reference\"]],\n",
        "            model=evaluator_llm,\n",
        "            template=LENIENT_QA_PROMPT_TEMPLATE,\n",
        "            rails=LENIENT_QA_RAILS,\n",
        "            provide_explanation=True,\n",
        "        )\n",
        "\n",
        "        logger.info(\"✅ QA Evaluation Results:\")\n",
        "        for i, row in flight_qa_results.iterrows():\n",
        "            query = flight_eval_data[i][\"input\"]\n",
        "            logger.info(f\"   Query: {query}\")\n",
        "            logger.info(f\"   QA Score: {row.get('label', row.get('classification', 'unknown'))}\")\n",
        "            logger.info(f\"   Explanation: {row.get('explanation', 'No explanation')}\")\n",
        "            logger.info(\"   \" + \"-\" * 30)\n",
        "\n",
        "        # 3. Hallucination Evaluation with Lenient Templates\n",
        "        logger.info(\"🔍 Running Hallucination Evaluation with Lenient Templates...\")\n",
        "        flight_hallucination_results = llm_classify(\n",
        "            data=flight_eval_df[[\"input\", \"reference\", \"output\"]],\n",
        "            model=evaluator_llm,\n",
        "            template=LENIENT_HALLUCINATION_PROMPT_TEMPLATE,\n",
        "            rails=LENIENT_HALLUCINATION_RAILS,\n",
        "            provide_explanation=True,\n",
        "        )\n",
        "\n",
        "        logger.info(\"✅ Hallucination Evaluation Results:\")\n",
        "        for i, row in flight_hallucination_results.iterrows():\n",
        "            query = flight_eval_data[i][\"input\"]\n",
        "            logger.info(f\"   Query: {query}\")\n",
        "            hallucination_result = row.get(\"label\", row.get(\"classification\", \"unknown\"))\n",
        "            logger.info(f\"   Hallucination: {hallucination_result}\")\n",
        "            logger.info(f\"   Explanation: {row.get('explanation', 'No explanation')}\")\n",
        "\n",
        "            # Add warning for hallucinated responses\n",
        "            if hallucination_result.lower() in [\"hallucinated\", \"hallucination\", \"yes\"]:\n",
        "                logger.warning(f\"⚠️  HALLUCINATION DETECTED in response to: {query}\")\n",
        "                logger.warning(f\"   Response may contain fabricated information!\")\n",
        "\n",
        "            logger.info(\"   \" + \"-\" * 30)\n",
        "\n",
        "        # 4. Toxicity Evaluation\n",
        "        logger.info(\"🔍 Running Toxicity Evaluation...\")\n",
        "        flight_toxicity_results = llm_classify(\n",
        "            data=flight_eval_df[[\"input\"]],\n",
        "            model=evaluator_llm,\n",
        "            template=TOXICITY_PROMPT_TEMPLATE,\n",
        "            rails=list(TOXICITY_PROMPT_RAILS_MAP.values()),\n",
        "            provide_explanation=True,\n",
        "        )\n",
        "\n",
        "        logger.info(\"✅ Toxicity Evaluation Results:\")\n",
        "        for i, row in flight_toxicity_results.iterrows():\n",
        "            query = flight_eval_data[i][\"input\"]\n",
        "            logger.info(f\"   Query: {query}\")\n",
        "            logger.info(f\"   Toxicity: {row.get('label', row.get('classification', 'unknown'))}\")\n",
        "            logger.info(f\"   Explanation: {row.get('explanation', 'No explanation')}\")\n",
        "            logger.info(\"   \" + \"-\" * 30)\n",
        "\n",
        "        # Summary with improved factual validation\n",
        "        logger.info(\"📊 EVALUATION SUMMARY\")\n",
        "        logger.info(\"=\" * 60)\n",
        "\n",
        "        factual_issues = 0\n",
        "        for i, query in enumerate([item[\"input\"] for item in flight_eval_data]):\n",
        "            relevance = flight_relevance_results.iloc[i].get(\"label\", \"unknown\")\n",
        "            qa_score = flight_qa_results.iloc[i].get(\"label\", \"unknown\")\n",
        "            hallucination = flight_hallucination_results.iloc[i].get(\"label\", \"unknown\")\n",
        "            toxicity = flight_toxicity_results.iloc[i].get(\"label\", \"unknown\")\n",
        "\n",
        "            logger.info(f\"Query {i + 1}: {query}\")\n",
        "            logger.info(f\"  Relevance: {relevance}\")\n",
        "            logger.info(f\"  QA Score: {qa_score}\")\n",
        "            logger.info(f\"  Hallucination: {hallucination}\")\n",
        "            logger.info(f\"  Toxicity: {toxicity}\")\n",
        "\n",
        "            # Check for factual issues\n",
        "            if hallucination.lower() in [\n",
        "                \"hallucinated\",\n",
        "                \"hallucination\",\n",
        "            ] or qa_score.lower() in [\"incorrect\"]:\n",
        "                factual_issues += 1\n",
        "                logger.warning(f\"  🚨 FACTUAL ISSUE DETECTED!\")\n",
        "\n",
        "            logger.info(\"  \" + \"-\" * 50)\n",
        "\n",
        "        # Overall factual quality assessment\n",
        "        logger.info(\"\\n🎯 FACTUAL QUALITY ASSESSMENT\")\n",
        "        logger.info(\"=\" * 40)\n",
        "        total_queries = len(flight_eval_data)\n",
        "        factual_accuracy = ((total_queries - factual_issues) / total_queries) * 100\n",
        "\n",
        "        logger.info(f\"Total Queries: {total_queries}\")\n",
        "        logger.info(f\"Factual Issues: {factual_issues}\")\n",
        "        logger.info(f\"Factual Accuracy: {factual_accuracy:.1f}%\")\n",
        "\n",
        "        if factual_accuracy < 80:\n",
        "            logger.error(\"❌ POOR FACTUAL ACCURACY - Need immediate attention!\")\n",
        "        elif factual_accuracy < 90:\n",
        "            logger.warning(\"⚠️  MODERATE FACTUAL ACCURACY - Review needed\")\n",
        "        else:\n",
        "            logger.info(\"✅ GOOD FACTUAL ACCURACY\")\n",
        "\n",
        "        logger.info(\"✅ All Phoenix evaluations completed successfully!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.exception(f\"❌ Phoenix evaluation failed: {e}\")\n",
        "        logger.info(\"💡 This might be due to API rate limits or model availability\")\n",
        "        logger.info(\"💡 Try again with a different model or check your API keys\")\n",
        "\n",
        "else:\n",
        "    if not ARIZE_AVAILABLE:\n",
        "        logger.info(\"❌ Phoenix evaluations skipped - Arize dependencies not available\")\n",
        "    else:\n",
        "        logger.info(\"❌ Phoenix evaluations skipped - No demo results to evaluate\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4WANBV3mTC8",
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Evaluation Configuration\n",
        "\n",
        "Configuration class for the evaluation system with all parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VtGx7JSvmTC8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import nest_asyncio\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Optional, Tuple, Any\n",
        "\n",
        "# Apply the patch to allow nested asyncio event loops\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Try to import Arize dependencies with fallback\n",
        "try:\n",
        "    import phoenix as px\n",
        "    from arize.experimental.datasets import ArizeDatasetsClient\n",
        "    from arize.experimental.datasets.utils.constants import GENERATIVE\n",
        "    from openinference.instrumentation.langchain import LangChainInstrumentor\n",
        "    from openinference.instrumentation.openai import OpenAIInstrumentor\n",
        "    from phoenix.evals import (\n",
        "        RAG_RELEVANCY_PROMPT_RAILS_MAP,\n",
        "        RAG_RELEVANCY_PROMPT_TEMPLATE,\n",
        "        TOXICITY_PROMPT_RAILS_MAP,\n",
        "        TOXICITY_PROMPT_TEMPLATE,\n",
        "        HallucinationEvaluator,\n",
        "        OpenAIModel,\n",
        "        QAEvaluator,\n",
        "        RelevanceEvaluator,\n",
        "        ToxicityEvaluator,\n",
        "        llm_classify,\n",
        "    )\n",
        "    from phoenix.otel import register\n",
        "\n",
        "    ARIZE_AVAILABLE = True\n",
        "except ImportError as e:\n",
        "    logger.warning(f\"Arize dependencies not available: {e}\")\n",
        "    logger.warning(\"Running in local evaluation mode only...\")\n",
        "    ARIZE_AVAILABLE = False\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class EvaluationConfig:\n",
        "    \"\"\"Configuration for the evaluation system.\"\"\"\n",
        "\n",
        "    # Arize Configuration\n",
        "    arize_space_id: str = os.getenv(\"ARIZE_SPACE_ID\", \"your-space-id\")\n",
        "    arize_api_key: str = os.getenv(\"ARIZE_API_KEY\", \"your-api-key\")\n",
        "    project_name: str = \"flight-search-agent-evaluation\"\n",
        "\n",
        "    # Phoenix Configuration\n",
        "    phoenix_base_port: int = 6006\n",
        "    phoenix_grpc_base_port: int = 4317\n",
        "    phoenix_max_port_attempts: int = 5\n",
        "    phoenix_startup_timeout: int = 30\n",
        "\n",
        "    # Evaluation Configuration\n",
        "    evaluator_model: str = \"gpt-4o\"\n",
        "    batch_size: int = 10\n",
        "    max_retries: int = 3\n",
        "    evaluation_timeout: int = 300\n",
        "\n",
        "    # Logging Configuration\n",
        "    log_level: str = \"INFO\"\n",
        "    detailed_logging: bool = True\n",
        "\n",
        "    # Dataset Configuration\n",
        "    dataset_name: str = \"flight-search-evaluation-queries\"\n",
        "    dataset_description: str = \"Flight search agent evaluation queries with expected responses\"\n",
        "\n",
        "    # Output Configuration\n",
        "    save_results: bool = True\n",
        "    results_format: str = \"csv\"  # csv, json, both\n",
        "    output_dir: str = \"output\"\n",
        "\n",
        "    def __post_init__(self):\n",
        "        \"\"\"Validate configuration after initialization.\"\"\"\n",
        "        if self.phoenix_base_port < 1024 or self.phoenix_base_port > 65535:\n",
        "            raise ValueError(\"Phoenix base port must be between 1024 and 65535\")\n",
        "\n",
        "        if self.batch_size < 1 or self.batch_size > 100:\n",
        "            raise ValueError(\"Batch size must be between 1 and 100\")\n",
        "\n",
        "        if self.evaluation_timeout < 30:\n",
        "            raise ValueError(\"Evaluation timeout must be at least 30 seconds\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zB9KfoEBmTC9",
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Phoenix Manager\n",
        "\n",
        "Manages Phoenix server lifecycle and port management.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-eWQOiSmTC9"
      },
      "outputs": [],
      "source": [
        "import socket\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "class PhoenixManager:\n",
        "    \"\"\"Manages Phoenix server lifecycle and port management.\"\"\"\n",
        "\n",
        "    def __init__(self, config: EvaluationConfig):\n",
        "        self.config = config\n",
        "        self.session = None\n",
        "        self.active_port = None\n",
        "        self.tracer_provider = None\n",
        "\n",
        "    def _is_port_in_use(self, port: int) -> bool:\n",
        "        \"\"\"Check if a port is in use.\"\"\"\n",
        "        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
        "            return s.connect_ex((\"localhost\", port)) == 0\n",
        "\n",
        "    def _kill_existing_phoenix_processes(self) -> None:\n",
        "        \"\"\"Kill any existing Phoenix processes.\"\"\"\n",
        "        try:\n",
        "            subprocess.run([\"pkill\", \"-f\", \"phoenix\"], check=False, capture_output=True)\n",
        "            time.sleep(2)  # Wait for processes to terminate\n",
        "        except Exception as e:\n",
        "            logger.debug(f\"Error killing Phoenix processes: {e}\")\n",
        "\n",
        "    def _find_available_port(self) -> Tuple[int, int]:\n",
        "        \"\"\"Find available ports for Phoenix.\"\"\"\n",
        "        phoenix_port = self.config.phoenix_base_port\n",
        "        grpc_port = self.config.phoenix_grpc_base_port\n",
        "\n",
        "        for _ in range(self.config.phoenix_max_port_attempts):\n",
        "            if not self._is_port_in_use(phoenix_port):\n",
        "                return phoenix_port, grpc_port\n",
        "            phoenix_port += 1\n",
        "            grpc_port += 1\n",
        "\n",
        "        raise RuntimeError(\n",
        "            f\"Could not find available ports after {self.config.phoenix_max_port_attempts} attempts\"\n",
        "        )\n",
        "\n",
        "    def start_phoenix(self) -> bool:\n",
        "        \"\"\"Start Phoenix server and return success status.\"\"\"\n",
        "        if not ARIZE_AVAILABLE:\n",
        "            logger.warning(\"⚠️ Phoenix dependencies not available\")\n",
        "            return False\n",
        "\n",
        "        try:\n",
        "            logger.info(\"🔧 Setting up Phoenix observability...\")\n",
        "\n",
        "            # Clean up existing processes\n",
        "            self._kill_existing_phoenix_processes()\n",
        "\n",
        "            # Find available ports\n",
        "            phoenix_port, grpc_port = self._find_available_port()\n",
        "\n",
        "            # Set environment variables\n",
        "            os.environ[\"PHOENIX_PORT\"] = str(phoenix_port)\n",
        "            os.environ[\"PHOENIX_GRPC_PORT\"] = str(grpc_port)\n",
        "\n",
        "            # Start Phoenix session\n",
        "            self.session = px.launch_app()\n",
        "            self.active_port = phoenix_port\n",
        "\n",
        "            if self.session:\n",
        "                logger.info(f\"🌐 Phoenix UI: {self.session.url}\")\n",
        "\n",
        "            # Register Phoenix OTEL\n",
        "            self.tracer_provider = register(\n",
        "                project_name=self.config.project_name,\n",
        "                endpoint=f\"http://localhost:{phoenix_port}/v1/traces\",\n",
        "            )\n",
        "\n",
        "            logger.info(\"✅ Phoenix setup completed successfully\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.exception(f\"❌ Phoenix setup failed: {e}\")\n",
        "            return False\n",
        "\n",
        "    def setup_instrumentation(self) -> bool:\n",
        "        \"\"\"Setup OpenTelemetry instrumentation.\"\"\"\n",
        "        if not self.tracer_provider or not ARIZE_AVAILABLE:\n",
        "            return False\n",
        "\n",
        "        try:\n",
        "            instrumentors = [\n",
        "                (\"LangChain\", LangChainInstrumentor),\n",
        "                (\"OpenAI\", OpenAIInstrumentor),\n",
        "            ]\n",
        "\n",
        "            for name, instrumentor_class in instrumentors:\n",
        "                try:\n",
        "                    instrumentor = instrumentor_class()\n",
        "                    instrumentor.instrument(tracer_provider=self.tracer_provider)\n",
        "                    logger.info(f\"✅ {name} instrumentation enabled\")\n",
        "                except Exception as e:\n",
        "                    logger.warning(f\"⚠️ {name} instrumentation failed: {e}\")\n",
        "\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.exception(f\"❌ Instrumentation setup failed: {e}\")\n",
        "            return False\n",
        "\n",
        "    def cleanup(self) -> None:\n",
        "        \"\"\"Clean up Phoenix resources.\"\"\"\n",
        "        try:\n",
        "            # Clean up environment variables\n",
        "            for var in [\"PHOENIX_PORT\", \"PHOENIX_GRPC_PORT\"]:\n",
        "                if var in os.environ:\n",
        "                    del os.environ[var]\n",
        "\n",
        "            logger.info(\"🔒 Phoenix cleanup completed\")\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"⚠️ Error during Phoenix cleanup: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mjlAzXBmTC9",
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Arize Dataset Manager\n",
        "\n",
        "Manages Arize dataset creation and management.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_MM1xe-mTC9"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "class ArizeDatasetManager:\n",
        "    \"\"\"Manages Arize dataset creation and management.\"\"\"\n",
        "\n",
        "    def __init__(self, config: EvaluationConfig):\n",
        "        self.config = config\n",
        "        self.client = None\n",
        "        self._setup_client()\n",
        "\n",
        "    def _setup_client(self) -> None:\n",
        "        \"\"\"Setup Arize datasets client.\"\"\"\n",
        "        if not ARIZE_AVAILABLE:\n",
        "            return\n",
        "\n",
        "        if (\n",
        "            self.config.arize_api_key != \"your-api-key\"\n",
        "            and self.config.arize_space_id != \"your-space-id\"\n",
        "        ):\n",
        "            try:\n",
        "                # Initialize with correct parameters - no space_id needed for datasets client\n",
        "                self.client = ArizeDatasetsClient(\n",
        "                    api_key=self.config.arize_api_key\n",
        "                )\n",
        "                logger.info(\"✅ Arize datasets client initialized successfully\")\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"⚠️ Could not initialize Arize datasets client: {e}\")\n",
        "                self.client = None\n",
        "        else:\n",
        "            logger.warning(\"⚠️ Arize API credentials not configured\")\n",
        "            self.client = None\n",
        "\n",
        "    def create_dataset(self, results_df: pd.DataFrame) -> Optional[str]:\n",
        "        \"\"\"Create Arize dataset from evaluation results.\"\"\"\n",
        "        if not self.client:\n",
        "            logger.warning(\"⚠️ Arize client not available - skipping dataset creation\")\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            dataset_name = f\"flight-search-evaluation-{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "\n",
        "            logger.info(\"📊 Creating Arize dataset...\")\n",
        "            dataset_id = self.client.create_dataset(\n",
        "                space_id=self.config.arize_space_id,\n",
        "                dataset_name=dataset_name,\n",
        "                dataset_type=GENERATIVE,\n",
        "                data=results_df,\n",
        "                convert_dict_to_json=True\n",
        "            )\n",
        "\n",
        "            if dataset_id:\n",
        "                logger.info(f\"✅ Arize dataset created successfully: {dataset_id}\")\n",
        "                return dataset_id\n",
        "            else:\n",
        "                logger.warning(\"⚠️ Dataset creation returned None\")\n",
        "                return None\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"❌ Error creating Arize dataset: {e}\")\n",
        "            return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZYhP6E3mTC9",
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Arize Flight Search Evaluator\n",
        "\n",
        "Main evaluator class for comprehensive flight search agent evaluation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7OmaB3d-mTC9"
      },
      "outputs": [],
      "source": [
        "class ArizeFlightSearchEvaluator:\n",
        "    \"\"\"\n",
        "    Streamlined flight search agent evaluator using only Arize Phoenix evaluators.\n",
        "\n",
        "    This class provides comprehensive evaluation capabilities using:\n",
        "    - Phoenix RelevanceEvaluator for response relevance\n",
        "    - Phoenix QAEvaluator for correctness assessment\n",
        "    - Phoenix HallucinationEvaluator for factual accuracy\n",
        "    - Phoenix ToxicityEvaluator for safety assessment\n",
        "    - No manual validation - Phoenix evaluators only\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: Optional[EvaluationConfig] = None):\n",
        "        \"\"\"Initialize the evaluator with configuration.\"\"\"\n",
        "        self.config = config or EvaluationConfig()\n",
        "        self._setup_logging()\n",
        "\n",
        "        # Initialize components\n",
        "        self.phoenix_manager = PhoenixManager(self.config)\n",
        "        self.dataset_manager = ArizeDatasetManager(self.config)\n",
        "\n",
        "        # Agent components\n",
        "        self.agent = None\n",
        "        self.span = None\n",
        "\n",
        "        # Phoenix evaluators\n",
        "        self.evaluators = {}\n",
        "        self.evaluator_llm = None\n",
        "\n",
        "        if ARIZE_AVAILABLE:\n",
        "            self._setup_phoenix_evaluators()\n",
        "\n",
        "    def _setup_logging(self) -> None:\n",
        "        \"\"\"Configure logging to suppress verbose modules.\"\"\"\n",
        "        verbose_modules = [\"openai\", \"httpx\", \"httpcore\", \"agentc_core\"]\n",
        "        for module in verbose_modules:\n",
        "            logging.getLogger(module).setLevel(logging.WARNING)\n",
        "\n",
        "    def _setup_phoenix_evaluators(self) -> None:\n",
        "        \"\"\"Setup Phoenix evaluators with robust error handling.\"\"\"\n",
        "        if not ARIZE_AVAILABLE:\n",
        "            logger.warning(\"⚠️ Phoenix dependencies not available - evaluations will be limited\")\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            self.evaluator_llm = OpenAIModel(model=self.config.evaluator_model)\n",
        "\n",
        "            # Initialize all Phoenix evaluators\n",
        "            self.evaluators = {\n",
        "                \"relevance\": RelevanceEvaluator(self.evaluator_llm),\n",
        "                \"qa_correctness\": QAEvaluator(self.evaluator_llm),\n",
        "                \"hallucination\": HallucinationEvaluator(self.evaluator_llm),\n",
        "                \"toxicity\": ToxicityEvaluator(self.evaluator_llm),\n",
        "            }\n",
        "\n",
        "            logger.info(\"✅ Phoenix evaluators initialized successfully\")\n",
        "            logger.info(f\"   🤖 Using evaluator model: {self.config.evaluator_model}\")\n",
        "            logger.info(f\"   📊 Available evaluators: {list(self.evaluators.keys())}\")\n",
        "\n",
        "            # Setup Phoenix if available\n",
        "            if self.phoenix_manager.start_phoenix():\n",
        "                self.phoenix_manager.setup_instrumentation()\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"⚠️ Phoenix evaluators setup failed: {e}\")\n",
        "            logger.info(\"Continuing with basic evaluation metrics only...\")\n",
        "            self.evaluators = {}\n",
        "\n",
        "    def setup_agent(self) -> bool:\n",
        "        \"\"\"Setup flight search agent using refactored main.py setup.\"\"\"\n",
        "        try:\n",
        "            logger.info(\"🔧 Setting up flight search agent...\")\n",
        "\n",
        "            # Use the refactored setup function from main.py\n",
        "            compiled_graph, application_span = setup_flight_search_agent()\n",
        "\n",
        "            self.agent = compiled_graph\n",
        "            self.span = application_span\n",
        "\n",
        "            logger.info(\"✅ Flight search agent setup completed successfully\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.exception(f\"❌ Error setting up flight search agent: {e}\")\n",
        "            return False\n",
        "\n",
        "    def _extract_response_content(self, result: Any) -> str:\n",
        "        \"\"\"Extract complete response content including tool results from agent result.\"\"\"\n",
        "        try:\n",
        "            response_parts = []\n",
        "\n",
        "            # Critical Fix: Extract tool outputs from search_results first\n",
        "            if isinstance(result, dict) and \"search_results\" in result:\n",
        "                search_results = result[\"search_results\"]\n",
        "                if search_results:\n",
        "                    # search_results contains the actual tool outputs we want\n",
        "                    response_parts.append(str(search_results))\n",
        "\n",
        "            # Also check for intermediate_steps (AgentExecutor format)\n",
        "            if isinstance(result, dict) and \"intermediate_steps\" in result:\n",
        "                for step in result[\"intermediate_steps\"]:\n",
        "                    if isinstance(step, tuple) and len(step) >= 2:\n",
        "                        # step[1] is the tool output/observation\n",
        "                        tool_output = str(step[1])\n",
        "                        if tool_output and tool_output.strip():\n",
        "                            response_parts.append(tool_output)\n",
        "\n",
        "            # Check for messages from LangGraph state (but filter out generic ones)\n",
        "            if hasattr(result, \"messages\") and result.messages:\n",
        "                for message in result.messages:\n",
        "                    if hasattr(message, \"content\") and message.content:\n",
        "                        content = str(message.content)\n",
        "                        # Skip generic system messages and human messages\n",
        "                        if (hasattr(message, \"type\") and message.type != \"human\" and\n",
        "                            not any(phrase in content.lower() for phrase in\n",
        "                                   [\"iteration limit\", \"time limit\", \"agent stopped\"])):\n",
        "                            response_parts.append(content)\n",
        "            elif isinstance(result, dict) and \"messages\" in result:\n",
        "                for message in result[\"messages\"]:\n",
        "                    if hasattr(message, \"content\") and message.content:\n",
        "                        content = str(message.content)\n",
        "                        # Skip generic system messages and human messages\n",
        "                        if (hasattr(message, \"__class__\") and \"Human\" not in message.__class__.__name__ and\n",
        "                            not any(phrase in content.lower() for phrase in\n",
        "                                   [\"iteration limit\", \"time limit\", \"agent stopped\"])):\n",
        "                            response_parts.append(content)\n",
        "\n",
        "            # If we have response parts, join them\n",
        "            if response_parts:\n",
        "                return \"\\n\\n\".join(response_parts)\n",
        "\n",
        "            # Fallback to full result conversion\n",
        "            result_str = str(result)\n",
        "\n",
        "            # If result is a dict, try to extract useful parts\n",
        "            if isinstance(result, dict):\n",
        "                useful_parts = []\n",
        "                for key in ['output', 'response', 'result', 'answer']:\n",
        "                    if key in result and result[key]:\n",
        "                        useful_parts.append(f\"{key.title()}: {result[key]}\")\n",
        "\n",
        "                if useful_parts:\n",
        "                    return \"\\n\".join(useful_parts)\n",
        "\n",
        "            return result_str\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"Error extracting response: {e}\"\n",
        "\n",
        "    def run_single_evaluation(self, query: str) -> Dict[str, Any]:\n",
        "        \"\"\"Run evaluation for a single query - no manual validation.\"\"\"\n",
        "        if not self.agent:\n",
        "            raise RuntimeError(\"Agent not initialized. Call setup_agent() first.\")\n",
        "\n",
        "        logger.info(f\"🔍 Evaluating query: {query}\")\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        try:\n",
        "            # Build starting state and run query\n",
        "            state = FlightSearchGraph.build_starting_state(query=query)\n",
        "            result = self.agent.invoke(state)\n",
        "\n",
        "            # Extract response content\n",
        "            response = self._extract_response_content(result)\n",
        "\n",
        "            # Create evaluation result - no manual scoring\n",
        "            evaluation_result = {\n",
        "                \"query\": query,\n",
        "                \"response\": response,\n",
        "                \"execution_time\": time.time() - start_time,\n",
        "                \"success\": True,\n",
        "            }\n",
        "\n",
        "            logger.info(f\"✅ Query completed in {evaluation_result['execution_time']:.2f}s\")\n",
        "            return evaluation_result\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.exception(f\"❌ Query failed: {e}\")\n",
        "            return {\n",
        "                \"query\": query,\n",
        "                \"response\": f\"Error: {str(e)}\",\n",
        "                \"execution_time\": time.time() - start_time,\n",
        "                \"success\": False,\n",
        "                \"error\": str(e),\n",
        "            }\n",
        "\n",
        "    def run_phoenix_evaluations(self, results_df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Run Phoenix evaluations on the results.\"\"\"\n",
        "        if not ARIZE_AVAILABLE or not self.evaluators:\n",
        "            logger.warning(\"⚠️ Phoenix evaluators not available - skipping evaluations\")\n",
        "            return results_df\n",
        "\n",
        "        logger.info(f\"🧠 Running Phoenix evaluations on {len(results_df)} responses...\")\n",
        "        logger.info(\"📋 Evaluation criteria:\")\n",
        "        logger.info(\"   🔍 Relevance: Does the response address the flight search query?\")\n",
        "        logger.info(\"   🎯 QA Correctness: Is the flight information accurate and helpful?\")\n",
        "        logger.info(\"   🚨 Hallucination: Does the response contain fabricated information?\")\n",
        "        logger.info(\"   ☠️ Toxicity: Is the response harmful or inappropriate?\")\n",
        "\n",
        "        try:\n",
        "            # Prepare evaluation data\n",
        "            evaluation_data = []\n",
        "            for _, row in results_df.iterrows():\n",
        "                query = row[\"query\"]\n",
        "                response = row[\"response\"]\n",
        "\n",
        "                # Create reference text based on query type\n",
        "                reference = QUERY_REFERENCE_ANSWERS.get(str(query), \"Reference answer not found\")\n",
        "\n",
        "                evaluation_data.append(\n",
        "                    {\n",
        "                        \"input\": query,\n",
        "                        \"output\": response,\n",
        "                        \"reference\": reference,\n",
        "                        \"query\": query,  # For hallucination evaluation\n",
        "                        \"response\": response,  # For hallucination evaluation\n",
        "                        \"text\": response,  # For toxicity evaluation\n",
        "                    }\n",
        "                )\n",
        "\n",
        "            eval_df = pd.DataFrame(evaluation_data)\n",
        "\n",
        "            # Run individual Phoenix evaluations\n",
        "            self._run_individual_phoenix_evaluations(eval_df, results_df)\n",
        "\n",
        "            logger.info(\"✅ Phoenix evaluations completed\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.exception(f\"❌ Error running Phoenix evaluations: {e}\")\n",
        "            # Add error indicators\n",
        "            for eval_type in [\"relevance\", \"qa_correctness\", \"hallucination\", \"toxicity\"]:\n",
        "                results_df[eval_type] = \"error\"\n",
        "                results_df[f\"{eval_type}_explanation\"] = f\"Error: {e}\"\n",
        "\n",
        "        return results_df\n",
        "\n",
        "    def _run_individual_phoenix_evaluations(\n",
        "        self, eval_df: pd.DataFrame, results_df: pd.DataFrame\n",
        "    ) -> None:\n",
        "        \"\"\"Run individual Phoenix evaluations.\"\"\"\n",
        "        for eval_name, evaluator in self.evaluators.items():\n",
        "            try:\n",
        "                logger.info(f\"   📊 Running {eval_name} evaluation...\")\n",
        "\n",
        "                # Prepare data based on evaluator requirements\n",
        "                if eval_name == \"relevance\":\n",
        "                    data = eval_df[[\"input\", \"reference\"]].copy()\n",
        "                    eval_results = llm_classify(\n",
        "                        data=data,\n",
        "                        model=self.evaluator_llm,\n",
        "                        template=RAG_RELEVANCY_PROMPT_TEMPLATE,\n",
        "                        rails=list(RAG_RELEVANCY_PROMPT_RAILS_MAP.values()),\n",
        "                        provide_explanation=True,\n",
        "                    )\n",
        "                elif eval_name == \"qa_correctness\":\n",
        "                    data = eval_df[[\"input\", \"output\", \"reference\"]].copy()\n",
        "                    eval_results = llm_classify(\n",
        "                        data=data,\n",
        "                        model=self.evaluator_llm,\n",
        "                        template=LENIENT_QA_PROMPT_TEMPLATE,\n",
        "                        rails=LENIENT_QA_RAILS,\n",
        "                        provide_explanation=True,\n",
        "                    )\n",
        "                elif eval_name == \"hallucination\":\n",
        "                    data = eval_df[[\"input\", \"reference\", \"output\"]].copy()\n",
        "                    eval_results = llm_classify(\n",
        "                        data=data,\n",
        "                        model=self.evaluator_llm,\n",
        "                        template=LENIENT_HALLUCINATION_PROMPT_TEMPLATE,\n",
        "                        rails=LENIENT_HALLUCINATION_RAILS,\n",
        "                        provide_explanation=True,\n",
        "                    )\n",
        "                elif eval_name == \"toxicity\":\n",
        "                    data = eval_df[[\"input\"]].copy()\n",
        "                    eval_results = llm_classify(\n",
        "                        data=data,\n",
        "                        model=self.evaluator_llm,\n",
        "                        template=TOXICITY_PROMPT_TEMPLATE,\n",
        "                        rails=list(TOXICITY_PROMPT_RAILS_MAP.values()),\n",
        "                        provide_explanation=True,\n",
        "                    )\n",
        "                else:\n",
        "                    logger.warning(f\"⚠️ Unknown evaluator: {eval_name}\")\n",
        "                    continue\n",
        "\n",
        "                # Process results\n",
        "                self._process_evaluation_results(eval_results, eval_name, results_df)\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"⚠️ {eval_name} evaluation failed: {e}\")\n",
        "                results_df[eval_name] = \"error\"\n",
        "                results_df[f\"{eval_name}_explanation\"] = f\"Error: {e}\"\n",
        "\n",
        "    def _process_evaluation_results(\n",
        "        self, eval_results: Any, eval_name: str, results_df: pd.DataFrame\n",
        "    ) -> None:\n",
        "        \"\"\"Process evaluation results and add to results DataFrame.\"\"\"\n",
        "        try:\n",
        "            if eval_results is None:\n",
        "                logger.warning(f\"⚠️ {eval_name} evaluation returned None\")\n",
        "                results_df[eval_name] = \"unknown\"\n",
        "                results_df[f\"{eval_name}_explanation\"] = \"Evaluation returned None\"\n",
        "                return\n",
        "\n",
        "            # Handle DataFrame results\n",
        "            if hasattr(eval_results, \"columns\"):\n",
        "                if \"label\" in eval_results.columns:\n",
        "                    results_df[eval_name] = eval_results[\"label\"].tolist()\n",
        "                elif \"classification\" in eval_results.columns:\n",
        "                    results_df[eval_name] = eval_results[\"classification\"].tolist()\n",
        "                else:\n",
        "                    results_df[eval_name] = \"unknown\"\n",
        "\n",
        "                if \"explanation\" in eval_results.columns:\n",
        "                    results_df[f\"{eval_name}_explanation\"] = eval_results[\"explanation\"].tolist()\n",
        "                elif \"reason\" in eval_results.columns:\n",
        "                    results_df[f\"{eval_name}_explanation\"] = eval_results[\"reason\"].tolist()\n",
        "                else:\n",
        "                    results_df[f\"{eval_name}_explanation\"] = \"No explanation provided\"\n",
        "\n",
        "                logger.info(f\"   ✅ {eval_name} evaluation completed\")\n",
        "\n",
        "            # Handle list results\n",
        "            elif isinstance(eval_results, list) and len(eval_results) > 0:\n",
        "                if isinstance(eval_results[0], dict):\n",
        "                    results_df[eval_name] = [item.get(\"label\", \"unknown\") for item in eval_results]\n",
        "                    results_df[f\"{eval_name}_explanation\"] = [\n",
        "                        item.get(\"explanation\", \"No explanation\") for item in eval_results\n",
        "                    ]\n",
        "                else:\n",
        "                    results_df[eval_name] = eval_results\n",
        "                    results_df[f\"{eval_name}_explanation\"] = \"List evaluation result\"\n",
        "\n",
        "                logger.info(f\"   ✅ {eval_name} evaluation completed (list format)\")\n",
        "\n",
        "            else:\n",
        "                logger.warning(f\"⚠️ {eval_name} evaluation returned unexpected format\")\n",
        "                results_df[eval_name] = \"unknown\"\n",
        "                results_df[f\"{eval_name}_explanation\"] = f\"Unexpected format: {type(eval_results)}\"\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"⚠️ Error processing {eval_name} results: {e}\")\n",
        "            results_df[eval_name] = \"error\"\n",
        "            results_df[f\"{eval_name}_explanation\"] = f\"Processing error: {e}\"\n",
        "\n",
        "    def run_evaluation(self, queries: List[str]) -> pd.DataFrame:\n",
        "        \"\"\"Run complete evaluation pipeline using only Phoenix evaluators.\"\"\"\n",
        "        # Clear existing bookings for a clean test run\n",
        "        clear_bookings_and_reviews()\n",
        "\n",
        "        if not self.setup_agent():\n",
        "            raise RuntimeError(\"Failed to setup agent\")\n",
        "\n",
        "        logger.info(f\"🚀 Starting evaluation with {len(queries)} queries\")\n",
        "\n",
        "        # Log available features\n",
        "        logger.info(\"📋 Evaluation Configuration:\")\n",
        "        logger.info(f\"   🤖 Agent: Flight Search Agent (LangGraph)\")\n",
        "        logger.info(f\"   🔧 Phoenix Available: {'✅' if ARIZE_AVAILABLE else '❌'}\")\n",
        "        logger.info(f\"   📊 Arize Datasets: {'✅' if ARIZE_AVAILABLE and (self.dataset_manager.client is not None) else '❌'}\")\n",
        "        if self.evaluators:\n",
        "            logger.info(f\"   🧠 Phoenix Evaluators: {list(self.evaluators.keys())}\")\n",
        "        else:\n",
        "            logger.info(\"   🧠 Phoenix Evaluators: ❌ (basic metrics only)\")\n",
        "\n",
        "        # Run queries (no manual validation)\n",
        "        results = []\n",
        "        for i, query in enumerate(queries, 1):\n",
        "            logger.info(f\"\\n📋 Query {i}/{len(queries)}\")\n",
        "            result = self.run_single_evaluation(query)\n",
        "            results.append(result)\n",
        "\n",
        "        # Create results DataFrame\n",
        "        results_df = pd.DataFrame(results)\n",
        "\n",
        "        # Run Phoenix evaluations only\n",
        "        results_df = self.run_phoenix_evaluations(results_df)\n",
        "\n",
        "        # Log summary\n",
        "        self._log_evaluation_summary(results_df)\n",
        "\n",
        "        # Create Arize dataset\n",
        "        dataset_id = self.dataset_manager.create_dataset(results_df)\n",
        "        if dataset_id:\n",
        "            logger.info(f\"📊 Arize dataset created: {dataset_id}\")\n",
        "        else:\n",
        "            logger.warning(\"⚠️ Dataset creation failed\")\n",
        "\n",
        "        return results_df\n",
        "\n",
        "    def _log_evaluation_summary(self, results_df: pd.DataFrame) -> None:\n",
        "        \"\"\"Log evaluation summary using Phoenix results only.\"\"\"\n",
        "        logger.info(\"\\n📊 Phoenix Evaluation Summary:\")\n",
        "        logger.info(f\"  Total queries: {len(results_df)}\")\n",
        "        logger.info(f\"  Successful executions: {results_df['success'].sum()}\")\n",
        "        logger.info(f\"  Failed executions: {(~results_df['success']).sum()}\")\n",
        "        logger.info(f\"  Average execution time: {results_df['execution_time'].mean():.2f}s\")\n",
        "\n",
        "        # Phoenix evaluation results\n",
        "        if ARIZE_AVAILABLE and self.evaluators:\n",
        "            logger.info(\"\\n🧠 Phoenix Evaluation Results:\")\n",
        "            for eval_type in [\"relevance\", \"qa_correctness\", \"hallucination\", \"toxicity\"]:\n",
        "                if eval_type in results_df.columns:\n",
        "                    counts = results_df[eval_type].value_counts()\n",
        "                    logger.info(f\"   {eval_type}: {dict(counts)}\")\n",
        "\n",
        "        # Quick scores summary\n",
        "        if len(results_df) > 0:\n",
        "            logger.info(\"\\n📊 Quick Scores Summary:\")\n",
        "            for i in range(len(results_df)):\n",
        "                row = results_df.iloc[i]\n",
        "                scores = []\n",
        "                for eval_type in [\"relevance\", \"qa_correctness\", \"hallucination\", \"toxicity\"]:\n",
        "                    if eval_type in row:\n",
        "                        result = row[eval_type]\n",
        "                        emoji = \"✅\" if result in [\"relevant\", \"correct\", \"factual\", \"non-toxic\"] else \"❌\"\n",
        "                        scores.append(f\"{emoji} {eval_type}: {result}\")\n",
        "\n",
        "            logger.info(f\"   Query {i+1}: {' | '.join(scores)}\")\n",
        "\n",
        "    def cleanup(self) -> None:\n",
        "        \"\"\"Clean up all resources.\"\"\"\n",
        "        self.phoenix_manager.cleanup()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZInHfhu5mTC9",
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Evaluator Methods - Part 2\n",
        "\n",
        "Additional methods for the ArizeFlightSearchEvaluator class.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKb3jE_OmTC-",
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Utility Functions\n",
        "\n",
        "Utility functions for running evaluations and demos.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06H-zcs6mTC-"
      },
      "outputs": [],
      "source": [
        "def get_default_queries() -> List[str]:\n",
        "    \"\"\"Get default test queries for evaluation.\"\"\"\n",
        "    return [\n",
        "        \"Find flights from JFK to LAX\",\n",
        "        \"What do passengers say about SpiceJet's service quality?\",\n",
        "        \"Book a flight from NYC to San Francisco\",\n",
        "        \"Retrieve my flight bookings\",\n",
        "        \"Search for reviews about Air India delays\"\n",
        "    ]\n",
        "\n",
        "def run_phoenix_demo() -> pd.DataFrame:\n",
        "    \"\"\"Run a simple Phoenix evaluation demo.\"\"\"\n",
        "    logger.info(\"🔧 Running Phoenix evaluation demo...\")\n",
        "\n",
        "    demo_queries = [\n",
        "        \"Find flights from JFK to LAX\",\n",
        "        \"What do passengers say about SpiceJet's service quality?\",\n",
        "    ]\n",
        "\n",
        "    evaluator = ArizeFlightSearchEvaluator()\n",
        "    try:\n",
        "        results = evaluator.run_evaluation(demo_queries)\n",
        "        logger.info(\"🎉 Phoenix evaluation demo complete!\")\n",
        "        logger.info(\"💡 Visit Phoenix UI to see detailed traces and evaluations\")\n",
        "        return results\n",
        "    finally:\n",
        "        evaluator.cleanup()\n",
        "\n",
        "def run_full_evaluation() -> pd.DataFrame:\n",
        "    \"\"\"Main evaluation function using only Phoenix evaluators.\"\"\"\n",
        "    evaluator = ArizeFlightSearchEvaluator()\n",
        "    try:\n",
        "        results = evaluator.run_evaluation(get_default_queries())\n",
        "        logger.info(\"\\n✅ Phoenix evaluation complete!\")\n",
        "        return results\n",
        "    finally:\n",
        "        evaluator.cleanup()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCLJZKGAmTC-",
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Run Evaluation\n",
        "\n",
        "Execute the flight search agent evaluation with Phoenix AI.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4YhW5LxmTC-"
      },
      "outputs": [],
      "source": [
        "# Run the evaluation\n",
        "if __name__ == \"__main__\":\n",
        "    # Choose evaluation mode:\n",
        "\n",
        "    # Option 1: Run demo with 2 queries (quick test)\n",
        "    # results = run_phoenix_demo()\n",
        "\n",
        "    # Option 2: Run full evaluation with all queries\n",
        "    results = run_full_evaluation()\n",
        "\n",
        "    # Display results\n",
        "    if results is not None and len(results) > 0:\n",
        "        print(\"\\n📊 EVALUATION RESULTS:\")\n",
        "        print(\"=\" * 50)\n",
        "        for i, row in results.iterrows():\n",
        "            print(f\"\\n🔍 Query {i+1}: {row['query']}\")\n",
        "            print(f\"⏱️ Execution time: {row['execution_time']:.2f}s\")\n",
        "            print(f\"✅ Success: {row['success']}\")\n",
        "\n",
        "            # Show evaluation scores if available\n",
        "            eval_types = [\"relevance\", \"qa_correctness\", \"hallucination\", \"toxicity\"]\n",
        "            for eval_type in eval_types:\n",
        "                if eval_type in row:\n",
        "                    score = row[eval_type]\n",
        "                    emoji = \"✅\" if score in [\"relevant\", \"correct\", \"factual\", \"non-toxic\"] else \"❌\"\n",
        "                    print(f\"   {emoji} {eval_type.title()}: {score}\")\n",
        "\n",
        "        print(\"\\n🎉 Evaluation completed! Check Phoenix UI for detailed traces.\")\n",
        "    else:\n",
        "        print(\"❌ No evaluation results generated.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aLNxJYomTC-",
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Summary\n",
        "\n",
        "This self-contained notebook demonstrates a complete flight search agent implementation using LangGraph framework with ReAct agents, Couchbase vector store for airline reviews, and Priority 1 Capella AI services. The agent handles flight search, booking, retrieval, and airline review queries with exact functionality from the working Python files. All components are inline and the notebook runs as a complete workflow when executed cell by cell. Set up environment variables (CB_*, CAPELLA_API_*) and dependencies, then execute all cells sequentially.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
