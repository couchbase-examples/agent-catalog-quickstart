#!/usr/bin/env python3
"""
Route Planner Agent - Tutorial with AgentC Integration

This tutorial demonstrates how to build an AI agent using:
- Agent Catalog (agentc) for tool discovery, prompt management, and span logging
- Couchbase Capella as vector store
- Capella AI Services for embeddings and LLM
- LlamaIndex for RAG pipeline and agent execution

The agent uses two tools discovered via AgentC:
1. search_routes - Find routes using vector search
2. calculate_distance - Calculate distances between cities
"""

import json
import logging
import os
import sys
import base64
import time
from datetime import timedelta
from typing import List

import agentc
from agentc_llamaindex.chat import Callback
from couchbase.auth import PasswordAuthenticator
from couchbase.cluster import Cluster
from couchbase.management.buckets import CreateBucketSettings
from couchbase.management.search import SearchIndex
from couchbase.options import ClusterOptions
from couchbase.exceptions import CouchbaseException

from llama_index.core import Settings, Document, VectorStoreIndex
from llama_index.core.ingestion import IngestionPipeline
from llama_index.core.node_parser import SentenceSplitter
from llama_index.core.agent import ReActAgent
from llama_index.core.tools import FunctionTool
from llama_index.vector_stores.couchbase import CouchbaseSearchVectorStore
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.llms.openai_like import OpenAILike

# Setup logging
logging.basicConfig(
    level=logging.INFO, 
    format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


def _set_if_undefined(var: str):
    """Helper function to prompt for missing environment variables."""
    if os.environ.get(var) is None:
        import getpass
        os.environ[var] = getpass.getpass(f"Please provide your {var}: ")


def setup_environment():
    """Setup environment variables with defaults and validation."""
    required_vars = ['CB_CONN_STRING', 'CB_USERNAME', 'CB_PASSWORD', 'CB_BUCKET']
    for var in required_vars:
        _set_if_undefined(var)
    
    # Optional Capella AI variables
    optional_vars = ['CAPELLA_API_ENDPOINT', 'CAPELLA_API_EMBEDDING_MODEL', 'CAPELLA_API_LLM_MODEL']
    for var in optional_vars:
        if not os.environ.get(var):
            print(f"‚ÑπÔ∏è {var} not provided - will use OpenAI fallback")
    
    # Set defaults
    defaults = {
        'CB_CONN_STRING': 'couchbase://localhost',
        'CB_USERNAME': 'Administrator',
        'CB_PASSWORD': 'password',
        'CB_BUCKET': 'route_planner',
        'CB_INDEX': 'route_data_index',
        'CB_SCOPE': 'agentc_data',
        'CB_COLLECTION': 'route_data',
        'CAPELLA_API_EMBEDDING_MODEL': 'intfloat/e5-mistral-7b-instruct',
        'CAPELLA_API_LLM_MODEL': 'meta-llama/Llama-3.1-8B-Instruct'
    }
    
    for key, default_value in defaults.items():
        if not os.environ.get(key):
            os.environ[key] = default_value
    
    # Generate Capella AI API key if endpoint is provided
    if os.environ.get('CAPELLA_API_ENDPOINT'):
        os.environ['CAPELLA_API_KEY'] = base64.b64encode(
            f"{os.environ['CB_USERNAME']}:{os.environ['CB_PASSWORD']}".encode("utf-8")
        ).decode("utf-8")
        
        # Use endpoint as provided
        print(f"Using Capella AI endpoint: {os.environ['CAPELLA_API_ENDPOINT']}")


def setup_couchbase_connection():
    """Setup Couchbase cluster connection."""
    try:
        auth = PasswordAuthenticator(os.environ['CB_USERNAME'], os.environ['CB_PASSWORD'])
        options = ClusterOptions(auth)
        
        # Use WAN profile for better timeout handling with remote clusters
        options.apply_profile("wan_development")
        
        # Additional timeout configurations for Capella cloud connections
        from couchbase.options import ClusterTimeoutOptions
        
        # Configure extended timeouts for cloud connectivity
        timeout_options = ClusterTimeoutOptions(
            kv_timeout=timedelta(seconds=10),  # Key-value operations
            kv_durable_timeout=timedelta(seconds=15),  # Durable writes
            query_timeout=timedelta(seconds=30),  # N1QL queries
            search_timeout=timedelta(seconds=30),  # Search operations
            management_timeout=timedelta(seconds=30),  # Management operations
            bootstrap_timeout=timedelta(seconds=20),  # Initial connection
        )
        options.timeout_options = timeout_options
        
        cluster = Cluster(os.environ['CB_CONN_STRING'], options)
        cluster.wait_until_ready(timedelta(seconds=20))  # Increased for cloud
        print("‚úÖ Successfully connected to Couchbase")
        return cluster
    except Exception as e:
        raise ConnectionError(f"‚ùå Failed to connect to Couchbase: {str(e)}")


def setup_collection(cluster, bucket_name, scope_name, collection_name):
    """Setup bucket, scope, and collection."""
    try:
        # Create bucket if needed
        try:
            bucket = cluster.bucket(bucket_name)
            print(f"‚úÖ Bucket '{bucket_name}' exists")
        except Exception:
            print(f"üì¶ Creating bucket '{bucket_name}'...")
            bucket_settings = CreateBucketSettings(
                name=bucket_name,
                bucket_type='couchbase',
                ram_quota_mb=1024,
                flush_enabled=True,
                num_replicas=0
            )
            cluster.buckets().create_bucket(bucket_settings)
            time.sleep(5)
            bucket = cluster.bucket(bucket_name)
            print(f"‚úÖ Bucket '{bucket_name}' created successfully")

        # Setup scope and collection
        bucket_manager = bucket.collections()
        scopes = bucket_manager.get_all_scopes()
        scope_exists = any(scope.name == scope_name for scope in scopes)
        
        if not scope_exists and scope_name != "_default":
            print(f"üìÅ Creating scope '{scope_name}'...")
            bucket_manager.create_scope(scope_name)
            print(f"‚úÖ Scope '{scope_name}' created successfully")

        collections = bucket_manager.get_all_scopes()
        collection_exists = any(
            scope.name == scope_name and collection_name in [col.name for col in scope.collections]
            for scope in collections
        )

        if not collection_exists:
            print(f"üìÑ Creating collection '{collection_name}'...")
            bucket_manager.create_collection(scope_name, collection_name)
            print(f"‚úÖ Collection '{collection_name}' created successfully")

        # Create primary index
        collection = bucket.scope(scope_name).collection(collection_name)
        time.sleep(3)

        try:
            cluster.query(f"CREATE PRIMARY INDEX IF NOT EXISTS ON `{bucket_name}`.`{scope_name}`.`{collection_name}`").execute()
            print("‚úÖ Primary index created successfully")
        except Exception as e:
            print(f"‚ö†Ô∏è Warning: Error creating primary index: {str(e)}")

        return collection
    except Exception as e:
        raise RuntimeError(f"‚ùå Error setting up collection: {str(e)}")


def setup_vector_search_index(cluster, index_definition):
    """Setup vector search index."""
    try:
        scope_index_manager = cluster.bucket(os.environ['CB_BUCKET']).scope(os.environ['CB_SCOPE']).search_indexes()
        
        existing_indexes = scope_index_manager.get_all_indexes()
        index_name = index_definition["name"]

        if index_name not in [index.name for index in existing_indexes]:
            print(f"üîç Creating vector search index '{index_name}'...")
            search_index = SearchIndex.from_json(index_definition)
            scope_index_manager.upsert_index(search_index)
            print(f"‚úÖ Vector search index '{index_name}' created successfully")
        else:
            print(f"‚úÖ Vector search index '{index_name}' already exists")
    except Exception as e:
        raise RuntimeError(f"‚ùå Error setting up vector search index: {str(e)}")


def setup_ai_models(span):
    """Setup embeddings and LLM using Capella AI Services or OpenAI."""
    try:
        if os.environ.get('CAPELLA_API_ENDPOINT') and os.environ.get('CAPELLA_API_KEY'):
            # Use Capella AI Services
            print("üîß Setting up Capella AI Services...")
            
            embed_model = OpenAIEmbedding(
                api_key=os.environ['CAPELLA_API_KEY'],
                api_base=os.environ['CAPELLA_API_ENDPOINT'] + '/v1',
                model_name=os.environ['CAPELLA_API_EMBEDDING_MODEL'],
                embed_batch_size=30
            )
            
            llm = OpenAILike(
                api_base=os.environ['CAPELLA_API_ENDPOINT'] + '/v1',
                api_key=os.environ['CAPELLA_API_KEY'],
                model=os.environ['CAPELLA_API_LLM_MODEL'],
                temperature=0,
                # Add AgentC callback for LlamaIndex
                callbacks=[Callback(span=span)]
            )
            
            print("‚úÖ Using Capella AI Services for embeddings and LLM")
        else:
            # Fallback to OpenAI
            print("üîß Setting up OpenAI (Capella AI not configured)...")
            _set_if_undefined("OPENAI_API_KEY")
            
            embed_model = OpenAIEmbedding(
                api_key=os.environ["OPENAI_API_KEY"],
                model="text-embedding-3-small"
            )
            
            from llama_index.llms.openai import OpenAI
            llm = OpenAI(
                api_key=os.environ["OPENAI_API_KEY"],
                model="gpt-4o",
                temperature=0,
                callbacks=[Callback(span=span)]
            )
            
            print("‚úÖ Using OpenAI for embeddings and LLM")
        
        # Configure LlamaIndex global settings
        Settings.embed_model = embed_model
        Settings.llm = llm
        
        return embed_model, llm
        
    except Exception as e:
        raise RuntimeError(f"‚ùå Failed to setup AI models: {str(e)}")


def get_sample_route_data():
    """Get sample route data for the demo."""
    return [
        "New York to Boston: A scenic coastal route through Connecticut and Rhode Island. Take I-95 for the fastest route (4.5 hours) or Route 1 for scenic coastal views. Must-see stops include Mystic Seaport and Newport mansions.",
        "San Francisco to Los Angeles: The iconic Pacific Coast Highway (PCH) offers breathtaking ocean views. The 380-mile journey takes 6-8 hours of driving time. Don't miss Big Sur, Monterey Bay, and Santa Barbara.",
        "Chicago to Milwaukee: A short 90-mile drive north through Wisconsin farmland. Takes about 1.5 hours via I-94. Alternative scenic route through Lake Geneva adds beautiful lake views.",
        "Denver to Aspen: A mountain route through the Rockies covering 160 miles. Take I-70 west then Highway 82 south. Allow 3-4 hours for mountain driving. Spectacular views of the Continental Divide.",
        "Seattle to Portland: The I-5 corridor through the Pacific Northwest. 173 miles taking about 3 hours. Consider stopping at Mount St. Helens or taking the scenic Route 14 along the Columbia River.",
        "Miami to Key West: The famous Overseas Highway (US-1) crosses 42 bridges over the Florida Keys. 160 miles of tropical paradise taking 3.5 hours. Amazing views of the Atlantic and Gulf waters.",
        "Las Vegas to Grand Canyon: Two main routes - the South Rim (4.5 hours) or North Rim (5 hours). South Rim is open year-round. Take Highway 64 for the most direct route to the main visitor areas.",
        "Nashville to Memphis: The musical highway through Tennessee. 210 miles on I-40 west taking about 3 hours. Consider stopping in Jackson for BBQ and music history.",
        "Phoenix to Sedona: A desert-to-red-rock transition covering 120 miles. Take I-17 north then Highway 179 for the most scenic approach. About 2 hours of driving with stunning desert landscapes.",
        "Austin to San Antonio: A quick 80-mile drive through Texas Hill Country. Take I-35 south for the fastest route (1.5 hours) or Highway 46 for scenic hill country views and wine country."
    ]


def ingest_route_data(vector_store, span, cluster):
    """Ingest route data using the dedicated data loading script."""
    try:
        with span.new("Route Data Loading"):
            print("üìä Loading route data using data loading script...")
            
            # Use the dedicated data loading script
            from data.route_data import load_route_data_to_couchbase
            load_route_data_to_couchbase(
                cluster=cluster,
                            bucket_name=os.environ['CB_BUCKET'],
            scope_name=os.environ['CB_SCOPE'],
            collection_name=os.environ['CB_COLLECTION'],
            embeddings=Settings.embed_model,
            index_name=os.environ['CB_INDEX']
            )
            
            print("‚úÖ Route data loaded successfully using data loading script")
            
    except Exception as e:
        print(f"‚ùå Error loading route data with script: {str(e)}")
        # Fallback to the original method
        print("‚ö†Ô∏è Falling back to direct ingestion method...")
        try:
            with span.new("Route Data Loading - Fallback"):
                route_texts = get_sample_route_data()
                
                # Create LlamaIndex documents
                documents = []
                for i, text in enumerate(route_texts):
                    doc = Document(
                        text=text,
                        metadata={
                            "source": "route_database",
                            "route_id": i,
                            "type": "route_description"
                        }
                    )
                    documents.append(doc)
                
                # Create ingestion pipeline
                pipeline = IngestionPipeline(
                    transformations=[
                        SentenceSplitter(chunk_size=512, chunk_overlap=50),
                        Settings.embed_model
                    ],
                    vector_store=vector_store
                )
                
                # Ingest documents
                print(f"‚ö° Ingesting {len(documents)} route documents...")
                pipeline.run(documents=documents)
                print("‚úÖ Route data ingestion complete using fallback")
                
        except Exception as fallback_error:
            print(f"‚ùå Error with fallback method: {str(fallback_error)}")
            print("‚ö†Ô∏è Continuing without pre-loaded route data...")


def create_llamaindex_agent(catalog, span):
    """Create LlamaIndex ReActAgent using tools and prompts from AgentC."""
    try:
        with span.new("Tool Discovery"):
            # Find tools from Agent Catalog
            tool_search = catalog.find("tool", name="search_routes")
            tool_distance = catalog.find("tool", name="calculate_distance")
            
            if not tool_search:
                raise ValueError("Could not find search_routes tool. Make sure it's indexed with 'agentc index tools/'")
            if not tool_distance:
                raise ValueError("Could not find calculate_distance tool. Make sure it's indexed with 'agentc index tools/'")
            
            # Create LlamaIndex FunctionTool objects
            tools = [
                FunctionTool.from_defaults(
                    fn=tool_search.func,
                    name=tool_search.meta.name,
                    description=tool_search.meta.description
                ),
                FunctionTool.from_defaults(
                    fn=tool_distance.func,
                    name=tool_distance.meta.name,
                    description=tool_distance.meta.description
                )
            ]
            
            print(f"‚úÖ Found {len(tools)} tools: {[tool.metadata.name for tool in tools]}")
        
        with span.new("Prompt Discovery"):
            # Get prompt from Agent Catalog
            route_prompt = catalog.find("prompt", name="route_planner_assistant")
            if not route_prompt:
                raise ValueError("Could not find route_planner_assistant prompt in catalog. Make sure it's indexed with 'agentc index prompts/'")
            
            print(f"‚úÖ Found prompt: {route_prompt.meta.name}")
            
            # Use the prompt content as system message
            system_prompt = route_prompt.content.strip()
        
        with span.new("Agent Creation"):
            # Create ReAct agent with tools and system prompt
            agent = ReActAgent.from_tools(
                tools=tools,
                llm=Settings.llm,
                system_prompt=system_prompt,
                verbose=True,
                max_iterations=15
            )
            
            print("‚úÖ LlamaIndex ReActAgent created successfully")
            return agent
        
    except Exception as e:
        raise RuntimeError(f"‚ùå Error creating LlamaIndex agent: {str(e)}")


def run_agent_demo(agent, span):
    """Run the agent demo with sample queries."""
    try:
        # Demo queries
        demo_queries = [
            "Find routes from New York to Boston",
            "Calculate distance between San Francisco and Los Angeles by car",
            "What are some scenic routes in California?",
            "Calculate the distance from Denver to Aspen by train"
        ]
        
        print(f"\nüó∫Ô∏è Route Planner Agent Demo")
        print("=" * 60)
        
        for i, query in enumerate(demo_queries, 1):
            with span.new(f"Query_{i}") as query_span:
                print(f"\nüîç Query {i}: {query}")
                print("-" * 40)
                
                try:
                    # Use LlamaIndex agent
                    response = agent.chat(query)
                    query_span["query"] = query
                    query_span["response"] = str(response)
                    print(f"üìù Response: {response}")
                    print("-" * 40)
                except Exception as e:
                    query_span["query"] = query
                    query_span["error"] = str(e)
                    print(f"‚ùå Error: {e}")
                    print("-" * 40)
        
    except Exception as e:
        raise RuntimeError(f"‚ùå Error in agent demo: {str(e)}")


def main():
    """Main function to run the Route Planner Agent tutorial."""
    try:
        # Load environment variables
        import dotenv
        dotenv.load_dotenv(override=True)
        
        # Initialize Agent Catalog and main span
        catalog = agentc.Catalog()
        application_span = catalog.Span(name="Route Planner Agent Tutorial")
        
        print("üöÄ Route Planner Agent Tutorial")
        print("=" * 60)
        print("This tutorial demonstrates:")
        print("‚Ä¢ AgentC for tool discovery, prompt management, and span logging")
        print("‚Ä¢ Couchbase Capella as vector store")
        print("‚Ä¢ Capella AI Services for embeddings and LLM")
        print("‚Ä¢ LlamaIndex ReActAgent for agent execution")
        print("=" * 60)
        
        with application_span.new("Environment Setup"):
            setup_environment()
        
        with application_span.new("Couchbase Connection"):
            cluster = setup_couchbase_connection()
        
        with application_span.new("Collection Setup"):
            setup_collection(
                cluster,
                os.environ['CB_BUCKET'],
                os.environ['CB_SCOPE'],
                os.environ['CB_COLLECTION']
            )
        
        with application_span.new("Vector Index Setup"):
            try:
                with open('agentcatalog_index.json', 'r') as file:
                    index_definition = json.load(file)
                print("‚úÖ Loaded vector search index definition from agentcatalog_index.json")
            except Exception as e:
                print(f"‚ö†Ô∏è Could not load index definition: {str(e)}")
                # Continue without index for demo purposes
                index_definition = None
            
            if index_definition:
                setup_vector_search_index(cluster, index_definition)
        
        with application_span.new("AI Models Setup"):
            embed_model, llm = setup_ai_models(application_span)
        
        with application_span.new("Vector Store Setup"):
            vector_store = CouchbaseSearchVectorStore(
                cluster=cluster,
                bucket_name=os.environ['CB_BUCKET'],
                scope_name=os.environ['CB_SCOPE'],
                collection_name=os.environ['CB_COLLECTION'],
                index_name=os.environ['CB_INDEX']
            )
            
            # Ingest route data
            ingest_route_data(vector_store, application_span, cluster)
        
        with application_span.new("Agent Setup"):
            agent = create_llamaindex_agent(catalog, application_span)
        
        with application_span.new("Demo Execution"):
            run_agent_demo(agent, application_span)
        
        print("\n‚úÖ Tutorial completed successfully!")
        print("Next steps:")
        print("‚Ä¢ Check AgentC span logs for detailed execution traces")
        print("‚Ä¢ Try asking your own route questions")
        print("‚Ä¢ Modify the tools to add new capabilities")
        print("‚Ä¢ Explore different LlamaIndex agent configurations")
        
    except Exception as e:
        logger.error(f"‚ùå Tutorial failed: {str(e)}")
        sys.exit(1)


if __name__ == "__main__":
    main()
