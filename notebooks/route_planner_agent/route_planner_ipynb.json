{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3f1a9ce",
   "metadata": {},
   "source": [
    "# Route Planner Agent Tutorial\n",
    "\n",
    "This notebook demonstrates the Agent Catalog route planner agent that helps users find travel routes and calculate distances using:\n",
    "- Agent Catalog (agentc) for tool discovery, prompt management, and span logging\n",
    "- Couchbase Capella as vector store\n",
    "- Capella AI Services for embeddings and LLM (with OpenAI fallback)\n",
    "- LlamaIndex for RAG pipeline and agent execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup_imports",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "Import all necessary modules for the route planner agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96071fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "import base64\n",
    "import getpass\n",
    "from datetime import timedelta\n",
    "\n",
    "import agentc\n",
    "from agentc_llamaindex.chat import Callback\n",
    "import dotenv\n",
    "from couchbase.auth import PasswordAuthenticator\n",
    "from couchbase.cluster import Cluster\n",
    "from couchbase.management.buckets import CreateBucketSettings\n",
    "from couchbase.management.search import SearchIndex\n",
    "from couchbase.options import ClusterOptions\n",
    "from couchbase.exceptions import CouchbaseException\n",
    "\n",
    "from llama_index.core import Settings, Document, VectorStoreIndex\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.agent import ReActAgent\n",
    "from llama_index.core.tools import FunctionTool\n",
    "from llama_index.vector_stores.couchbase import CouchbaseSearchVectorStore\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.llms.openai_like import OpenAILike\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load environment variables\n",
    "dotenv.load_dotenv(override=True)\n",
    "\n",
    "print(\"✅ Setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "env_setup",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Setup required environment variables with Capella cloud defaults and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "env_setup_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _set_if_undefined(var: str):\n",
    "    \"\"\"Helper function to prompt for missing environment variables.\"\"\"\n",
    "    if os.environ.get(var) is None:\n",
    "        os.environ[var] = getpass.getpass(f\"Please provide your {var}: \")\n",
    "\n",
    "def setup_environment():\n",
    "    \"\"\"Setup environment variables with defaults and validation.\"\"\"\n",
    "    required_vars = ['CB_CONN_STRING', 'CB_USERNAME', 'CB_PASSWORD', 'CB_BUCKET']\n",
    "    for var in required_vars:\n",
    "        _set_if_undefined(var)\n",
    "    \n",
    "    # Optional Capella AI variables\n",
    "    optional_vars = ['CAPELLA_API_ENDPOINT', 'CAPELLA_API_EMBEDDING_MODEL', 'CAPELLA_API_LLM_MODEL']\n",
    "    for var in optional_vars:\n",
    "        if not os.environ.get(var):\n",
    "            print(f\"ℹ️ {var} not provided - will use OpenAI fallback\")\n",
    "    \n",
    "    # Set defaults\n",
    "    defaults = {\n",
    "        'CB_CONN_STRING': 'couchbases://cb.hlcup4o4jmjr55yf.cloud.couchbase.com',\n",
    "        'CB_USERNAME': 'kaustavcluster',\n",
    "        'CB_PASSWORD': 'Password@123',\n",
    "        'CB_BUCKET': 'vector-search-testing',\n",
    "        'CB_INDEX': 'route_data_index',\n",
    "        'CB_SCOPE': 'agentc_data',\n",
    "        'CB_COLLECTION': 'route_data',\n",
    "        'CAPELLA_API_EMBEDDING_MODEL': 'intfloat/e5-mistral-7b-instruct',\n",
    "        'CAPELLA_API_LLM_MODEL': 'meta-llama/Llama-3.1-8B-Instruct'\n",
    "    }\n",
    "    \n",
    "    for key, default_value in defaults.items():\n",
    "        if not os.environ.get(key):\n",
    "            os.environ[key] = default_value\n",
    "    \n",
    "    # Generate Capella AI API key if endpoint is provided\n",
    "    if os.environ.get('CAPELLA_API_ENDPOINT'):\n",
    "        os.environ['CAPELLA_API_KEY'] = base64.b64encode(\n",
    "            f\"{os.environ['CB_USERNAME']}:{os.environ['CB_PASSWORD']}\".encode(\"utf-8\")\n",
    "        ).decode(\"utf-8\")\n",
    "        \n",
    "        # Use endpoint as provided\n",
    "        print(f\"Using Capella AI endpoint: {os.environ['CAPELLA_API_ENDPOINT']}\")\n",
    "\n",
    "setup_environment()\n",
    "print(\"✅ Environment configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "couchbase_client_class",
   "metadata": {},
   "source": [
    "## CouchbaseClient Class\n",
    "\n",
    "Define the centralized CouchbaseClient class for all database operations and AI model setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "couchbase_client_def",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CouchbaseClient:\n",
    "    \"\"\"Centralized Couchbase client for all database operations.\"\"\"\n",
    "\n",
    "    def __init__(self, conn_string: str, username: str, password: str, bucket_name: str):\n",
    "        \"\"\"Initialize Couchbase client with connection details.\"\"\"\n",
    "        self.conn_string = conn_string\n",
    "        self.username = username\n",
    "        self.password = password\n",
    "        self.bucket_name = bucket_name\n",
    "        self.cluster = None\n",
    "        self.bucket = None\n",
    "        self._collections = {}\n",
    "        self.embed_model = None\n",
    "        self.llm = None\n",
    "\n",
    "    def connect(self):\n",
    "        \"\"\"Establish connection to Couchbase cluster.\"\"\"\n",
    "        try:\n",
    "            auth = PasswordAuthenticator(self.username, self.password)\n",
    "            options = ClusterOptions(auth)\n",
    "            \n",
    "            # Use WAN profile for better timeout handling with remote clusters\n",
    "            options.apply_profile(\"wan_development\")\n",
    "            \n",
    "            self.cluster = Cluster(self.conn_string, options)\n",
    "            \n",
    "            # Wait until the cluster is ready for use\n",
    "            self.cluster.wait_until_ready(timedelta(seconds=30))\n",
    "            logger.info(f\"Successfully connected to Couchbase cluster: {self.conn_string}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to connect to Couchbase: {e}\")\n",
    "            raise\n",
    "\n",
    "    def setup_collection(self, scope_name: str, collection_name: str):\n",
    "        \"\"\"Setup bucket, scope and collection all in one function.\"\"\"\n",
    "        if not self.cluster:\n",
    "            raise RuntimeError(\"Cluster connection not established. Call connect() first.\")\n",
    "\n",
    "        try:\n",
    "            # Get or create bucket\n",
    "            bucket_manager = self.cluster.buckets()\n",
    "            try:\n",
    "                bucket_manager.get_bucket(self.bucket_name)\n",
    "                logger.info(f\"Bucket '{self.bucket_name}' already exists\")\n",
    "            except Exception:\n",
    "                logger.info(f\"Creating bucket '{self.bucket_name}'\")\n",
    "                bucket_settings = CreateBucketSettings(\n",
    "                    name=self.bucket_name,\n",
    "                    bucket_type=\"couchbase\",\n",
    "                    ram_quota_mb=256,\n",
    "                    num_replicas=0\n",
    "                )\n",
    "                bucket_manager.create_bucket(bucket_settings)\n",
    "                time.sleep(5)\n",
    "\n",
    "            self.bucket = self.cluster.bucket(self.bucket_name)\n",
    "            \n",
    "            # Setup scope and collection\n",
    "            scope_manager = self.bucket.collections()\n",
    "            \n",
    "            # Create scope if it doesn't exist\n",
    "            try:\n",
    "                scope_manager.create_scope(scope_name)\n",
    "                logger.info(f\"Created scope '{scope_name}'\")\n",
    "                time.sleep(2)\n",
    "            except Exception as e:\n",
    "                if \"already exists\" in str(e):\n",
    "                    logger.info(f\"Scope '{scope_name}' already exists\")\n",
    "                else:\n",
    "                    logger.warning(f\"Error creating scope: {e}\")\n",
    "\n",
    "            # Create collection if it doesn't exist\n",
    "            try:\n",
    "                scope_manager.create_collection(scope_name, collection_name)\n",
    "                logger.info(f\"Created collection '{collection_name}' in scope '{scope_name}'\")\n",
    "                time.sleep(2)\n",
    "            except Exception as e:\n",
    "                if \"already exists\" in str(e):\n",
    "                    logger.info(f\"Collection '{collection_name}' already exists in scope '{scope_name}'\")\n",
    "                else:\n",
    "                    logger.warning(f\"Error creating collection: {e}\")\n",
    "\n",
    "            # Store collection reference\n",
    "            collection = self.bucket.scope(scope_name).collection(collection_name)\n",
    "            self._collections[f\"{scope_name}.{collection_name}\"] = collection\n",
    "            \n",
    "            logger.info(f\"Collection setup complete for {scope_name}.{collection_name}\")\n",
    "            return collection\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to setup collection {scope_name}.{collection_name}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def get_collection(self, scope_name: str, collection_name: str):\n",
    "        \"\"\"Get a collection reference.\"\"\"\n",
    "        key = f\"{scope_name}.{collection_name}\"\n",
    "        if key not in self._collections:\n",
    "            self.setup_collection(scope_name, collection_name)\n",
    "        return self._collections[key]\n",
    "\n",
    "    def setup_vector_search_index(self, index_definition: dict):\n",
    "        \"\"\"Setup vector search index.\"\"\"\n",
    "        if not self.bucket:\n",
    "            raise RuntimeError(\"Bucket not initialized. Call setup_collection first.\")\n",
    "        \n",
    "        try:\n",
    "            search_index_manager = self.cluster.search_indexes()\n",
    "            index_name = index_definition[\"name\"]\n",
    "            \n",
    "            # Check if index already exists\n",
    "            try:\n",
    "                search_index_manager.get_index(index_name)\n",
    "                logger.info(f\"Vector search index '{index_name}' already exists\")\n",
    "            except Exception:\n",
    "                logger.info(f\"Creating vector search index '{index_name}'\")\n",
    "                search_index = SearchIndex.from_json(json.dumps(index_definition))\n",
    "                search_index_manager.upsert_index(search_index)\n",
    "                logger.info(f\"Vector search index '{index_name}' created successfully\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to setup vector search index: {e}\")\n",
    "            raise\n",
    "\n",
    "    def setup_ai_models(self, span):\n",
    "        \"\"\"Setup embeddings and LLM using Capella AI Services or OpenAI.\"\"\"\n",
    "        try:\n",
    "            # Setup embeddings\n",
    "            if os.environ.get('CAPELLA_API_ENDPOINT'):\n",
    "                try:\n",
    "                    self.embed_model = OpenAIEmbedding(\n",
    "                        api_key=os.environ['CAPELLA_API_KEY'],\n",
    "                        api_base=os.environ['CAPELLA_API_ENDPOINT'],\n",
    "                        model=os.environ['CAPELLA_API_EMBEDDING_MODEL']\n",
    "                    )\n",
    "                    logger.info(f\"Using Capella AI embeddings: {os.environ['CAPELLA_API_EMBEDDING_MODEL']}\")\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Failed to setup Capella AI embeddings: {e}\")\n",
    "                    self.embed_model = OpenAIEmbedding()\n",
    "                    logger.info(\"Falling back to OpenAI embeddings\")\n",
    "            else:\n",
    "                self.embed_model = OpenAIEmbedding()\n",
    "                logger.info(\"Using OpenAI embeddings\")\n",
    "\n",
    "            # Setup LLM\n",
    "            if os.environ.get('CAPELLA_API_ENDPOINT'):\n",
    "                try:\n",
    "                    self.llm = OpenAILike(\n",
    "                        api_key=os.environ['CAPELLA_API_KEY'],\n",
    "                        api_base=os.environ['CAPELLA_API_ENDPOINT'],\n",
    "                        model=os.environ['CAPELLA_API_LLM_MODEL'],\n",
    "                        temperature=0.1,\n",
    "                        context_window=4096,\n",
    "                        max_tokens=1024,\n",
    "                        callback_manager=Callback.get_callback_manager(span)\n",
    "                    )\n",
    "                    logger.info(f\"Using Capella AI LLM: {os.environ['CAPELLA_API_LLM_MODEL']}\")\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Failed to setup Capella AI LLM: {e}\")\n",
    "                    from llama_index.llms.openai import OpenAI\n",
    "                    self.llm = OpenAI(temperature=0.1, callback_manager=Callback.get_callback_manager(span))\n",
    "                    logger.info(\"Falling back to OpenAI LLM\")\n",
    "            else:\n",
    "                from llama_index.llms.openai import OpenAI\n",
    "                self.llm = OpenAI(temperature=0.1, callback_manager=Callback.get_callback_manager(span))\n",
    "                logger.info(\"Using OpenAI LLM\")\n",
    "\n",
    "            # Set global settings\n",
    "            Settings.embed_model = self.embed_model\n",
    "            Settings.llm = self.llm\n",
    "            \n",
    "            return self.embed_model, self.llm\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to setup AI models: {e}\")\n",
    "            raise RuntimeError(f\"❌ Failed to setup AI models: {str(e)}\")\n",
    "\n",
    "    def load_route_data(self, vector_store, span):\n",
    "        \"\"\"Load route data using the data loading script.\"\"\"\n",
    "        try:\n",
    "            # Import and run the route data loader\n",
    "            import sys\n",
    "            import importlib.util\n",
    "            \n",
    "            # Load the route data module\n",
    "            spec = importlib.util.spec_from_file_location(\"route_data\", \"data/route_data.py\")\n",
    "            route_data_module = importlib.util.module_from_spec(spec)\n",
    "            spec.loader.exec_module(route_data_module)\n",
    "            \n",
    "            # Call the load function\n",
    "            route_data_module.load_route_data()\n",
    "            logger.info(\"Route data loaded successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load route data: {e}\")\n",
    "            raise\n",
    "\n",
    "    def setup_vector_store(self, span):\n",
    "        \"\"\"Setup vector store and load data.\"\"\"\n",
    "        try:\n",
    "            scope_name = os.environ['CB_SCOPE']\n",
    "            collection_name = os.environ['CB_COLLECTION']\n",
    "            index_name = os.environ['CB_INDEX']\n",
    "            \n",
    "            # Create vector store\n",
    "            vector_store = CouchbaseSearchVectorStore(\n",
    "                cluster=self.cluster,\n",
    "                bucket_name=self.bucket_name,\n",
    "                scope_name=scope_name,\n",
    "                collection_name=collection_name,\n",
    "                index_name=index_name\n",
    "            )\n",
    "            \n",
    "            # Create index from vector store\n",
    "            index = VectorStoreIndex.from_vector_store(vector_store)\n",
    "            \n",
    "            logger.info(f\"Vector store setup completed for {scope_name}.{collection_name}\")\n",
    "            return vector_store, index\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to setup vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "    def create_llamaindex_agent(self, catalog, span):\n",
    "        \"\"\"Create LlamaIndex ReAct agent with tools and prompts from Agent Catalog.\"\"\"\n",
    "        try:\n",
    "            # Get tools from Agent Catalog\n",
    "            catalog_tools = []\n",
    "            for tool in catalog.tools():\n",
    "                try:\n",
    "                    func_tool = FunctionTool.from_defaults(\n",
    "                        fn=tool.implementation,\n",
    "                        name=tool.name,\n",
    "                        description=tool.description\n",
    "                    )\n",
    "                    catalog_tools.append(func_tool)\n",
    "                    logger.info(f\"Added tool: {tool.name}\")\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Failed to add tool {tool.name}: {e}\")\n",
    "            \n",
    "            # Get system prompt from Agent Catalog\n",
    "            try:\n",
    "                system_prompt = catalog.get_prompt(\"route_planner_assistant\").content\n",
    "                logger.info(\"Using route_planner_assistant prompt from Agent Catalog\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Failed to get prompt from catalog: {e}\")\n",
    "                system_prompt = \"You are a helpful route planning assistant.\"\n",
    "            \n",
    "            # Create ReAct agent\n",
    "            agent = ReActAgent.from_tools(\n",
    "                tools=catalog_tools,\n",
    "                llm=self.llm,\n",
    "                verbose=True,\n",
    "                system_prompt=system_prompt\n",
    "            )\n",
    "            \n",
    "            logger.info(f\"Created LlamaIndex agent with {len(catalog_tools)} tools\")\n",
    "            return agent\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to create LlamaIndex agent: {e}\")\n",
    "            raise\n",
    "\n",
    "print(\"✅ CouchbaseClient class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "catalog_setup",
   "metadata": {},
   "source": [
    "## Agent Catalog and Span Setup\n",
    "\n",
    "Initialize Agent Catalog with the catalog index and create application span for logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "catalog_init",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize agent catalog\n",
    "catalog = agentc.from_index(\"agentcatalog_index.json\")\n",
    "\n",
    "# Create application span for logging\n",
    "application_span = catalog.create_span(\"Route Planner Agent\", span_type=\"workflow\")\n",
    "\n",
    "print(\"✅ Agent Catalog initialized\")\n",
    "print(f\"Available tools: {[tool.name for tool in catalog.tools()]}\")\n",
    "print(f\"Available prompts: {[prompt.name for prompt in catalog.prompts()]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agent_setup",
   "metadata": {},
   "source": [
    "## Route Planner Agent Setup\n",
    "\n",
    "Setup the complete route planner agent with Couchbase infrastructure and Agent Catalog integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agent_setup_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_route_planner_agent():\n",
    "    \"\"\"Setup route planner agent with all infrastructure.\"\"\"\n",
    "    try:\n",
    "        # Setup environment\n",
    "        setup_environment()\n",
    "        \n",
    "        with application_span.new(\"Couchbase Setup\"):\n",
    "            # Initialize Couchbase client\n",
    "            client = CouchbaseClient(\n",
    "                conn_string=os.environ[\"CB_CONN_STRING\"],\n",
    "                username=os.environ[\"CB_USERNAME\"],\n",
    "                password=os.environ[\"CB_PASSWORD\"],\n",
    "                bucket_name=os.environ[\"CB_BUCKET\"]\n",
    "            )\n",
    "            client.connect()\n",
    "            client.setup_collection(\n",
    "                scope_name=os.environ[\"CB_SCOPE\"],\n",
    "                collection_name=os.environ[\"CB_COLLECTION\"]\n",
    "            )\n",
    "        \n",
    "        with application_span.new(\"Vector Index Setup\"):\n",
    "            # Load vector search index definition\n",
    "            with open(\"agentcatalog_index.json\", \"r\") as f:\n",
    "                agentc_index = json.load(f)\n",
    "            \n",
    "            # Setup vector search index\n",
    "            if \"index_definition\" in agentc_index:\n",
    "                try:\n",
    "                    client.setup_vector_search_index(agentc_index[\"index_definition\"])\n",
    "                    logger.info(\"Vector search index setup completed\")\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Vector search index setup failed: {e}\")\n",
    "        \n",
    "        with application_span.new(\"AI Models Setup\"):\n",
    "            # Setup AI models\n",
    "            embed_model, llm = client.setup_ai_models(application_span)\n",
    "        \n",
    "        with application_span.new(\"Vector Store Setup\"):\n",
    "            # Setup vector store\n",
    "            vector_store, index = client.setup_vector_store(application_span)\n",
    "        \n",
    "        with application_span.new(\"Agent Creation\"):\n",
    "            # Create LlamaIndex agent\n",
    "            agent = client.create_llamaindex_agent(catalog, application_span)\n",
    "            \n",
    "            logger.info(\"Route planner agent created successfully\")\n",
    "            return agent, client, application_span\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.exception(f\"Setup error: {e}\")\n",
    "        raise\n",
    "\n",
    "# Setup the agent\n",
    "agent, couchbase_client, application_span = setup_route_planner_agent()\n",
    "print(\"✅ Route planner agent setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interactive_demo",
   "metadata": {},
   "source": [
    "## Interactive Demo\n",
    "\n",
    "Test the route planner agent with sample queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demo_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_interactive_demo():\n",
    "    \"\"\"Run interactive demo of the route planner agent.\"\"\"\n",
    "    print(\"\\n🗺️ Route Planner Agent Interactive Demo\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    sample_queries = [\n",
    "        \"What's the best route from San Francisco to Los Angeles?\",\n",
    "        \"Calculate the distance between New York and Boston\",\n",
    "        \"Find me routes from Miami to Orlando\",\n",
    "        \"What's the travel time from Seattle to Portland?\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nSample queries you can try:\")\n",
    "    for i, query in enumerate(sample_queries, 1):\n",
    "        print(f\"{i}. {query}\")\n",
    "    \n",
    "    print(\"\\nType 'quit' to exit the demo.\\n\")\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            user_input = input(\"\\n💬 Your query: \").strip()\n",
    "            \n",
    "            if user_input.lower() in ['quit', 'exit', 'q']:\n",
    "                print(\"👋 Thanks for using the route planner agent!\")\n",
    "                break\n",
    "            \n",
    "            if not user_input:\n",
    "                print(\"Please enter a query or 'quit' to exit.\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"\\n🔍 Processing: {user_input}\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            # Create span for this query\n",
    "            with application_span.new(\"Route Planning Query\") as query_span:\n",
    "                query_span.log(query=user_input)\n",
    "                \n",
    "                # Execute the query\n",
    "                response = agent.chat(user_input)\n",
    "                \n",
    "                print(f\"\\n🤖 Response: {response.response}\")\n",
    "                query_span.log(response=str(response.response))\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n👋 Demo interrupted. Goodbye!\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error: {e}\")\n",
    "            print(\"Please try another query.\")\n",
    "\n",
    "# Run the interactive demo\n",
    "run_interactive_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test_single_query",
   "metadata": {},
   "source": [
    "## Test Single Query\n",
    "\n",
    "Test the agent with a single query for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test_query_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a single query\n",
    "test_query = \"What's the best route from San Francisco to Los Angeles?\"\n",
    "\n",
    "with application_span.new(\"Test Query\") as test_span:\n",
    "    test_span.log(query=test_query)\n",
    "    \n",
    "    response = agent.chat(test_query)\n",
    "    \n",
    "    print(f\"\\n🔍 Query: {test_query}\")\n",
    "    print(f\"🤖 Response: {response.response}\")\n",
    "    \n",
    "    test_span.log(response=str(response.response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arize_evaluation",
   "metadata": {},
   "source": [
    "## Arize Phoenix Evaluation Demo\n",
      "\n",
    "This section demonstrates how to evaluate the route planner agent using Arize Phoenix observability platform. The evaluation includes:\n",
      "\n",
    "- **Relevance Scoring**: Using Phoenix evaluators to score how relevant responses are to queries\n",
    "- **QA Scoring**: Using Phoenix evaluators to score answer quality\n",
    "- **Route-specific Evaluations**: Custom evaluations for route information, distance calculations, and travel times\n",
    "- **Phoenix UI**: Real-time observability dashboard at `http://localhost:6006/`\n",
      "\n",
    "The evaluation system provides comprehensive metrics for route planner agent performance monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phoenix_imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import evaluation dependencies\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# Phoenix imports with fallback\n",
    "try:\n",
    "    import phoenix as px\n",
    "    from phoenix.otel import register\n",
    "    from openinference.instrumentation.llama_index import LlamaIndexInstrumentor\n",
    "    from opentelemetry import trace\n",
    "    from opentelemetry.sdk.trace import TracerProvider\n",
    "    from opentelemetry.sdk.trace.export import SimpleSpanProcessor\n",
    "    from phoenix.evals import (\n",
    "        OpenAIModel,\n",
    "        llm_classify,\n",
    "        RAG_RELEVANCY_PROMPT_TEMPLATE,\n",
    "        QA_PROMPT_TEMPLATE,\n",
    "    )\n",
    "    import urllib3\n",
    "    PHOENIX_AVAILABLE = True\n",
    "    print(\"✅ Phoenix dependencies loaded successfully\")\n",
    "    \n",
    "    # Suppress warnings\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"⚠️ Phoenix not available: {e}\")\n",
    "    print(\"📦 Install with: pip install arize-phoenix openinference-instrumentation-llama-index\")\n",
    "    PHOENIX_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "route_evaluator_class",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RouteEvaluator:\n",
    "    \"\"\"Phoenix evaluation class for route planner agent.\"\"\"\n",
      "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the route evaluator.\"\"\"\n",
    "        self.phoenix_session = None\n",
    "        self.tracer_provider = None\n",
    "        self.agent = None\n",
    "        self.client = None\n",
    "        self.span = None\n",
    "        print(\"🔧 RouteEvaluator initialized\")\n",
      "\n",
    "    def setup_phoenix(self):\n",
    "        \"\"\"Setup Phoenix session and instrumentation.\"\"\"\n",
    "        if not PHOENIX_AVAILABLE:\n",
    "            print(\"⚠️ Phoenix not available, skipping setup\")\n",
    "            return False\n",
      "\n",
    "        try:\n",
    "            # Start Phoenix session\n",
    "            self.phoenix_session = px.launch_app()\n",
    "            print(f\"✅ Phoenix UI started at http://localhost:6006\")\n",
      "\n",
    "            # Setup tracing\n",
    "            self.tracer_provider = TracerProvider()\n",
    "            trace.set_tracer_provider(self.tracer_provider)\n",
    "            register()\n",
      "\n",
    "            # Instrument LlamaIndex\n",
    "            LlamaIndexInstrumentor().instrument()\n",
    "            print(\"🔧 LlamaIndex instrumentation completed\")\n",
      "\n",
    "            return True\n",
      "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to setup Phoenix: {e}\")\n",
    "            return False\n",
      "\n",
    "    def cleanup_phoenix(self):\n",
    "        \"\"\"Cleanup Phoenix session.\"\"\"\n",
    "        if self.phoenix_session:\n",
    "            try:\n",
    "                # Phoenix sessions cleanup automatically\n",
    "                print(\"🧹 Phoenix session cleanup completed\")\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Error during Phoenix cleanup: {e}\")\n",
      "\n",
    "    def setup_agent(self):\n",
    "        \"\"\"Setup the route planner agent for evaluation.\"\"\"\n",
    "        try:\n",
    "            # Use the existing agent and client from the notebook\n",
    "            self.agent = agent\n",
    "            self.client = couchbase_client\n",
    "            self.span = application_span\n",
    "            \n",
    "            print(\"✅ Agent setup completed for evaluation\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to setup agent: {e}\")\n",
    "            return False\n",
      "\n",
    "    def run_evaluation(self, test_queries: List[str]) -> pd.DataFrame:\n",
    "        \"\"\"Run comprehensive evaluation on route planner queries.\"\"\"\n",
    "        print(f\"🚀 Running evaluation on {len(test_queries)} route planning queries\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for i, query in enumerate(test_queries, 1):\n",
    "            print(f\"\\n🔍 Query {i}/{len(test_queries)}: {query}\")\n",
    "            \n",
    "            try:\n",
    "                with self.span.new(f\"Route Query {i}\") as query_span:\n",
    "                    query_span.log(query=query)\n",
    "                    \n",
    "                    # Execute query using LlamaIndex agent\n",
    "                    response = self.agent.chat(query)\n",
    "                    response_text = str(response.response)\n",
    "                    \n",
    "                    query_span.log(response=response_text)\n",
    "                    \n",
    "                    # Clean and evaluate response\n",
    "                    cleaned_response = self._clean_response(response_text)\n",
    "                    \n",
    "                    # Route-specific evaluations\n",
    "                    has_route_info = self._check_route_info(cleaned_response)\n",
    "                    has_distance_info = self._check_distance_info(cleaned_response)\n",
    "                    has_travel_time = self._check_travel_time(cleaned_response)\n",
    "                    appropriate_length = self._check_appropriate_length(cleaned_response)\n",
    "                    is_relevant = self._check_relevance(query, cleaned_response)\n",
    "                    \n",
    "                    # Calculate overall quality score\n",
    "                    quality_score = self._calculate_quality_score(\n",
    "                        has_route_info, has_distance_info, has_travel_time,\n",
    "                        appropriate_length, is_relevant\n",
    "                    )\n",
    "                    \n",
    "                    result = {\n",
    "                        'query_index': i,\n",
    "                        'query': query,\n",
    "                        'response': cleaned_response,\n",
    "                        'has_route_info': has_route_info,\n",
    "                        'has_distance_info': has_distance_info,\n",
    "                        'has_travel_time': has_travel_time,\n",
    "                        'appropriate_length': appropriate_length,\n",
    "                        'is_relevant': is_relevant,\n",
    "                        'quality_score': quality_score,\n",
    "                        'response_length': len(cleaned_response)\n",
    "                    }\n",
    "                    \n",
    "                    results.append(result)\n",
    "                    \n",
    "                    # Print quick evaluation\n",
    "                    print(f\"   ✅ Quality Score: {quality_score:.2f}/1.0\")\n",
    "                    print(f\"   📍 Route Info: {'✓' if has_route_info else '✗'}\")\n",
    "                    print(f\"   📏 Distance Info: {'✓' if has_distance_info else '✗'}\")\n",
    "                    print(f\"   ⏱️ Travel Time: {'✓' if has_travel_time else '✗'}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"   ❌ Error processing query: {e}\")\n",
    "                # Add error result\n",
    "                results.append({\n",
    "                    'query_index': i,\n",
    "                    'query': query,\n",
    "                    'response': f\"Error: {str(e)}\",\n",
    "                    'has_route_info': False,\n",
    "                    'has_distance_info': False,\n",
    "                    'has_travel_time': False,\n",
    "                    'appropriate_length': False,\n",
    "                    'is_relevant': False,\n",
    "                    'quality_score': 0.0,\n",
    "                    'response_length': 0\n",
    "                })\n",
    "        \n",
    "        return pd.DataFrame(results)\n",
      "\n",
    "    def _clean_response(self, response: str) -> str:\n",
    "        \"\"\"Clean response text for evaluation.\"\"\"\n",
    "        if not response:\n",
    "            return \"\"\n",
    "        \n",
    "        # Remove common LlamaIndex artifacts\n",
    "        response = response.replace(\"assistant: \", \"\")\n",
    "        response = response.replace(\"Assistant: \", \"\")\n",
    "        \n",
    "        # Remove excessive whitespace\n",
    "        import re\n",
    "        response = re.sub(r'\\s+', ' ', response)\n",
    "        response = response.strip()\n",
    "        \n",
    "        return response\n",
      "\n",
    "    def _check_route_info(self, response: str) -> bool:\n",
    "        \"\"\"Check if response contains route information.\"\"\"\n",
    "        route_keywords = ['route', 'path', 'highway', 'road', 'interstate', 'direction', 'way']\n",
    "        response_lower = response.lower()\n",
    "        return any(keyword in response_lower for keyword in route_keywords)\n",
      "\n",
    "    def _check_distance_info(self, response: str) -> bool:\n",
    "        \"\"\"Check if response contains distance information.\"\"\"\n",
    "        distance_keywords = ['mile', 'km', 'kilometer', 'distance', 'far', 'away']\n",
    "        response_lower = response.lower()\n",
    "        return any(keyword in response_lower for keyword in distance_keywords)\n",
      "\n",
    "    def _check_travel_time(self, response: str) -> bool:\n",
    "        \"\"\"Check if response contains travel time information.\"\"\"\n",
    "        time_keywords = ['hour', 'minute', 'time', 'duration', 'take', 'drive', 'travel']\n",
    "        response_lower = response.lower()\n",
    "        return any(keyword in response_lower for keyword in time_keywords)\n",
      "\n",
    "    def _check_appropriate_length(self, response: str) -> bool:\n",
    "        \"\"\"Check if response has appropriate length.\"\"\"\n",
    "        return 20 <= len(response) <= 2000\n",
      "\n",
    "    def _check_relevance(self, query: str, response: str) -> bool:\n",
    "        \"\"\"Basic relevance check between query and response.\"\"\"\n",
    "        if len(response) < 10:\n",
    "            return False\n",
    "        \n",
    "        query_words = set(query.lower().split())\n",
    "        response_words = set(response.lower().split())\n",
    "        \n",
    "        # Check for word overlap\n",
    "        overlap = len(query_words.intersection(response_words))\n",
    "        return overlap >= 2\n",
    "\n",
    "    def _calculate_quality_score(\n",
    "        self, has_route_info: bool, has_distance_info: bool, has_travel_time: bool,\n",
    "        appropriate_length: bool, is_relevant: bool,\n",
    "    ) -> float:\n",
    "        \"\"\"Calculate overall quality score for the response.\"\"\"\n",
    "        # Weight different aspects of quality\n",
    "        weights = {\n",
    "            'route_info': 0.3,\n",
    "            'distance_info': 0.25,\n",
    "            'travel_time': 0.2,\n",
    "            'appropriate_length': 0.1,\n",
    "            'relevance': 0.15\n",
    "        }\n",
    "        \n",
    "        score = (\n",
    "            weights['route_info'] * has_route_info +\n",
    "            weights['distance_info'] * has_distance_info +\n",
    "            weights['travel_time'] * has_travel_time +\n",
    "            weights['appropriate_length'] * appropriate_length +\n",
    "            weights['relevance'] * is_relevant\n",
    "        )\n",
    "        \n",
    "        return round(score, 2)\n",
    "\n",
    "    def _run_phoenix_evaluations(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Run Phoenix AI evaluations on the results.\"\"\"\n",
    "        if not PHOENIX_AVAILABLE:\n",
    "            print(\"⚠️ Phoenix not available, skipping AI evaluations\")\n",
    "            return df\n",
    "\n",
    "        print(\"\\n🧠 Running Phoenix AI evaluations...\")\n",
    "        \n",
    "        try:\n",
    "            # Setup evaluator model\n",
    "            eval_model = OpenAIModel(model=\"gpt-4\")\n",
    "            \n",
    "            # Prepare evaluation dataframe\n",
    "            eval_df = df[['query', 'response']].copy()\n",
    "            eval_df.rename(columns={'query': 'input', 'response': 'output'}, inplace=True)\n",
    "            \n",
    "            # Add reference answers for context\n",
    "            eval_df['reference'] = df['query'].apply(self._get_reference_answer)\n",
    "            \n",
    "            # Run relevance evaluation\n",
    "            print(\"  📋 Running relevance evaluation...\")\n",
    "            relevance_results = llm_classify(\n",
    "                dataframe=eval_df,\n",
    "                model=eval_model,\n",
    "                template=RAG_RELEVANCY_PROMPT_TEMPLATE,\n",
    "                rails=[\"relevant\", \"irrelevant\"]\n",
    "            )\n",
    "            df['phoenix_relevance'] = relevance_results\n",
    "            \n",
    "            # Run QA evaluation\n",
    "            print(\"  ✅ Running QA evaluation...\")\n",
    "            qa_results = llm_classify(\n",
    "                dataframe=eval_df,\n",
    "                model=eval_model,\n",
    "                template=QA_PROMPT_TEMPLATE,\n",
    "                rails=[\"correct\", \"incorrect\"]\n",
    "            )\n",
    "            df['phoenix_qa'] = qa_results\n",
    "            \n",
    "            print(\"✅ Phoenix evaluations completed\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Phoenix evaluation failed: {e}\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def _get_reference_answer(self, query: str) -> str:\n",
    "        \"\"\"Generate reference answer for evaluation context.\"\"\"\n",
    "        # Simple reference answers based on query type\n",
    "        query_lower = query.lower()\n",
    "        \n",
    "        if 'distance' in query_lower or 'far' in query_lower:\n",
    "            return \"The response should include specific distance information in miles or kilometers.\"\n",
    "        elif 'route' in query_lower or 'find' in query_lower:\n",
    "            return \"The response should provide specific route information with directions or road names.\"\n",
    "        elif 'time' in query_lower or 'travel' in query_lower:\n",
    "            return \"The response should include travel time estimates in hours or minutes.\"\n",
    "        else:\n",
    "            return \"The response should provide relevant route planning information.\"\n",
    "\n",
    "    def print_summary(self, df: pd.DataFrame):\n",
    "        \"\"\"Print evaluation summary statistics.\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"📊 ROUTE PLANNER AGENT EVALUATION SUMMARY\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        total_queries = len(df)\n",
    "        avg_quality_score = df['quality_score'].mean()\n",
    "        \n",
    "        print(f\"📋 Total Queries Processed: {total_queries}\")\n",
    "        print(f\"🎯 Average Quality Score: {avg_quality_score:.2f}/1.0\")\n",
    "        print(f\"📍 Route Info Success: {df['has_route_info'].sum()}/{total_queries} ({df['has_route_info'].mean()*100:.1f}%)\")\n",
    "        print(f\"📏 Distance Info Success: {df['has_distance_info'].sum()}/{total_queries} ({df['has_distance_info'].mean()*100:.1f}%)\")\n",
    "        print(f\"⏱️ Travel Time Success: {df['has_travel_time'].sum()}/{total_queries} ({df['has_travel_time'].mean()*100:.1f}%)\")\n",
    "        print(f\"✅ Relevant Responses: {df['is_relevant'].sum()}/{total_queries} ({df['is_relevant'].mean()*100:.1f}%)\")\n",
    "        \n",
    "        # Phoenix evaluation results\n",
    "        if 'phoenix_relevance' in df.columns:\n",
    "            relevance_scores = df['phoenix_relevance'].value_counts()\n",
    "            print(f\"\\n🔍 Phoenix Relevance Evaluation:\")\n",
    "            for score, count in relevance_scores.items():\n",
    "                print(f\"   {score}: {count} ({count/total_queries*100:.1f}%)\")\n",
    "        \n",
    "        if 'phoenix_qa' in df.columns:\n",
    "            qa_scores = df['phoenix_qa'].value_counts()\n",
    "            print(f\"\\n✅ Phoenix QA Evaluation:\")\n",
    "            for score, count in qa_scores.items():\n",
    "                print(f\"   {score}: {count} ({count/total_queries*100:.1f}%)\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "\n",
    "print(\"✅ RouteEvaluator class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run_route_evaluation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_route_evaluation_demo():\n",
    "    \"\"\"Run comprehensive route planner evaluation with Phoenix.\"\"\"\n",
    "    print(\"🚀 Starting Route Planner Agent Evaluation with Phoenix\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Test queries focused on the 2 available tools\n",
    "    test_queries = [\n",
    "        \"Find routes from New York to Boston\",\n",
    "        \"Calculate the driving distance from San Francisco to Los Angeles\",\n",
    "        \"What are scenic routes in California?\",\n",
    "        \"How far is it to fly from Miami to New York?\",\n",
    "        \"Find mountain routes in Colorado\",\n",
    "        \"Calculate train travel time from Chicago to Detroit\",\n",
    "    ]\n",
    "\n",
    "    # Initialize evaluator\n",
    "    evaluator = RouteEvaluator()\n",
    "\n",
    "    try:\n",
    "        # Setup Phoenix if available\n",
    "        phoenix_setup = evaluator.setup_phoenix()\n",
    "        \n",
    "        # Setup agent\n",
    "        if not evaluator.setup_agent():\n",
    "            print(\"❌ Could not setup agent. Exiting.\")\n",
    "            return None\n",
    "\n",
    "        # Run evaluation\n",
    "        results_df = evaluator.run_evaluation(test_queries)\n",
    "        \n",
    "        # Run Phoenix AI evaluations if available\n",
    "        if phoenix_setup:\n",
    "            results_df = evaluator._run_phoenix_evaluations(results_df)\n",
    "\n",
    "        # Print summary\n",
    "        evaluator.print_summary(results_df)\n",
    "\n",
    "        # Keep Phoenix UI running\n",
    "        if phoenix_setup:\n",
    "            print(f\"\\n🌟 Phoenix UI is running at: http://localhost:6006/\")\n",
    "            print(\"💡 Visit the Phoenix UI to see detailed traces and evaluations\")\n",
    "            print(\"📊 The UI shows LlamaIndex execution, tool calls, and evaluation scores\")\n",
    "            print(\"\\nThe Phoenix session will continue running for trace exploration.\")\n",
    "\n",
    "        print(\"\\n✅ Evaluation completed successfully!\")\n",
    "        return results_df\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n⚠️ Evaluation interrupted by user\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Evaluation failed: {e}\")\n",
    "        logger.error(f\"Evaluation error: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        # Note: We don't cleanup Phoenix here to keep the UI running for exploration\n",
    "        pass\n",
    "\n",
    "# Run the evaluation demo\n",
    "route_evaluation_results = run_route_evaluation_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detailed_results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display detailed evaluation results\n",
    "if route_evaluation_results is not None and not route_evaluation_results.empty:\n",
    "    print(\"\\n📋 Detailed Route Evaluation Results:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for _, row in route_evaluation_results.iterrows():\n",
    "        print(f\"\\n🔍 Query {row['query_index']}: {row['query']}\")\n",
    "        print(f\"🤖 Response: {row['response'][:200]}{'...' if len(row['response']) > 200 else ''}\")\n",
    "        print(f\"📊 Quality Score: {row['quality_score']}/1.0\")\n",
    "        print(f\"✅ Evaluation Metrics:\")\n",
    "        print(f\"   📍 Route Info: {'✓' if row['has_route_info'] else '✗'}\")\n",
    "        print(f\"   📏 Distance Info: {'✓' if row['has_distance_info'] else '✗'}\")\n",
    "        print(f\"   ⏱️ Travel Time: {'✓' if row['has_travel_time'] else '✗'}\")\n",
    "        print(f\"   📝 Appropriate Length: {'✓' if row['appropriate_length'] else '✗'}\")\n",
    "        print(f\"   🎯 Relevant: {'✓' if row['is_relevant'] else '✗'}\")\n",
    "        \n",
    "        # Phoenix evaluation results\n",
    "        if 'phoenix_relevance' in row and pd.notna(row['phoenix_relevance']):\n",
    "            print(f\"   🔍 Phoenix Relevance: {row['phoenix_relevance']}\")\n",
    "        if 'phoenix_qa' in row and pd.notna(row['phoenix_qa']):\n",
    "            print(f\"   ✅ Phoenix QA: {row['phoenix_qa']}\")\n",
    "        \n",
    "        print(\"-\" * 50)\n",
    "else:\n",
    "    print(\"⚠️ No evaluation results to display\")\n",
    "\n",
    "# Cleanup note\n",
    "print(\"\\n🧹 Note: To cleanup the Phoenix session, restart the kernel or run:\")\n",
    "print(\"evaluator.cleanup_phoenix()\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
