{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Landmark Search Agent Tutorial - Priority 1 Implementation\n",
        "\n",
        "This notebook demonstrates the Agent Catalog landmark search agent using LlamaIndex with Couchbase vector store and Arize Phoenix evaluation. Uses Priority 1 AI services with standard OpenAI wrappers and Capella (simple & fast).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Setup and Imports\n",
        "\n",
        "Import all necessary modules for the landmark search agent using self-contained setup.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import base64\n",
        "import getpass\n",
        "import httpx\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "from datetime import timedelta\n",
        "\n",
        "import agentc\n",
        "import dotenv\n",
        "from couchbase.auth import PasswordAuthenticator\n",
        "from couchbase.cluster import Cluster\n",
        "from couchbase.management.buckets import BucketType, CreateBucketSettings\n",
        "from couchbase.management.search import SearchIndex\n",
        "from couchbase.options import ClusterOptions\n",
        "from llama_index.core import Settings\n",
        "from llama_index.core.agent import ReActAgent\n",
        "from llama_index.core.tools import FunctionTool\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.llms.nvidia import NVIDIA\n",
        "from llama_index.llms.openai_like import OpenAILike\n",
        "from llama_index.vector_stores.couchbase import CouchbaseSearchVectorStore\n",
        "from pydantic import SecretStr\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Reduce noise from various libraries during embedding/vector operations\n",
        "logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n",
        "logging.getLogger(\"httpcore\").setLevel(logging.WARNING)\n",
        "logging.getLogger(\"urllib3\").setLevel(logging.WARNING)\n",
        "\n",
        "# Load environment variables\n",
        "dotenv.load_dotenv(override=True)\n",
        "\n",
        "# Set default values for travel-sample bucket configuration\n",
        "DEFAULT_BUCKET = \"travel-sample\"\n",
        "DEFAULT_SCOPE = \"agentc_data\"\n",
        "DEFAULT_COLLECTION = \"landmark_data\"\n",
        "DEFAULT_INDEX = \"landmark_data_index\"\n",
        "DEFAULT_CAPELLA_API_EMBEDDING_MODEL = \"Snowflake/snowflake-arctic-embed-l-v2.0\"\n",
        "DEFAULT_CAPELLA_API_LLM_MODEL = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
        "DEFAULT_NVIDIA_API_LLM_MODEL = \"meta/llama-3.1-70b-instruct\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Self-Contained Setup Functions\n",
        "\n",
        "Define all necessary setup functions inline for a self-contained notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def setup_environment():\n",
        "    \"\"\"Setup default environment variables for agent operations.\"\"\"\n",
        "    defaults = {\n",
        "        \"CB_BUCKET\": \"travel-sample\",\n",
        "        \"CB_SCOPE\": \"agentc_data\",\n",
        "        \"CB_COLLECTION\": \"landmark_data\",\n",
        "        \"CB_INDEX\": \"landmark_data_index\",\n",
        "        \"NVIDIA_API_EMBEDDING_MODEL\": \"nvidia/nv-embedqa-e5-v5\",\n",
        "        \"NVIDIA_API_LLM_MODEL\": \"meta/llama-3.1-70b-instruct\",\n",
        "        \"CAPELLA_API_EMBEDDING_MODEL\": \"nvidia/nv-embedqa-e5-v5\",\n",
        "        \"CAPELLA_API_LLM_MODEL\": \"meta-llama/Llama-3.1-8B-Instruct\",\n",
        "    }\n",
        "    \n",
        "    for key, value in defaults.items():\n",
        "        if not os.getenv(key):\n",
        "            os.environ[key] = value\n",
        "    \n",
        "    logger.info(\"✅ Environment variables configured\")\n",
        "\n",
        "\n",
        "def test_capella_connectivity(api_key: str = None, endpoint: str = None) -> bool:\n",
        "    \"\"\"Test connectivity to Capella AI services.\"\"\"\n",
        "    try:\n",
        "        test_key = api_key or os.getenv(\"CAPELLA_API_EMBEDDINGS_KEY\") or os.getenv(\"CAPELLA_API_LLM_KEY\")\n",
        "        test_endpoint = endpoint or os.getenv(\"CAPELLA_API_ENDPOINT\")\n",
        "        \n",
        "        if not test_key or not test_endpoint:\n",
        "            return False\n",
        "        \n",
        "        # Simple connectivity test\n",
        "        headers = {\"Authorization\": f\"Bearer {test_key}\"}\n",
        "        \n",
        "        with httpx.Client(timeout=10.0) as client:\n",
        "            response = client.get(f\"{test_endpoint.rstrip('/')}/v1/models\", headers=headers)\n",
        "            return response.status_code < 500\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"⚠️ Capella connectivity test failed: {e}\")\n",
        "        return False\n",
        "\n",
        "\n",
        "def setup_ai_services(framework: str = \"llamaindex\", temperature: float = 0.0, application_span=None):\n",
        "    \"\"\"Priority 1: Capella AI with OpenAI wrappers (simple & fast) for LlamaIndex.\"\"\"\n",
        "    embeddings = None\n",
        "    llm = None\n",
        "    \n",
        "    logger.info(f\"🔧 Setting up Priority 1 AI services for {framework} framework...\")\n",
        "    \n",
        "    # Priority 1: Capella AI with direct API keys and OpenAI wrappers\n",
        "    if not embeddings and os.getenv(\"CAPELLA_API_ENDPOINT\") and os.getenv(\"CAPELLA_API_EMBEDDINGS_KEY\"):\n",
        "        try:\n",
        "            endpoint = os.getenv(\"CAPELLA_API_ENDPOINT\")\n",
        "            api_key = os.getenv(\"CAPELLA_API_EMBEDDINGS_KEY\")\n",
        "            model = os.getenv(\"CAPELLA_API_EMBEDDING_MODEL\")\n",
        "            \n",
        "            # Handle endpoint that may or may not already have /v1 suffix\n",
        "            if endpoint.endswith('/v1'):\n",
        "                api_base = endpoint\n",
        "            else:\n",
        "                api_base = f\"{endpoint}/v1\"\n",
        "            \n",
        "            # Debug logging - same pattern as working test\n",
        "            logger.info(f\"🔧 Endpoint: {endpoint}\")\n",
        "            logger.info(f\"🔧 Model: {model}\")\n",
        "            logger.info(f\"🔧 API Base: {api_base}\")\n",
        "            \n",
        "            embeddings = OpenAIEmbedding(\n",
        "                api_key=api_key,\n",
        "                api_base=api_base,\n",
        "                model_name=model,\n",
        "                embed_batch_size=30,\n",
        "                # Note: LlamaIndex doesn't need check_embedding_ctx_length=False\n",
        "            )\n",
        "            logger.info(\"✅ Using Priority 1: Capella AI embeddings (OpenAI wrapper)\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"❌ Priority 1 Capella AI embeddings failed: {type(e).__name__}: {e}\")\n",
        "    \n",
        "    if not llm and os.getenv(\"CAPELLA_API_ENDPOINT\") and os.getenv(\"CAPELLA_API_LLM_KEY\"):\n",
        "        try:\n",
        "            endpoint = os.getenv(\"CAPELLA_API_ENDPOINT\")\n",
        "            llm_key = os.getenv(\"CAPELLA_API_LLM_KEY\")\n",
        "            llm_model = os.getenv(\"CAPELLA_API_LLM_MODEL\")\n",
        "            \n",
        "            # Handle endpoint that may or may not already have /v1 suffix\n",
        "            if endpoint.endswith('/v1'):\n",
        "                api_base = endpoint\n",
        "            else:\n",
        "                api_base = f\"{endpoint}/v1\"\n",
        "            \n",
        "            # Debug logging\n",
        "            logger.info(f\"🔧 LLM Endpoint: {endpoint}\")\n",
        "            logger.info(f\"🔧 LLM Model: {llm_model}\")\n",
        "            logger.info(f\"🔧 LLM API Base: {api_base}\")\n",
        "            \n",
        "            llm = OpenAILike(\n",
        "                model=llm_model,\n",
        "                api_base=api_base,\n",
        "                api_key=llm_key,\n",
        "                is_chat_model=True,\n",
        "                is_function_calling_model=False,  # KEY FIX - prevents 500 errors\n",
        "                context_window=128000,  # Add context window for compatibility\n",
        "                temperature=temperature,\n",
        "                max_retries=1,  # Faster debugging\n",
        "            )\n",
        "            # Test the LLM works\n",
        "            test_response = llm.complete(\"Hello\")\n",
        "            logger.info(\"✅ Using Priority 1: Capella AI LLM (OpenAI wrapper)\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"❌ Priority 1 Capella AI LLM failed: {type(e).__name__}: {e}\")\n",
        "            llm = None\n",
        "    \n",
        "    # Fallback: OpenAI\n",
        "    if not embeddings and os.getenv(\"OPENAI_API_KEY\"):\n",
        "        try:\n",
        "            embeddings = OpenAIEmbedding(\n",
        "                model_name=\"text-embedding-3-small\",\n",
        "                api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
        "            )\n",
        "            logger.info(\"✅ Using OpenAI embeddings fallback\")\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"⚠️ OpenAI embeddings failed: {e}\")\n",
        "    \n",
        "    if not llm and os.getenv(\"OPENAI_API_KEY\"):\n",
        "        try:\n",
        "            llm = OpenAILike(\n",
        "                model=\"gpt-4o\",\n",
        "                api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
        "                is_chat_model=True,\n",
        "                is_function_calling_model=False,\n",
        "                temperature=temperature,\n",
        "            )\n",
        "            logger.info(\"✅ Using OpenAI LLM fallback\")\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"⚠️ OpenAI LLM failed: {e}\")\n",
        "    \n",
        "    if not embeddings:\n",
        "        raise ValueError(\"❌ No embeddings service could be initialized\")\n",
        "    if not llm:\n",
        "        raise ValueError(\"❌ No LLM service could be initialized\")\n",
        "    \n",
        "    logger.info(f\"✅ Priority 1 AI services setup completed for {framework}\")\n",
        "    return embeddings, llm\n",
        "\n",
        "\n",
        "# Setup environment\n",
        "setup_environment()\n",
        "\n",
        "# Test Capella AI connectivity if configured\n",
        "if os.getenv(\"CAPELLA_API_ENDPOINT\"):\n",
        "    if not test_capella_connectivity():\n",
        "        logger.warning(\"❌ Capella AI connectivity test failed. Will use fallback models.\")\n",
        "else:\n",
        "    logger.info(\"ℹ️ Capella API not configured - will use fallback models\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## CouchbaseClient Class\n",
        "\n",
        "Define the CouchbaseClient for all database operations and LlamaIndex agent creation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CouchbaseClient:\n",
        "    \"\"\"Centralized Couchbase client for all database operations.\"\"\"\n",
        "\n",
        "    def __init__(self, conn_string: str, username: str, password: str, bucket_name: str):\n",
        "        \"\"\"Initialize Couchbase client with connection details.\"\"\"\n",
        "        self.conn_string = conn_string\n",
        "        self.username = username\n",
        "        self.password = password\n",
        "        self.bucket_name = bucket_name\n",
        "        self.cluster = None\n",
        "        self.bucket = None\n",
        "        self._collections = {}\n",
        "\n",
        "    def connect(self):\n",
        "        \"\"\"Establish connection to Couchbase cluster.\"\"\"\n",
        "        try:\n",
        "            auth = PasswordAuthenticator(self.username, self.password)\n",
        "            options = ClusterOptions(auth)\n",
        "\n",
        "            # Use WAN profile for better timeout handling with remote clusters\n",
        "            options.apply_profile(\"wan_development\")\n",
        "            self.cluster = Cluster(self.conn_string, options)\n",
        "            self.cluster.wait_until_ready(timedelta(seconds=20))\n",
        "            logger.info(\"Successfully connected to Couchbase\")\n",
        "            return self.cluster\n",
        "        except Exception as e:\n",
        "            raise ConnectionError(f\"Failed to connect to Couchbase: {e!s}\")\n",
        "\n",
        "    def setup_collection(self, scope_name: str, collection_name: str):\n",
        "        \"\"\"Setup collection - create scope and collection if they don't exist.\"\"\"\n",
        "        try:\n",
        "            # Ensure cluster connection\n",
        "            if not self.cluster:\n",
        "                self.connect()\n",
        "\n",
        "            # For travel-sample bucket, assume it exists\n",
        "            if not self.bucket:\n",
        "                self.bucket = self.cluster.bucket(self.bucket_name)\n",
        "                logger.info(f\"Connected to bucket '{self.bucket_name}'\")\n",
        "\n",
        "            # Setup scope\n",
        "            bucket_manager = self.bucket.collections()\n",
        "            scopes = bucket_manager.get_all_scopes()\n",
        "            scope_exists = any(scope.name == scope_name for scope in scopes)\n",
        "\n",
        "            if not scope_exists and scope_name != \"_default\":\n",
        "                logger.info(f\"Creating scope '{scope_name}'...\")\n",
        "                bucket_manager.create_scope(scope_name)\n",
        "                logger.info(f\"Scope '{scope_name}' created successfully\")\n",
        "\n",
        "            # Setup collection - clear if exists, create if doesn't\n",
        "            collections = bucket_manager.get_all_scopes()\n",
        "            collection_exists = any(\n",
        "                scope.name == scope_name\n",
        "                and collection_name in [col.name for col in scope.collections]\n",
        "                for scope in collections\n",
        "            )\n",
        "\n",
        "            if collection_exists:\n",
        "                logger.info(f\"Collection '{collection_name}' exists, clearing data...\")\n",
        "                # Clear existing data\n",
        "                self.clear_collection_data(scope_name, collection_name)\n",
        "            else:\n",
        "                logger.info(f\"Creating collection '{collection_name}'...\")\n",
        "                bucket_manager.create_collection(scope_name, collection_name)\n",
        "                logger.info(f\"Collection '{collection_name}' created successfully\")\n",
        "\n",
        "            time.sleep(3)\n",
        "\n",
        "            # Create primary index\n",
        "            try:\n",
        "                self.cluster.query(\n",
        "                    f\"CREATE PRIMARY INDEX IF NOT EXISTS ON `{self.bucket_name}`.`{scope_name}`.`{collection_name}`\"\n",
        "                ).execute()\n",
        "                logger.info(\"Primary index created successfully\")\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"Error creating primary index: {e}\")\n",
        "\n",
        "            logger.info(\"Collection setup complete\")\n",
        "            return self.bucket.scope(scope_name).collection(collection_name)\n",
        "\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Error setting up collection: {e!s}\")\n",
        "\n",
        "    def clear_collection_data(self, scope_name: str, collection_name: str):\n",
        "        \"\"\"Clear all data from a collection.\"\"\"\n",
        "        try:\n",
        "            logger.info(f\"Clearing data from {self.bucket_name}.{scope_name}.{collection_name}...\")\n",
        "\n",
        "            # Use N1QL to delete all documents with explicit execution\n",
        "            delete_query = f\"DELETE FROM `{self.bucket_name}`.`{scope_name}`.`{collection_name}`\"\n",
        "            result = self.cluster.query(delete_query)\n",
        "\n",
        "            # Execute the query and get the results\n",
        "            rows = list(result)\n",
        "\n",
        "            # Wait a moment for the deletion to propagate\n",
        "            time.sleep(2)\n",
        "\n",
        "            # Verify collection is empty\n",
        "            count_query = f\"SELECT COUNT(*) as count FROM `{self.bucket_name}`.`{scope_name}`.`{collection_name}`\"\n",
        "            count_result = self.cluster.query(count_query)\n",
        "            count_row = list(count_result)[0]\n",
        "            remaining_count = count_row[\"count\"]\n",
        "\n",
        "            if remaining_count == 0:\n",
        "                logger.info(f\"Collection cleared successfully, {remaining_count} documents remaining\")\n",
        "            else:\n",
        "                logger.warning(f\"Collection clear incomplete, {remaining_count} documents remaining\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Error clearing collection data: {e}\")\n",
        "            # If N1QL fails, try to continue anyway\n",
        "            pass\n",
        "\n",
        "    def get_collection(self, scope_name: str, collection_name: str):\n",
        "        \"\"\"Get a collection object.\"\"\"\n",
        "        key = f\"{scope_name}.{collection_name}\"\n",
        "        if key not in self._collections:\n",
        "            self._collections[key] = self.bucket.scope(scope_name).collection(collection_name)\n",
        "        return self._collections[key]\n",
        "\n",
        "    def setup_vector_search_index(self, index_definition: dict, scope_name: str):\n",
        "        \"\"\"Setup vector search index for the specified scope.\"\"\"\n",
        "        try:\n",
        "            if not self.bucket:\n",
        "                raise RuntimeError(\"Bucket not initialized. Call setup_collection first.\")\n",
        "\n",
        "            scope_index_manager = self.bucket.scope(scope_name).search_indexes()\n",
        "            existing_indexes = scope_index_manager.get_all_indexes()\n",
        "            index_name = index_definition[\"name\"]\n",
        "\n",
        "            if index_name not in [index.name for index in existing_indexes]:\n",
        "                logger.info(f\"Creating vector search index '{index_name}'...\")\n",
        "                search_index = SearchIndex.from_json(index_definition)\n",
        "                scope_index_manager.upsert_index(search_index)\n",
        "                logger.info(f\"Vector search index '{index_name}' created successfully\")\n",
        "            else:\n",
        "                logger.info(f\"Vector search index '{index_name}' already exists\")\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Error setting up vector search index: {e!s}\")\n",
        "\n",
        "    def load_landmark_data(self, scope_name, collection_name, index_name, embeddings):\n",
        "        \"\"\"Load landmark data into Couchbase.\"\"\"\n",
        "        try:\n",
        "            # Import landmark data loading function\n",
        "            sys.path.append(os.path.join(os.path.dirname(__file__), \"data\"))\n",
        "            from landmark_data import load_landmark_data_to_couchbase\n",
        "\n",
        "            # Load landmark data using the data loading script\n",
        "            load_landmark_data_to_couchbase(\n",
        "                cluster=self.cluster,\n",
        "                bucket_name=self.bucket_name,\n",
        "                scope_name=scope_name,\n",
        "                collection_name=collection_name,\n",
        "                embeddings=embeddings,\n",
        "                index_name=index_name,\n",
        "            )\n",
        "            logger.info(\"Landmark data loaded into vector store successfully\")\n",
        "\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Error loading landmark data: {e!s}\")\n",
        "\n",
        "    def setup_vector_store_and_agent(self, catalog, span):\n",
        "        \"\"\"Setup vector store with landmark data and create agent.\"\"\"\n",
        "        # Setup AI services using Priority 1: Capella AI + OpenAI wrappers\n",
        "        embeddings, llm = setup_ai_services(framework=\"llamaindex\", temperature=0.1, application_span=span)\n",
        "        \n",
        "        # Set global LlamaIndex settings\n",
        "        Settings.llm = llm\n",
        "        Settings.embed_model = embeddings\n",
        "        \n",
        "        # Setup collection\n",
        "        self.setup_collection(os.environ[\"CB_SCOPE\"], os.environ[\"CB_COLLECTION\"])\n",
        "        \n",
        "        # Setup vector search index - MUST have agentcatalog_index.json\n",
        "        with open(\"agentcatalog_index.json\") as file:\n",
        "            index_definition = json.load(file)\n",
        "        logger.info(\"Loaded vector search index definition from agentcatalog_index.json\")\n",
        "        self.setup_vector_search_index(index_definition, os.environ[\"CB_SCOPE\"])\n",
        "        \n",
        "        # Load landmark data\n",
        "        self.load_landmark_data(\n",
        "            os.environ[\"CB_SCOPE\"],\n",
        "            os.environ[\"CB_COLLECTION\"],\n",
        "            os.environ[\"CB_INDEX\"],\n",
        "            embeddings,\n",
        "        )\n",
        "        \n",
        "        # Create LlamaIndex ReAct agent\n",
        "        agent = self.create_llamaindex_agent(catalog, span)\n",
        "        \n",
        "        return agent\n",
        "\n",
        "    def create_llamaindex_agent(self, catalog, span):\n",
        "        \"\"\"Create LlamaIndex ReAct agent with landmark search tool from Agent Catalog.\"\"\"\n",
        "        try:\n",
        "            # Get tools from Agent Catalog\n",
        "            tools = []\n",
        "\n",
        "            # Search landmarks tool\n",
        "            search_tool_result = catalog.find(\"tool\", name=\"search_landmarks\")\n",
        "            if search_tool_result:\n",
        "                tools.append(\n",
        "                    FunctionTool.from_defaults(\n",
        "                        fn=search_tool_result.func,\n",
        "                        name=\"search_landmarks\",\n",
        "                        description=getattr(search_tool_result.meta, \"description\", None)\n",
        "                        or \"Search for landmark information using semantic vector search. Use for finding attractions, monuments, museums, parks, and other points of interest.\",\n",
        "                    )\n",
        "                )\n",
        "                logger.info(\"Loaded search_landmarks tool from AgentC\")\n",
        "\n",
        "            if not tools:\n",
        "                logger.warning(\"No tools found in Agent Catalog\")\n",
        "            else:\n",
        "                logger.info(f\"Loaded {len(tools)} tools from Agent Catalog\")\n",
        "\n",
        "            # Get prompt from Agent Catalog - REQUIRED, no fallbacks\n",
        "            prompt_result = catalog.find(\"prompt\", name=\"landmark_search_assistant\")\n",
        "            if not prompt_result:\n",
        "                raise RuntimeError(\"Prompt 'landmark_search_assistant' not found in Agent Catalog\")\n",
        "\n",
        "            # Try different possible attributes for the prompt content\n",
        "            system_prompt = (\n",
        "                getattr(prompt_result, \"content\", None)\n",
        "                or getattr(prompt_result, \"template\", None)\n",
        "                or getattr(prompt_result, \"text\", None)\n",
        "            )\n",
        "            if not system_prompt:\n",
        "                raise RuntimeError(\n",
        "                    \"Could not access prompt content from AgentC - prompt content is None or empty\"\n",
        "                )\n",
        "\n",
        "            logger.info(\"Loaded system prompt from Agent Catalog\")\n",
        "\n",
        "            # Create ReAct agent with limits to prevent excessive iterations\n",
        "            agent = ReActAgent.from_tools(\n",
        "                tools=tools,\n",
        "                llm=Settings.llm,\n",
        "                verbose=True,\n",
        "                system_prompt=system_prompt,\n",
        "                max_iterations=12,\n",
        "            )\n",
        "\n",
        "            logger.info(\"LlamaIndex ReAct agent created successfully\")\n",
        "            return agent\n",
        "\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Error creating LlamaIndex agent: {e!s}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Standalone Agent Creation Function\n",
        "\n",
        "Standalone version of the agent creation function for compatibility with main.py structure.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_llamaindex_agent(catalog, span):\n",
        "    \"\"\"Create LlamaIndex ReAct agent with landmark search tool from Agent Catalog.\"\"\"\n",
        "    try:\n",
        "        from llama_index.core.agent import ReActAgent\n",
        "        from llama_index.core.tools import FunctionTool\n",
        "\n",
        "        # Get tools from Agent Catalog\n",
        "        tools = []\n",
        "\n",
        "        # Search landmarks tool\n",
        "        search_tool_result = catalog.find(\"tool\", name=\"search_landmarks\")\n",
        "        if search_tool_result:\n",
        "            tools.append(\n",
        "                FunctionTool.from_defaults(\n",
        "                    fn=search_tool_result.func,\n",
        "                    name=\"search_landmarks\",\n",
        "                    description=getattr(search_tool_result.meta, \"description\", None)\n",
        "                    or \"Search for landmark information using semantic vector search. Use for finding attractions, monuments, museums, parks, and other points of interest.\",\n",
        "                )\n",
        "            )\n",
        "            logger.info(\"Loaded search_landmarks tool from AgentC\")\n",
        "\n",
        "        if not tools:\n",
        "            logger.warning(\"No tools found in Agent Catalog\")\n",
        "        else:\n",
        "            logger.info(f\"Loaded {len(tools)} tools from Agent Catalog\")\n",
        "\n",
        "        # Get prompt from Agent Catalog - REQUIRED, no fallbacks\n",
        "        prompt_result = catalog.find(\"prompt\", name=\"landmark_search_assistant\")\n",
        "        if not prompt_result:\n",
        "            raise RuntimeError(\"Prompt 'landmark_search_assistant' not found in Agent Catalog\")\n",
        "\n",
        "        # Try different possible attributes for the prompt content\n",
        "        system_prompt = (\n",
        "            getattr(prompt_result, \"content\", None)\n",
        "            or getattr(prompt_result, \"template\", None)\n",
        "            or getattr(prompt_result, \"text\", None)\n",
        "        )\n",
        "        if not system_prompt:\n",
        "            raise RuntimeError(\n",
        "                \"Could not access prompt content from AgentC - prompt content is None or empty\"\n",
        "            )\n",
        "\n",
        "        logger.info(\"Loaded system prompt from Agent Catalog\")\n",
        "\n",
        "        # Create ReAct agent with reasonable iteration limit\n",
        "        agent = ReActAgent.from_tools(\n",
        "            tools=tools,\n",
        "            llm=Settings.llm,\n",
        "            verbose=True,  # Keep on for debugging\n",
        "            system_prompt=system_prompt,\n",
        "            max_iterations=4,  # Allow one tool call and finalization without warnings\n",
        "        )\n",
        "\n",
        "        logger.info(\"LlamaIndex ReAct agent created successfully\")\n",
        "        return agent\n",
        "\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Error creating LlamaIndex agent: {e!s}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Data Loading Module\n",
        "\n",
        "Complete landmark data loading functions from data/landmark_data.py - inline for self-contained operation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data loading functions from data/landmark_data.py\n",
        "import couchbase.auth\n",
        "import couchbase.cluster\n",
        "import couchbase.exceptions\n",
        "import couchbase.options\n",
        "from llama_index.core import Document\n",
        "from llama_index.core.ingestion import IngestionPipeline\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.vector_stores.couchbase import CouchbaseSearchVectorStore\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def get_cluster_connection():\n",
        "    \"\"\"Get a fresh cluster connection for each request.\"\"\"\n",
        "    try:\n",
        "        auth = couchbase.auth.PasswordAuthenticator(\n",
        "            username=os.environ[\"CB_USERNAME\"],\n",
        "            password=os.environ[\"CB_PASSWORD\"],\n",
        "        )\n",
        "        options = couchbase.options.ClusterOptions(authenticator=auth)\n",
        "        # Use WAN profile for better timeout handling with remote clusters\n",
        "        options.apply_profile(\"wan_development\")\n",
        "\n",
        "        cluster = couchbase.cluster.Cluster(\n",
        "            os.environ[\"CB_CONN_STRING\"], options\n",
        "        )\n",
        "        cluster.wait_until_ready(timedelta(seconds=15))\n",
        "        return cluster\n",
        "    except couchbase.exceptions.CouchbaseException as e:\n",
        "        logger.error(f\"Could not connect to Couchbase cluster: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def load_landmark_data_from_travel_sample():\n",
        "    \"\"\"Load landmark data from travel-sample.inventory.landmark collection.\"\"\"\n",
        "    try:\n",
        "        cluster = get_cluster_connection()\n",
        "        if not cluster:\n",
        "            raise ConnectionError(\"Could not connect to Couchbase cluster\")\n",
        "\n",
        "        # Query to get all landmark documents from travel-sample.inventory.landmark\n",
        "        query = \\\"\\\"\\\"\n",
        "            SELECT l.*, META(l).id as doc_id\n",
        "            FROM `travel-sample`.inventory.landmark l\n",
        "            ORDER BY l.name\n",
        "        \\\"\\\"\\\"\n",
        "\n",
        "        logger.info(\"Loading landmark data from travel-sample.inventory.landmark...\")\n",
        "        result = cluster.query(query)\n",
        "\n",
        "        landmarks = []\n",
        "        logger.info(\"Processing landmark documents...\")\n",
        "\n",
        "        # Convert to list to get total count for progress bar\n",
        "        landmark_rows = list(result)\n",
        "\n",
        "        # Use tqdm for progress bar\n",
        "        for row in tqdm(landmark_rows, desc=\"Loading landmarks\", unit=\"landmarks\"):\n",
        "            landmark = row\n",
        "            landmarks.append(landmark)\n",
        "\n",
        "        logger.info(f\"Loaded {len(landmarks)} landmarks from travel-sample.inventory.landmark\")\n",
        "        return landmarks\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading landmark data: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "def get_landmark_texts():\n",
        "    \"\"\"Returns formatted landmark texts for vector store embedding from travel-sample data.\"\"\"\n",
        "    landmarks = load_landmark_data_from_travel_sample()\n",
        "    landmark_texts = []\n",
        "\n",
        "    logger.info(\"Generating landmark text embeddings...\")\n",
        "\n",
        "    # Use tqdm for progress bar while processing landmarks\n",
        "    for landmark in tqdm(landmarks, desc=\"Processing landmarks\", unit=\"landmarks\"):\n",
        "        # Start with basic info\n",
        "        name = landmark.get(\"name\", \"Unknown Landmark\")\n",
        "        title = landmark.get(\"title\", name)\n",
        "        city = landmark.get(\"city\", \"Unknown City\")\n",
        "        country = landmark.get(\"country\", \"Unknown Country\")\n",
        "\n",
        "        # Build comprehensive text with all available fields\n",
        "        text_parts = [f\"{title} ({name}) in {city}, {country}\"]\n",
        "\n",
        "        # Add all fields dynamically instead of manual selection\n",
        "        field_mappings = {\n",
        "            \"content\": \"Description\",\n",
        "            \"address\": \"Address\",\n",
        "            \"directions\": \"Directions\",\n",
        "            \"phone\": \"Phone\",\n",
        "            \"tollfree\": \"Toll-free\",\n",
        "            \"email\": \"Email\",\n",
        "            \"url\": \"Website\",\n",
        "            \"hours\": \"Hours\",\n",
        "            \"price\": \"Price\",\n",
        "            \"activity\": \"Activity type\",\n",
        "            \"type\": \"Type\",\n",
        "            \"state\": \"State\",\n",
        "            \"alt\": \"Alternative name\",\n",
        "            \"image\": \"Image\",\n",
        "        }\n",
        "\n",
        "        # Add all available fields\n",
        "        for field, label in field_mappings.items():\n",
        "            value = landmark.get(field)\n",
        "            if value is not None and value != \"\" and value != \"None\":\n",
        "                if isinstance(value, bool):\n",
        "                    text_parts.append(f\"{label}: {'Yes' if value else 'No'}\")\n",
        "                else:\n",
        "                    text_parts.append(f\"{label}: {value}\")\n",
        "\n",
        "        # Add geographic coordinates if available\n",
        "        if landmark.get(\"geo\"):\n",
        "            geo = landmark[\"geo\"]\n",
        "            if geo.get(\"lat\") and geo.get(\"lon\"):\n",
        "                accuracy = geo.get(\"accuracy\", \"Unknown\")\n",
        "                text_parts.append(f\"Coordinates: {geo['lat']}, {geo['lon']} (accuracy: {accuracy})\")\n",
        "\n",
        "        # Add ID for reference\n",
        "        if landmark.get(\"id\"):\n",
        "            text_parts.append(f\"ID: {landmark['id']}\")\n",
        "\n",
        "        # Join all parts with \". \"\n",
        "        text = \". \".join(text_parts)\n",
        "        landmark_texts.append(text)\n",
        "\n",
        "    logger.info(f\"Generated {len(landmark_texts)} landmark text embeddings\")\n",
        "    return landmark_texts\n",
        "\n",
        "\n",
        "def load_landmark_data_to_couchbase(\n",
        "    cluster, bucket_name: str, scope_name: str, collection_name: str, embeddings, index_name: str\n",
        "):\n",
        "    \"\"\"Load landmark data from travel-sample into the target collection with embeddings.\"\"\"\n",
        "    try:\n",
        "        # Check if data already exists\n",
        "        count_query = (\n",
        "            f\"SELECT COUNT(*) as count FROM `{bucket_name}`.`{scope_name}`.`{collection_name}`\"\n",
        "        )\n",
        "        count_result = cluster.query(count_query)\n",
        "        count_row = list(count_result)[0]\n",
        "        existing_count = count_row[\"count\"]\n",
        "\n",
        "        if existing_count > 0:\n",
        "            logger.info(\n",
        "                f\"Found {existing_count} existing documents in collection, skipping data load\"\n",
        "            )\n",
        "            return\n",
        "\n",
        "        # Get the source landmarks from travel-sample\n",
        "        landmarks = load_landmark_data_from_travel_sample()\n",
        "        landmark_texts = get_landmark_texts()\n",
        "\n",
        "        # Setup vector store for the target collection\n",
        "        vector_store = CouchbaseSearchVectorStore(\n",
        "            cluster=cluster,\n",
        "            bucket_name=bucket_name,\n",
        "            scope_name=scope_name,\n",
        "            collection_name=collection_name,\n",
        "            index_name=index_name,\n",
        "        )\n",
        "\n",
        "        # Create LlamaIndex Documents\n",
        "        logger.info(f\"Creating {len(landmark_texts)} LlamaIndex Documents...\")\n",
        "        documents = []\n",
        "        \n",
        "        for i, (landmark, text) in enumerate(zip(landmarks, landmark_texts)):\n",
        "            document = Document(\n",
        "                text=text,\n",
        "                metadata={\n",
        "                    \"landmark_id\": landmark.get(\"id\", f\"landmark_{i}\"),\n",
        "                    \"name\": landmark.get(\"name\", \"Unknown\"),\n",
        "                    \"city\": landmark.get(\"city\", \"Unknown\"),\n",
        "                    \"country\": landmark.get(\"country\", \"Unknown\"),\n",
        "                    \"activity\": landmark.get(\"activity\", \"\"),\n",
        "                    \"type\": landmark.get(\"type\", \"\"),\n",
        "                    # Add the missing fields that search tool expects\n",
        "                    \"address\": landmark.get(\"address\", \"\"),\n",
        "                    \"phone\": landmark.get(\"phone\", \"\"),\n",
        "                    \"url\": landmark.get(\"url\", \"\"),\n",
        "                    \"hours\": landmark.get(\"hours\", \"\"),\n",
        "                    \"price\": landmark.get(\"price\", \"\"),\n",
        "                    \"state\": landmark.get(\"state\", \"\"),\n",
        "                }\n",
        "            )\n",
        "            documents.append(document)\n",
        "\n",
        "        # Use IngestionPipeline to process documents with embeddings\n",
        "        logger.info(f\"Processing documents with ingestion pipeline...\")\n",
        "        pipeline = IngestionPipeline(\n",
        "            transformations=[SentenceSplitter(chunk_size=800, chunk_overlap=100), embeddings],\n",
        "            vector_store=vector_store,\n",
        "        )\n",
        "\n",
        "        # Process documents in batches to avoid memory issues\n",
        "        batch_size = 25  # Well below Capella AI embedding model limit\n",
        "        total_batches = (len(documents) + batch_size - 1) // batch_size\n",
        "\n",
        "        logger.info(f\"Processing {len(documents)} documents in {total_batches} batches...\")\n",
        "        \n",
        "        # Process in batches\n",
        "        for i in tqdm(\n",
        "            range(0, len(documents), batch_size),\n",
        "            desc=\"Loading batches\",\n",
        "            unit=\"batch\",\n",
        "            total=total_batches,\n",
        "        ):\n",
        "            batch = documents[i : i + batch_size]\n",
        "            pipeline.run(documents=batch)\n",
        "\n",
        "        logger.info(\n",
        "            f\"Successfully loaded {len(documents)} landmark documents to vector store\"\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading landmark data to Couchbase: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "def get_landmark_count():\n",
        "    \"\"\"Get the count of landmarks in travel-sample.inventory.landmark.\"\"\"\n",
        "    try:\n",
        "        cluster = get_cluster_connection()\n",
        "        if not cluster:\n",
        "            raise ConnectionError(\"Could not connect to Couchbase cluster\")\n",
        "\n",
        "        query = \"SELECT COUNT(*) as count FROM `travel-sample`.inventory.landmark\"\n",
        "        result = cluster.query(query)\n",
        "\n",
        "        for row in result:\n",
        "            return row[\"count\"]\n",
        "\n",
        "        return 0\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error getting landmark count: {str(e)}\")\n",
        "        return 0\n",
        "\n",
        "\n",
        "def get_landmarks_by_city(city: str, limit: int = 10):\n",
        "    \"\"\"Get landmarks for a specific city.\"\"\"\n",
        "    try:\n",
        "        cluster = get_cluster_connection()\n",
        "        if not cluster:\n",
        "            raise ConnectionError(\"Could not connect to Couchbase cluster\")\n",
        "\n",
        "        query = f\\\"\\\"\\\"\n",
        "            SELECT l.*, META(l).id as doc_id\n",
        "            FROM `travel-sample`.inventory.landmark l\n",
        "            WHERE LOWER(l.city) = LOWER('{city}')\n",
        "            ORDER BY l.name\n",
        "            LIMIT {limit}\n",
        "        \\\"\\\"\\\"\n",
        "\n",
        "        result = cluster.query(query)\n",
        "        landmarks = []\n",
        "\n",
        "        for row in result:\n",
        "            landmarks.append(row)\n",
        "\n",
        "        return landmarks\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error getting landmarks by city: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "\n",
        "def get_landmarks_by_activity(activity: str, limit: int = 10):\n",
        "    \"\"\"Get landmarks for a specific activity type.\"\"\"\n",
        "    try:\n",
        "        cluster = get_cluster_connection()\n",
        "        if not cluster:\n",
        "            raise ConnectionError(\"Could not connect to Couchbase cluster\")\n",
        "\n",
        "        query = f\\\"\\\"\\\"\n",
        "            SELECT l.*, META(l).id as doc_id\n",
        "            FROM `travel-sample`.inventory.landmark l\n",
        "            WHERE LOWER(l.activity) = LOWER('{activity}')\n",
        "            ORDER BY l.name\n",
        "            LIMIT {limit}\n",
        "        \\\"\\\"\\\"\n",
        "\n",
        "        result = cluster.query(query)\n",
        "        landmarks = []\n",
        "\n",
        "        for row in result:\n",
        "            landmarks.append(row)\n",
        "\n",
        "        return landmarks\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error getting landmarks by activity: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "\n",
        "def get_landmarks_by_country(country: str, limit: int = 10):\n",
        "    \"\"\"Get landmarks for a specific country.\"\"\"\n",
        "    try:\n",
        "        cluster = get_cluster_connection()\n",
        "        if not cluster:\n",
        "            raise ConnectionError(\"Could not connect to Couchbase cluster\")\n",
        "\n",
        "        query = f\\\"\\\"\\\"\n",
        "            SELECT l.*, META(l).id as doc_id\n",
        "            FROM `travel-sample`.inventory.landmark l\n",
        "            WHERE LOWER(l.country) = LOWER('{country}')\n",
        "            ORDER BY l.name\n",
        "            LIMIT {limit}\n",
        "        \\\"\\\"\\\"\n",
        "\n",
        "        result = cluster.query(query)\n",
        "        landmarks = []\n",
        "\n",
        "        for row in result:\n",
        "            landmarks.append(row)\n",
        "\n",
        "        return landmarks\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error getting landmarks by country: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "\n",
        "def search_landmarks_by_text(search_text: str, limit: int = 10):\n",
        "    \"\"\"Search landmarks by text content.\"\"\"\n",
        "    try:\n",
        "        cluster = get_cluster_connection()\n",
        "        if not cluster:\n",
        "            raise ConnectionError(\"Could not connect to Couchbase cluster\")\n",
        "\n",
        "        query = f\\\"\\\"\\\"\n",
        "            SELECT l.*, META(l).id as doc_id\n",
        "            FROM `travel-sample`.inventory.landmark l\n",
        "            WHERE LOWER(l.name) LIKE LOWER('%{search_text}%')\n",
        "               OR LOWER(l.title) LIKE LOWER('%{search_text}%')\n",
        "               OR LOWER(l.content) LIKE LOWER('%{search_text}%')\n",
        "               OR LOWER(l.address) LIKE LOWER('%{search_text}%')\n",
        "            ORDER BY l.name\n",
        "            LIMIT {limit}\n",
        "        \\\"\\\"\\\"\n",
        "\n",
        "        result = cluster.query(query)\n",
        "        landmarks = []\n",
        "\n",
        "        for row in result:\n",
        "            landmarks.append(row)\n",
        "\n",
        "        return landmarks\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error searching landmarks: {str(e)}\")\n",
        "        return []\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Query Module\n",
        "\n",
        "Complete query collections and functions from data/queries.py - inline for self-contained operation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Query functions and data from data/queries.py\n",
        "from typing import Dict, List\n",
        "\n",
        "# Landmark search queries (based on travel-sample data)\n",
        "LANDMARK_SEARCH_QUERIES = [\n",
        "    \"Find museums and galleries in Glasgow\",  # Art & Culture, Scotland\n",
        "    \"Show me restaurants serving Asian cuisine\",  # Food & Dining, Real Asian restaurants\n",
        "    \"What attractions can I see in Glasgow?\",  # General sightseeing, Scotland\n",
        "    \"Tell me about Monet's House\",  # Specific landmark, France\n",
        "    \"Find places to eat in Gillingham\",  # Food, Real UK town\n",
        "]\n",
        "\n",
        "# Comprehensive reference answers based on ACTUAL agent responses from travel-sample.inventory.landmark data\n",
        "LANDMARK_REFERENCE_ANSWERS = [\n",
        "    # Query 1: Glasgow museums and galleries\n",
        "    \"\"\"Glasgow has several museums and galleries including the Gallery of Modern Art (Glasgow) located at Royal Exchange Square with a terrific collection of recent paintings and sculptures, the Kelvingrove Art Gallery and Museum on Argyle Street with one of the finest civic collections in Europe including works by Van Gogh, Monet and Rembrandt, the Hunterian Museum and Art Gallery at University of Glasgow with a world famous Whistler collection, and the Riverside Museum at 100 Pointhouse Place with an excellent collection of vehicles and transport history. All offer free admission except for special exhibitions.\"\"\",\n",
        "\n",
        "    # Query 2: Asian cuisine restaurants\n",
        "    \"\"\"There are several Asian restaurants available including Shangri-la Chinese Restaurant in Birmingham at 51 Station Street offering good quality Chinese food with spring rolls and sizzling steak, Taiwan Restaurant in San Francisco famous for their dumplings, Hong Kong Seafood Restaurant in San Francisco for sit-down dim sum, Cheung Hing Chinese Restaurant in San Francisco for Cantonese BBQ and roast duck, Vietnam Restaurant in San Francisco for Vietnamese dishes including crab soup and pork sandwich, and various other Chinese and Asian establishments across different locations.\"\"\",\n",
        "\n",
        "    # Query 3: Glasgow attractions\n",
        "    \"\"\"Glasgow attractions include Glasgow Green (founded by Royal grant in 1450) with Nelson's Memorial and the Doulton Fountain, Glasgow University (founded 1451) with neo-Gothic architecture and commanding views, Glasgow Cathedral with fine Gothic architecture from medieval times, the City Chambers in George Square built in 1888 in Italian Renaissance style with guided tours available, Glasgow Central Station with its grand interior, and Kelvingrove Park which is popular with students and contains the Art Gallery and Museum.\"\"\",\n",
        "\n",
        "    # Query 4: Monet's House\n",
        "    \"\"\"Monet's House is located in Giverny, France at 84 rue Claude Monet. The house is quietly eccentric and highly interesting in an Orient-influenced style, featuring Monet's collection of Japanese prints. The main attraction is the gardens around the house, including the water garden with the Japanese bridge, weeping willows and waterlilies which are now iconic. It's open April-October, Monday-Sunday 9:30-18:00, with admission €9 for adults, €5 for students, €4 for disabled visitors, and free for under-7s. E-tickets can be purchased online and wheelchair access is available.\"\"\",\n",
        "\n",
        "    # Query 5: Gillingham restaurants\n",
        "    \"\"\"Gillingham has various dining options including Beijing Inn (Chinese restaurant at 3 King Street), Spice Court (Indian restaurant at 56-58 Balmoral Road opposite the railway station, award-winning with Sunday Buffet for £8.50), Hollywood Bowl (American-style restaurant at 4 High Street with burgers and ribs in a Hollywood-themed setting), Ossie's Fish and Chips (at 75 Richmond Road, known for the best fish and chips in the area), and Thai Won Mien (oriental restaurant at 59-61 High Street with noodles, duck and other oriental dishes).\"\"\",\n",
        "]\n",
        "\n",
        "# Create dictionary for backward compatibility\n",
        "QUERY_REFERENCE_ANSWERS = {\n",
        "    query: answer for query, answer in zip(LANDMARK_SEARCH_QUERIES, LANDMARK_REFERENCE_ANSWERS)\n",
        "}\n",
        "\n",
        "# Category-based queries for testing specific search capabilities (based on real data)\n",
        "CATEGORY_QUERIES = {\n",
        "    \"cultural\": [\n",
        "        \"Find museums and galleries in Glasgow\",\n",
        "        \"Show me historic buildings and architecture\",\n",
        "        \"What art collections can I visit?\",\n",
        "    ],\n",
        "    \"culinary\": [\n",
        "        \"Show me restaurants serving Asian cuisine\",\n",
        "        \"Find places to eat in Gillingham\",\n",
        "        \"What dining options are available?\",\n",
        "    ],\n",
        "    \"sightseeing\": [\n",
        "        \"What attractions can I see in Glasgow?\",\n",
        "        \"Show me historic landmarks and buildings\",\n",
        "        \"Find interesting places to visit\",\n",
        "    ],\n",
        "    \"specific\": [\n",
        "        \"Tell me about Monet's House\",\n",
        "        \"Show me the Glasgow Cathedral\",\n",
        "        \"What can you tell me about the Burrell Collection?\",\n",
        "    ],\n",
        "}\n",
        "\n",
        "# Location-based queries for geographic diversity testing (based on real data)\n",
        "LOCATION_QUERIES = {\n",
        "    \"Scotland\": [\n",
        "        \"Find museums and galleries in Glasgow\",\n",
        "        \"What attractions can I see in Glasgow?\",\n",
        "        \"Show me historic buildings in Glasgow\",\n",
        "    ],\n",
        "    \"England\": [\n",
        "        \"Find places to eat in Gillingham\",\n",
        "        \"Show me restaurants serving Asian cuisine\",\n",
        "        \"What landmarks are in Gillingham?\",\n",
        "    ],\n",
        "    \"France\": [\n",
        "        \"Tell me about Monet's House\",\n",
        "        \"Show me attractions in Giverny\",\n",
        "        \"What can I visit in France?\",\n",
        "    ],\n",
        "    \"UK_General\": [\n",
        "        \"Find attractions in the United Kingdom\",\n",
        "        \"Show me places to visit in the UK\",\n",
        "        \"What can I see in Britain?\",\n",
        "    ],\n",
        "}\n",
        "\n",
        "# Activity-based queries for testing different search patterns\n",
        "ACTIVITY_QUERIES = [\n",
        "    \"What can I see in Glasgow?\",  # 'see' activity queries\n",
        "    \"Where can I eat in Gillingham?\",  # 'eat' activity queries\n",
        "    \"Show me places to dine\",  # Generic eating queries\n",
        "    \"Find things to visit and see\",  # Generic sightseeing queries\n",
        "    \"What museums can I visit?\",  # Specific venue type queries\n",
        "]\n",
        "\n",
        "\n",
        "def get_all_queries() -> List[str]:\n",
        "    \"\"\"Get all queries for comprehensive testing.\"\"\"\n",
        "    all_queries = LANDMARK_SEARCH_QUERIES.copy()\n",
        "\n",
        "    # Add category queries\n",
        "    for category_list in CATEGORY_QUERIES.values():\n",
        "        all_queries.extend(category_list)\n",
        "\n",
        "    # Add location queries\n",
        "    for location_list in LOCATION_QUERIES.values():\n",
        "        all_queries.extend(location_list)\n",
        "\n",
        "    # Add activity queries\n",
        "    all_queries.extend(ACTIVITY_QUERIES)\n",
        "\n",
        "    return all_queries\n",
        "\n",
        "\n",
        "def get_reference_answer(query: str) -> str:\n",
        "    \"\"\"Get reference answer for a specific query.\"\"\"\n",
        "    return QUERY_REFERENCE_ANSWERS.get(query, \"No reference answer available for this query.\")\n",
        "\n",
        "\n",
        "def get_queries_by_category(category: str) -> List[str]:\n",
        "    \"\"\"Get queries filtered by category.\"\"\"\n",
        "    if category == \"basic\":\n",
        "        return LANDMARK_SEARCH_QUERIES\n",
        "    elif category == \"category\":\n",
        "        return [q for queries in CATEGORY_QUERIES.values() for q in queries]\n",
        "    elif category == \"location\":\n",
        "        return [q for queries in LOCATION_QUERIES.values() for q in queries]\n",
        "    elif category == \"activity\":\n",
        "        return ACTIVITY_QUERIES\n",
        "    else:\n",
        "        return get_all_queries()\n",
        "\n",
        "\n",
        "def get_queries_for_evaluation(limit: int = 5) -> List[str]:\n",
        "    \"\"\"Get a subset of queries for evaluation purposes.\"\"\"\n",
        "    return LANDMARK_SEARCH_QUERIES[:limit]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Landmark Search Agent Setup\n",
        "\n",
        "Setup the complete landmark search agent infrastructure using LlamaIndex.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def setup_landmark_agent():\n",
        "    \"\"\"Setup the complete landmark search agent infrastructure and return the agent.\"\"\"\n",
        "    setup_environment()\n",
        "\n",
        "    # Initialize Agent Catalog with credentials\n",
        "    catalog = agentc.Catalog()\n",
        "    span = catalog.Span(name=\"Landmark Search Agent Setup\", blacklist=set())\n",
        "\n",
        "    # Setup LLM and embeddings\n",
        "    embeddings, llm = setup_ai_services(framework=\"llamaindex\", temperature=0.1, application_span=span)\n",
        "\n",
        "    # Set global LlamaIndex settings\n",
        "    Settings.llm = llm\n",
        "    Settings.embed_model = embeddings\n","\n",
        "\n",
        "    # Setup database client\n",
        "    client = CouchbaseClient(\n",
        "        conn_string=os.environ[\"CB_CONN_STRING\"],\n",
        "        username=os.environ[\"CB_USERNAME\"],\n",
        "        password=os.environ[\"CB_PASSWORD\"],\n",
        "        bucket_name=os.environ[\"CB_BUCKET\"],\n",
        "    )\n",
        "\n",
        "    client.connect()\n",
        "\n",
        "    # Setup vector store and agent\n",
        "    agent = client.setup_vector_store_and_agent(catalog, span)\n",
        "\n",
        "    return agent, client\n",
        "\n",
        "\n",
        "# Inline evaluation templates for lenient evaluation\n",
        "LENIENT_QA_PROMPT_TEMPLATE = \"\"\"\n",
        "You are an expert evaluator assessing if an AI assistant's response correctly answers the user's question about landmarks and attractions.\n",
        "\n",
        "FOCUS ON FUNCTIONAL SUCCESS, NOT EXACT MATCHING:\n",
        "1. Did the agent provide the requested landmark information?\n",
        "2. Is the core information accurate and helpful to the user?\n",
        "3. Would the user be satisfied with what they received?\n",
        "\n",
        "DYNAMIC DATA IS EXPECTED AND CORRECT:\n",
        "- Landmark search results vary based on current database state\n",
        "- Different search queries may return different but valid landmarks\n",
        "- Order of results may vary (this is normal for search results)\n",
        "- Formatting differences are acceptable\n",
        "\n",
        "IGNORE THESE DIFFERENCES:\n",
        "- Format differences, duplicate searches, system messages\n",
        "- Different result ordering or landmark selection\n",
        "- Reference mismatches due to dynamic search results\n",
        "\n",
        "MARK AS CORRECT IF:\n",
        "- Agent successfully found landmarks matching the request\n",
        "- User received useful, accurate landmark information\n",
        "- Core functionality worked as expected (search worked, results filtered properly)\n",
        "\n",
        "MARK AS INCORRECT ONLY IF:\n",
        "- Agent completely failed to provide landmark information\n",
        "- Response is totally irrelevant to the landmark search request\n",
        "- Agent provided clearly wrong or nonsensical information\n",
        "\n",
        "**Question:** {input}\n",
        "\n",
        "**Reference Answer:** {reference}\n",
        "\n",
        "**AI Response:** {output}\n",
        "\n",
        "Based on the criteria above, is the AI response correct?\n",
        "\n",
        "Answer: [correct/incorrect]\n",
        "\n",
        "Explanation: [Provide a brief explanation focusing on functional success]\n",
        "\"\"\"\n",
        "\n",
        "# Lenient hallucination evaluation template  \n",
        "LENIENT_HALLUCINATION_PROMPT_TEMPLATE = \"\"\"\n",
        "You are evaluating whether an AI assistant's response about landmarks contains hallucinated (fabricated) information.\n",
        "\n",
        "DYNAMIC DATA IS EXPECTED AND FACTUAL:\n",
        "- Landmark search results are pulled from a real database\n",
        "- Different searches return different valid landmarks (this is correct behavior)\n",
        "- Landmark details like addresses, descriptions, and activities come from actual data\n",
        "- Search result variations are normal and factual\n",
        "\n",
        "MARK AS FACTUAL IF:\n",
        "- Response contains \"iteration limit\" or \"time limit\" (system issue, not hallucination)\n",
        "- Agent provides plausible landmark data from search results\n",
        "- Information is consistent with typical landmark search functionality\n",
        "- Results differ from reference due to dynamic search (this is expected!)\n",
        "\n",
        "ONLY MARK AS HALLUCINATED IF:\n",
        "- Response contains clearly impossible landmark information\n",
        "- Agent makes up fake landmark names, addresses, or details\n",
        "- Response contradicts fundamental facts about landmark search\n",
        "- Agent claims to have data it cannot access\n",
        "\n",
        "REMEMBER: Different search results are EXPECTED dynamic behavior, not hallucinations!\n",
        "\n",
        "**Question:** {input}\n",
        "\n",
        "**Reference Answer:** {reference}\n",
        "\n",
        "**AI Response:** {output}\n",
        "\n",
        "Based on the criteria above, does the response contain hallucinated information?\n",
        "\n",
        "Answer: [factual/hallucinated]\n",
        "\n",
        "Explanation: [Focus on whether information is plausible vs clearly fabricated]\n",
        "\"\"\"\n",
        "\n",
        "# Setup the landmark search agent\n",
        "agent, client = setup_landmark_agent()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Test Functions\n",
        "Define test functions to demonstrate the landmark search agent functionality.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_landmark_query(query: str, agent):\n",
        "    \"\"\"Run a single landmark query with error handling.\"\"\"\n",
        "    logger.info(f\"🏛️ Landmark Query: {query}\")\n",
        "    \n",
        "    try:\n",
        "        # Run the agent with LlamaIndex chat interface\n",
        "        response = agent.chat(query, chat_history=[])\n",
        "        result = response.response\n",
        "        \n",
        "        logger.info(f\"🤖 AI Response: {result}\")\n",
        "        logger.info(\"✅ Query completed successfully\")\n",
        "        \n",
        "        return result\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.exception(f\"❌ Query failed: {e}\")\n",
        "        return f\"Error: {str(e)}\"\n",
        "\n",
        "\n",
        "def test_landmark_data_loading():\n",
        "    \"\"\"Test landmark data loading from travel-sample independently.\"\"\"\n",
        "    logger.info(\"Testing Landmark Data Loading from travel-sample\")\n",
        "    logger.info(\"=\" * 50)\n",
        "    \n",
        "    try:\n",
        "        # Import landmark data functions\n",
        "        sys.path.append(os.path.join(os.path.dirname(__file__), \"data\"))\n",
        "        from landmark_data import get_landmark_count, get_landmark_texts\n",
        "        \n",
        "        # Test landmark count\n",
        "        count = get_landmark_count()\n",
        "        logger.info(f\"✅ Landmark count in travel-sample.inventory.landmark: {count}\")\n",
        "        \n",
        "        # Test landmark text generation\n",
        "        texts = get_landmark_texts()\n",
        "        logger.info(f\"✅ Generated {len(texts)} landmark texts for embeddings\")\n",
        "        \n",
        "        if texts:\n",
        "            logger.info(f\"✅ First landmark text sample: {texts[0][:200]}...\")\n",
        "        \n",
        "        logger.info(\"✅ Data loading test completed successfully\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.exception(f\"❌ Data loading test failed: {e}\")\n",
        "\n",
        "\n",
        "# Test landmark data loading\n",
        "test_landmark_data_loading()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Test 1: Landmarks in Tokyo\n",
        "\n",
        "Search for landmarks and attractions in Tokyo, Japan.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result1 = run_landmark_query(\"Find me landmarks in Tokyo\", agent)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Test 2: Museums in London\n",
        "\n",
        "Search for museums and cultural attractions in London, UK.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result2 = run_landmark_query(\"Show me museums in London\", agent)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "xml"
        }
      },
      "source": [
        "## Test 3: Parks in Paris\n",
        "\n",
        "Search for parks and outdoor spaces in Paris, France.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result3 = run_landmark_query(\"What parks can I visit in Paris?\", agent)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Comprehensive Phoenix Evaluation System\n",
        "\n",
        "Complete Phoenix evaluation system from evals/eval_arize.py - inline for self-contained operation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comprehensive Phoenix evaluation system from evals/eval_arize.py\n",
        "import nest_asyncio\n",
        "import socket\n",
        "import subprocess\n",
        "import warnings\n",
        "from dataclasses import dataclass\n",
        "from datetime import datetime\n",
        "from typing import Any, Dict, List, Optional\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Apply nest_asyncio to handle nested event loops in Jupyter/LlamaIndex\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "@dataclass\n",
        "class EvaluationConfig:\n",
        "    \"\"\"Configuration for the evaluation system.\"\"\"\n",
        "\n",
        "    # Arize Configuration\n",
        "    arize_space_id: str = os.getenv(\"ARIZE_SPACE_ID\", \"default-space\")\n",
        "    arize_api_key: str = os.getenv(\"ARIZE_API_KEY\", \"\")\n",
        "    project_name: str = \"landmark-search-agent-evaluation\"\n",
        "\n",
        "    # Phoenix Configuration\n",
        "    phoenix_base_port: int = 6006\n",
        "    phoenix_grpc_base_port: int = 4317\n",
        "    phoenix_max_port_attempts: int = 5\n",
        "\n",
        "    # Evaluation Configuration\n",
        "    evaluator_model: str = \"gpt-4o\"\n",
        "    max_queries: int = 10\n",
        "    evaluation_timeout: int = 300\n",
        "\n",
        "\n",
        "class PhoenixManager:\n",
        "    \"\"\"Manages Phoenix server lifecycle.\"\"\"\n",
        "\n",
        "    def __init__(self, config: EvaluationConfig):\n",
        "        self.config = config\n",
        "        self.session = None\n",
        "        self.active_port = None\n",
        "\n",
        "    def _is_port_in_use(self, port: int) -> bool:\n",
        "        \"\"\"Check if a port is in use.\"\"\"\n",
        "        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
        "            return s.connect_ex((\"localhost\", port)) == 0\n",
        "\n",
        "    def _kill_existing_phoenix_processes(self) -> None:\n",
        "        \"\"\"Kill any existing Phoenix processes.\"\"\"\n",
        "        try:\n",
        "            subprocess.run([\"pkill\", \"-f\", \"phoenix\"], check=False, capture_output=True)\n",
        "            time.sleep(2)  # Wait for processes to terminate\n",
        "        except Exception as e:\n",
        "            logger.debug(f\"Error killing Phoenix processes: {e}\")\n",
        "\n",
        "    def _find_available_port(self) -> tuple[int, int]:\n",
        "        \"\"\"Find available ports for Phoenix.\"\"\"\n",
        "        phoenix_port = self.config.phoenix_base_port\n",
        "        grpc_port = self.config.phoenix_grpc_base_port\n",
        "\n",
        "        for _ in range(self.config.phoenix_max_port_attempts):\n",
        "            if not self._is_port_in_use(phoenix_port):\n",
        "                return phoenix_port, grpc_port\n",
        "            phoenix_port += 1\n",
        "            grpc_port += 1\n",
        "\n",
        "        raise RuntimeError(\n",
        "            f\"Could not find available ports after {self.config.phoenix_max_port_attempts} attempts\"\n",
        "        )\n",
        "\n",
        "    def start_phoenix(self) -> bool:\n",
        "        \"\"\"Start Phoenix server and return success status.\"\"\"\n",
        "        try:\n",
        "            import phoenix as px\n",
        "            from phoenix.otel import register\n",
        "            from openinference.instrumentation.llama_index import LlamaIndexInstrumentor\n",
        "            \n",
        "            logger.info(\"🔧 Setting up Phoenix observability...\")\n",
        "\n",
        "            # Clean up existing processes\n",
        "            self._kill_existing_phoenix_processes()\n",
        "\n",
        "            # Find available ports\n",
        "            phoenix_port, grpc_port = self._find_available_port()\n",
        "\n",
        "            # Set environment variables\n",
        "            os.environ[\"PHOENIX_PORT\"] = str(phoenix_port)\n",
        "            os.environ[\"PHOENIX_GRPC_PORT\"] = str(grpc_port)\n",
        "\n",
        "            # Start Phoenix session\n",
        "            self.session = px.launch_app()\n",
        "            self.active_port = phoenix_port\n",
        "\n",
        "            if self.session:\n",
        "                logger.info(f\"🌐 Phoenix UI: {self.session.url}\")\n",
        "\n",
        "            # Register Phoenix OTEL for LlamaIndex\n",
        "            register(\n",
        "                project_name=self.config.project_name,\n",
        "                endpoint=f\"http://localhost:{self.active_port}/v1/traces\",\n",
        "            )\n",
        "\n",
        "            # Instrument LlamaIndex specifically\n",
        "            LlamaIndexInstrumentor().instrument()\n",
        "\n",
        "            logger.info(\"✅ Phoenix setup completed successfully\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.exception(f\"❌ Phoenix setup failed: {e}\")\n",
        "            return False\n",
        "\n",
        "    def cleanup(self) -> None:\n",
        "        \"\"\"Clean up Phoenix resources.\"\"\"\n",
        "        try:\n",
        "            if self.session:\n",
        "                # Phoenix session cleanup happens automatically\n",
        "                pass\n",
        "            logger.info(\"🔒 Phoenix cleanup completed\")\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"⚠️ Error during Phoenix cleanup: {e}\")\n",
        "\n",
        "\n",
        "class LandmarkSearchEvaluator:\n",
        "    \"\"\"LlamaIndex-specific evaluator for the landmark search agent.\"\"\"\n",
        "\n",
        "    def __init__(self, config: Optional[EvaluationConfig] = None):\n",
        "        \"\"\"Initialize the evaluator with configuration.\"\"\"\n",
        "        self.config = config or EvaluationConfig()\n",
        "        self.phoenix_manager = PhoenixManager(self.config)\n",
        "\n",
        "        # Agent components\n",
        "        self.agent = None\n",
        "        self.client = None\n",
        "\n",
        "        # Phoenix evaluators\n",
        "        self.evaluator_llm = None\n",
        "\n",
        "        # Add option to bypass Phoenix for debugging\n",
        "        try:\n",
        "            import phoenix as px\n",
        "            if not os.getenv(\"SKIP_PHOENIX\", \"false\").lower() == \"true\":\n",
        "                self._setup_phoenix_evaluators()\n",
        "            elif os.getenv(\"SKIP_PHOENIX\", \"false\").lower() == \"true\":\n",
        "                logger.info(\"🔧 Phoenix setup skipped due to SKIP_PHOENIX=true\")\n",
        "        except ImportError:\n",
        "            logger.warning(\"Phoenix not available - skipping Phoenix setup\")\n",
        "\n",
        "    def _setup_phoenix_evaluators(self) -> None:\n",
        "        \"\"\"Setup Phoenix evaluators for LLM-based evaluation.\"\"\"\n",
        "        try:\n",
        "            from phoenix.evals import OpenAIModel\n",
        "            \n",
        "            self.evaluator_llm = OpenAIModel(model=self.config.evaluator_model)\n",
        "            logger.info(\"✅ Phoenix evaluators initialized\")\n",
        "\n",
        "            # Start Phoenix\n",
        "            if self.phoenix_manager.start_phoenix():\n",
        "                logger.info(\"✅ Phoenix instrumentation enabled for LlamaIndex\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"⚠️ Phoenix evaluators setup failed: {e}\")\n",
        "            self.evaluator_llm = None\n",
        "\n",
        "    def setup_agent(self) -> bool:\n",
        "        \"\"\"Setup landmark search agent using the setup function.\"\"\"\n",
        "        try:\n",
        "            logger.info(\"🔧 Setting up landmark search agent...\")\n",
        "\n",
        "            self.agent, self.client = setup_landmark_agent()\n",
        "\n",
        "            logger.info(\"✅ Landmark search agent setup completed successfully\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.exception(f\"❌ Error setting up landmark search agent: {e}\")\n",
        "            return False\n",
        "\n",
        "    def _extract_response_content(self, result: Any) -> str:\n",
        "        \"\"\"Extract clean response content from LlamaIndex agent result.\"\"\"\n",
        "        try:\n",
        "            # Prefer explicit response field\n",
        "            if hasattr(result, \"response\"):\n",
        "                response_content = str(result.response).strip()\n",
        "                if response_content and not response_content.lower().startswith(\"error:\"):\n",
        "                    return response_content\n",
        "\n",
        "            # Some LlamaIndex results may carry a .message or .output\n",
        "            for attr in (\"message\", \"output\", \"final_response\"):\n",
        "                if hasattr(result, attr):\n",
        "                    text = str(getattr(result, attr)).strip()\n",
        "                    if text:\n",
        "                        return text\n",
        "\n",
        "            # Last resort fallback\n",
        "            text = str(result).strip()\n",
        "            return text if text else \"\"\n",
        "                \n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Error extracting response content: {e}\")\n",
        "            return f\"Error extracting response: {e}\"\n",
        "\n",
        "    def run_single_evaluation(self, query: str) -> Dict[str, Any]:\n",
        "        \"\"\"Run evaluation for a single query using LlamaIndex agent.\"\"\"\n",
        "        if not self.agent:\n",
        "            raise RuntimeError(\"Agent not initialized. Call setup_agent() first.\")\n",
        "\n",
        "        logger.info(f\"🔍 Evaluating query: {query}\")\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        try:\n",
        "            # Use LlamaIndex .chat() method\n",
        "            result = self.agent.chat(query, chat_history=[])\n",
        "\n",
        "            # Extract response content\n",
        "            response = self._extract_response_content(result)\n",
        "\n",
        "            # Create evaluation result\n",
        "            evaluation_result = {\n",
        "                \"query\": query,\n",
        "                \"response\": response,\n",
        "                \"execution_time\": time.time() - start_time,\n",
        "                \"success\": True,\n",
        "                \"sources\": [],\n",
        "                \"num_sources\": 0,\n",
        "            }\n",
        "\n",
        "            logger.info(f\"✅ Query completed in {evaluation_result['execution_time']:.2f}s\")\n",
        "\n",
        "            return evaluation_result\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.exception(f\"❌ Query failed: {e}\")\n",
        "            return {\n",
        "                \"query\": query,\n",
        "                \"response\": f\"Error: {str(e)}\",\n",
        "                \"execution_time\": time.time() - start_time,\n",
        "                \"success\": False,\n",
        "                \"error\": str(e),\n",
        "                \"sources\": [],\n",
        "                \"num_sources\": 0,\n",
        "            }\n",
        "\n",
        "    def run_phoenix_evaluations(self, results_df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Run Phoenix evaluations on the results.\"\"\"\n",
        "        if not self.evaluator_llm:\n",
        "            logger.warning(\"⚠️ Phoenix evaluators not available - skipping LLM evaluations\")\n",
        "            return results_df\n",
        "\n",
        "        logger.info(f\"🧠 Running Phoenix evaluations on {len(results_df)} responses...\")\n",
        "\n",
        "        try:\n",
        "            from phoenix.evals import (\n",
        "                RAG_RELEVANCY_PROMPT_TEMPLATE,\n",
        "                RAG_RELEVANCY_PROMPT_RAILS_MAP,\n",
        "                TOXICITY_PROMPT_TEMPLATE,\n",
        "                TOXICITY_PROMPT_RAILS_MAP,\n",
        "                llm_classify,\n",
        "            )\n",
        "            \n",
        "            # Prepare evaluation data\n",
        "            evaluation_data = []\n",
        "            for _, row in results_df.iterrows():\n",
        "                query = row[\"query\"]\n",
        "                response = row[\"response\"]\n",
        "\n",
        "                # Get reference answer for this query\n",
        "                reference = self._get_reference_answer(str(query))\n",
        "\n",
        "                evaluation_data.append(\n",
        "                    {\n",
        "                        \"input\": query,\n",
        "                        \"output\": response,\n",
        "                        \"reference\": reference,\n",
        "                        \"context\": \"Landmark search results\",\n",
        "                        \"text\": response,  # For toxicity evaluation\n",
        "                    }\n",
        "                )\n",
        "\n",
        "            eval_df = pd.DataFrame(evaluation_data)\n",
        "\n",
        "            # Run individual Phoenix evaluations\n",
        "            evaluations = {\n",
        "                \"relevance\": {\n",
        "                    \"template\": RAG_RELEVANCY_PROMPT_TEMPLATE,\n",
        "                    \"rails\": list(RAG_RELEVANCY_PROMPT_RAILS_MAP.values()),\n",
        "                    \"data_cols\": [\"input\", \"reference\"],\n",
        "                },\n",
        "                \"qa_correctness\": {\n",
        "                    \"template\": LENIENT_QA_PROMPT_TEMPLATE,\n",
        "                    \"rails\": [\"correct\", \"incorrect\"],\n",
        "                    \"data_cols\": [\"input\", \"output\", \"reference\"],\n",
        "                },\n",
        "                \"hallucination\": {\n",
        "                    \"template\": LENIENT_HALLUCINATION_PROMPT_TEMPLATE,\n",
        "                    \"rails\": [\"factual\", \"hallucinated\"],\n",
        "                    \"data_cols\": [\"input\", \"reference\", \"output\"],\n",
        "                },\n",
        "                \"toxicity\": {\n",
        "                    \"template\": TOXICITY_PROMPT_TEMPLATE,\n",
        "                    \"rails\": list(TOXICITY_PROMPT_RAILS_MAP.values()),\n",
        "                    \"data_cols\": [\"text\"],\n",
        "                },\n",
        "            }\n",
        "\n",
        "            for eval_name, eval_config in evaluations.items():\n",
        "                try:\n",
        "                    logger.info(f\"   📊 Running {eval_name} evaluation...\")\n",
        "\n",
        "                    # Prepare data for this evaluator\n",
        "                    data = eval_df[eval_config[\"data_cols\"]].copy()\n",
        "\n",
        "                    # Run evaluation\n",
        "                    eval_results = llm_classify(\n",
        "                        dataframe=data,\n",
        "                        model=self.evaluator_llm,\n",
        "                        template=eval_config[\"template\"],\n",
        "                        rails=eval_config[\"rails\"],\n",
        "                    )\n",
        "\n",
        "                    # Process results\n",
        "                    if hasattr(eval_results, \"label\"):\n",
        "                        results_df[eval_name] = eval_results.label.tolist()\n",
        "                    else:\n",
        "                        results_df[eval_name] = \"unknown\"\n",
        "\n",
        "                except Exception as e:\n",
        "                    logger.warning(f\"⚠️ {eval_name} evaluation failed: {e}\")\n",
        "                    results_df[eval_name] = \"error\"\n",
        "\n",
        "            logger.info(\"✅ Phoenix evaluations completed\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.exception(f\"❌ Error running Phoenix evaluations: {e}\")\n",
        "            # Add error indicators\n",
        "            for eval_type in [\"relevance\", \"qa_correctness\", \"hallucination\", \"toxicity\"]:\n",
        "                results_df[eval_type] = \"error\"\n",
        "\n",
        "        return results_df\n",
        "\n",
        "    def _get_reference_answer(self, query: str) -> str:\n",
        "        \"\"\"Get reference answer for evaluation.\"\"\"\n",
        "        try:\n",
        "            reference_answer = get_reference_answer(query)\n",
        "\n",
        "            if reference_answer.startswith(\"No reference answer available\"):\n",
        "                # Create a basic reference based on query\n",
        "                if \"museum\" in query.lower() or \"gallery\" in query.lower():\n",
        "                    return \"Should provide information about museums and galleries with accurate names, addresses, and descriptions.\"\n",
        "                elif \"restaurant\" in query.lower() or \"food\" in query.lower():\n",
        "                    return \"Should provide information about restaurants and food establishments.\"\n",
        "                else:\n",
        "                    return \"Should provide relevant and accurate landmark information.\"\n",
        "\n",
        "            return reference_answer\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Could not get reference answer for '{query}': {e}\")\n",
        "            return \"Should provide relevant and accurate landmark information.\"\n",
        "\n",
        "    def run_evaluation(self, queries: List[str]) -> pd.DataFrame:\n",
        "        \"\"\"Run complete evaluation pipeline.\"\"\"\n",
        "        if not self.setup_agent():\n",
        "            raise RuntimeError(\"Failed to setup agent\")\n",
        "\n",
        "        # Limit queries if specified\n",
        "        if len(queries) > self.config.max_queries:\n",
        "            queries = queries[: self.config.max_queries]\n",
        "            logger.info(f\"Limited to {self.config.max_queries} queries for evaluation\")\n",
        "\n",
        "        logger.info(\n",
        "            f\"🚀 Starting LlamaIndex landmark search evaluation with {len(queries)} queries\"\n",
        "        )\n",
        "\n",
        "        # Run queries\n",
        "        results = []\n",
        "        for i, query in enumerate(queries, 1):\n",
        "            logger.info(f\"\\n📋 Query {i}/{len(queries)}\")\n",
        "            result = self.run_single_evaluation(query)\n",
        "            results.append(result)\n",
        "\n",
        "        # Create results DataFrame\n",
        "        results_df = pd.DataFrame(results)\n",
        "\n",
        "        # Run Phoenix evaluations\n",
        "        results_df = self.run_phoenix_evaluations(results_df)\n",
        "\n",
        "        # Log summary\n",
        "        self._log_evaluation_summary(results_df)\n",
        "\n",
        "        return results_df\n",
        "\n",
        "    def _log_evaluation_summary(self, results_df: pd.DataFrame) -> None:\n",
        "        \"\"\"Log evaluation summary.\"\"\"\n",
        "        logger.info(\"\\n📊 Evaluation Summary:\")\n",
        "        logger.info(f\"  Total queries: {len(results_df)}\")\n",
        "        logger.info(f\"  Successful executions: {results_df['success'].sum()}\")\n",
        "        logger.info(f\"  Failed executions: {(~results_df['success']).sum()}\")\n",
        "        logger.info(f\"  Average execution time: {results_df['execution_time'].mean():.2f}s\")\n",
        "\n",
        "    def cleanup(self) -> None:\n",
        "        \"\"\"Clean up all resources.\"\"\"\n",
        "        self.phoenix_manager.cleanup()\n",
        "\n",
        "\n",
        "def get_default_queries() -> List[str]:\n",
        "    \"\"\"Get default test queries for evaluation.\"\"\"\n",
        "    return get_queries_for_evaluation(limit=10)\n",
        "\n",
        "\n",
        "def run_comprehensive_evaluation() -> pd.DataFrame:\n",
        "    \"\"\"Run comprehensive evaluation with all Phoenix evaluators.\"\"\"\n",
        "    evaluator = LandmarkSearchEvaluator()\n",
        "    try:\n",
        "        queries = get_default_queries()\n",
        "        results = evaluator.run_evaluation(queries)\n",
        "        logger.info(\"\\n✅ Comprehensive landmark search evaluation complete!\")\n",
        "        return results\n",
        "    finally:\n",
        "        evaluator.cleanup()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Demo and Test Functions\n",
        "\n",
        "Interactive demo and test functions from main.py - inline for self-contained operation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demo and test functions from main.py\n",
        "\n",
        "def run_interactive_demo():\n",
        "    \"\"\"Run an interactive landmark search demo.\"\"\"\n",
        "    logger.info(\"Landmark Search Agent - Interactive Demo\")\n",
        "    logger.info(\"=\" * 50)\n",
        "\n",
        "    try:\n",
        "        agent, client = setup_landmark_agent()\n",
        "\n",
        "        # Interactive landmark search loop\n",
        "        logger.info(\"Available commands:\")\n",
        "        logger.info(\"- Enter landmark search queries (e.g., 'Find landmarks in Paris')\")\n",
        "        logger.info(\"- 'quit' - Exit the demo\")\n",
        "        logger.info(\"Try asking: 'Find me landmarks in Tokyo' or 'Show me museums in London'\")\n",
        "        logger.info(\"─\" * 40)\n",
        "\n",
        "        while True:\n",
        "            query = input(\"🔍 Enter landmark search query (or 'quit' to exit): \").strip()\n",
        "\n",
        "            if query.lower() in [\"quit\", \"exit\", \"q\"]:\n",
        "                logger.info(\"Thanks for using Landmark Search Agent!\")\n",
        "                break\n",
        "\n",
        "            if not query:\n",
        "                logger.warning(\"Please enter a search query\")\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                response = agent.chat(query, chat_history=[])\n",
        "                result = response.response\n",
        "\n",
        "                logger.info(f\"\\n🏛️ Agent Response:\\n{result}\\n\")\n",
        "                logger.info(\"─\" * 40)\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error processing query: {e}\")\n",
        "                logger.error(f\"❌ Error: {e}\")\n",
        "                logger.info(\"─\" * 40)\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        logger.info(\"Demo interrupted by user\")\n",
        "    except Exception as e:\n",
        "        logger.exception(f\"Demo error: {e}\")\n",
        "    finally:\n",
        "        logger.info(\"Demo completed\")\n",
        "\n",
        "\n",
        "def run_test():\n",
        "    \"\"\"Run comprehensive test of landmark search agent with queries from queries.py.\"\"\"\n",
        "    logger.info(\"Landmark Search Agent - Comprehensive Test Suite\")\n",
        "    logger.info(\"=\" * 55)\n",
        "\n",
        "    try:\n",
        "        agent, client = setup_landmark_agent()\n",
        "\n",
        "        # Test scenarios covering different types of landmark searches\n",
        "        test_queries = get_queries_for_evaluation()\n",
        "\n",
        "        logger.info(f\"Running {len(test_queries)} test queries...\")\n",
        "\n",
        "        for i, query in enumerate(test_queries, 1):\n",
        "            logger.info(f\"\\n🔍 Test {i}: {query}\")\n",
        "            try:\n",
        "                response = agent.chat(query, chat_history=[])\n",
        "                result = response.response\n",
        "\n",
        "                # Display the response\n",
        "                logger.info(f\"🤖 AI Response: {result}\")\n",
        "                logger.info(f\"✅ Test {i} completed successfully\")\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.exception(f\"❌ Test {i} failed: {e}\")\n",
        "\n",
        "            logger.info(\"-\" * 50)\n",
        "\n",
        "        logger.info(\"All tests completed!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.exception(f\"Test error: {e}\")\n",
        "\n",
        "\n",
        "def run_landmark_data_test():\n",
        "    \"\"\"Test landmark data loading and querying functions.\"\"\"\n",
        "    logger.info(\"Landmark Data Test Suite\")\n",
        "    logger.info(\"=\" * 30)\n",
        "    \n",
        "    try:\n",
        "        # Test landmark count\n",
        "        count = get_landmark_count()\n",
        "        logger.info(f\"✅ Landmark count in travel-sample.inventory.landmark: {count}\")\n",
        "        \n",
        "        # Test city search\n",
        "        logger.info(\"\\nTesting city search for 'London':\")\n",
        "        london_landmarks = get_landmarks_by_city(\"London\", 3)\n",
        "        for landmark in london_landmarks:\n",
        "            logger.info(f\"- {landmark.get('name', 'Unknown')} in {landmark.get('city', 'Unknown')}\")\n",
        "        \n",
        "        # Test activity search\n",
        "        logger.info(\"\\nTesting activity search for 'see':\")\n",
        "        see_landmarks = get_landmarks_by_activity(\"see\", 3)\n",
        "        for landmark in see_landmarks:\n",
        "            logger.info(f\"- {landmark.get('name', 'Unknown')} ({landmark.get('activity', 'Unknown')})\")\n",
        "        \n",
        "        # Test text search\n",
        "        logger.info(\"\\nTesting text search for 'museum':\")\n",
        "        museum_landmarks = search_landmarks_by_text(\"museum\", 3)\n",
        "        for landmark in museum_landmarks:\n",
        "            logger.info(f\"- {landmark.get('name', 'Unknown')} in {landmark.get('city', 'Unknown')}\")\n",
        "            \n",
        "        logger.info(\"\\n✅ All landmark data tests completed successfully!\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.exception(f\"❌ Landmark data test error: {e}\")\n",
        "\n",
        "\n",
        "# Quick demo functions for notebook use\n",
        "def run_demo_queries():\n",
        "    \"\"\"Run a few demo queries for quick testing.\"\"\"\n",
        "    demo_queries = [\n",
        "        \"Find museums and galleries in Glasgow\",\n",
        "        \"Tell me about Monet's House\",\n",
        "        \"What attractions can I see in Glasgow?\"\n",
        "    ]\n",
        "    \n",
        "    agent, client = setup_landmark_agent()\n",
        "    \n",
        "    for i, query in enumerate(demo_queries, 1):\n",
        "        logger.info(f\"\\n🔍 Demo Query {i}: {query}\")\n",
        "        try:\n",
        "            response = agent.chat(query, chat_history=[])\n",
        "            logger.info(f\"🏛️ Response: {response.response}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"❌ Error: {e}\")\n",
        "        logger.info(\"-\" * 50)\n",
        "\n",
        "\n",
        "# Run comprehensive evaluation (uses the Phoenix evaluator above)\n",
        "def run_evaluation():\n",
        "    \"\"\"Run the comprehensive Phoenix evaluation.\"\"\"\n",
        "    return run_comprehensive_evaluation()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Arize Phoenix Evaluation\n",
        "\n",
        "This section demonstrates how to evaluate the landmark search agent using Arize Phoenix observability platform. The evaluation includes:\n",
        "\n",
        "- **Relevance Scoring**: Using Phoenix RelevanceEvaluator to score how relevant responses are to queries\n",
        "- **QA Scoring**: Using Phoenix QAEvaluator to score answer quality\n",
        "- **Hallucination Detection**: Using Phoenix HallucinationEvaluator to detect fabricated information  \n",
        "- **Toxicity Detection**: Using Phoenix ToxicityEvaluator to detect harmful content\n",
        "- **Phoenix UI**: Real-time observability dashboard at `http://localhost:6006/`\n",
        "\n",
        "We'll run landmark search queries and evaluate the responses for quality and safety using LlamaIndex instrumentation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import Phoenix evaluation components\n",
        "try:\n",
        "    import phoenix as px\n",
        "    from openinference.instrumentation.llama_index import LlamaIndexInstrumentor\n",
        "    from phoenix.evals import (\n",
        "        HALLUCINATION_PROMPT_RAILS_MAP,\n",
        "        HALLUCINATION_PROMPT_TEMPLATE,\n",
        "        QA_PROMPT_RAILS_MAP,\n",
        "        QA_PROMPT_TEMPLATE,\n",
        "        RAG_RELEVANCY_PROMPT_RAILS_MAP,\n",
        "        RAG_RELEVANCY_PROMPT_TEMPLATE,\n",
        "        TOXICITY_PROMPT_RAILS_MAP,\n",
        "        TOXICITY_PROMPT_TEMPLATE,\n",
        "        OpenAIModel,\n",
        "        llm_classify,\n",
        "    )\n",
        "    from phoenix.otel import register\n",
        "    import pandas as pd\n",
        "    \n",
        "    ARIZE_AVAILABLE = True\n",
        "    logger.info(\"✅ Arize Phoenix evaluation components available\")\n",
        "except ImportError as e:\n",
        "    logger.warning(f\"Arize dependencies not available: {e}\")\n",
        "    logger.warning(\"Skipping evaluation section...\")\n",
        "    ARIZE_AVAILABLE = False\n",
        "\n",
        "if ARIZE_AVAILABLE:\n",
        "    # Start Phoenix session for observability\n",
        "    try:\n",
        "        px.launch_app(port=6006)\n",
        "        logger.info(\"🚀 Phoenix UI available at http://localhost:6006/\")\n",
        "        \n",
        "        # Register LlamaIndex instrumentation\n",
        "        tracer_provider = register(\n",
        "            project_name=\"landmark-search-agent-evaluation\",\n",
        "            endpoint=\"http://localhost:6006/v1/traces\"\n",
        "        )\n",
        "        \n",
        "        # Instrument LlamaIndex\n",
        "        LlamaIndexInstrumentor().instrument(tracer_provider=tracer_provider)\n",
        "        logger.info(\"✅ LlamaIndex instrumentation enabled\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.warning(f\"Could not start Phoenix UI: {e}\")\n",
        "\n",
        "    # Demo queries for evaluation\n",
        "    landmark_demo_queries = [\n",
        "        \"Find me landmarks in Tokyo\",\n",
        "        \"Show me museums in London\"\n",
        "    ]\n",
        "    \n",
        "    # Run demo queries and collect responses for evaluation\n",
        "    landmark_demo_results = []\n",
        "    \n",
        "    for i, query in enumerate(landmark_demo_queries, 1):\n",
        "        try:\n",
        "            logger.info(f\"🔍 Running evaluation query {i}: {query}\")\n",
        "            \n",
        "            # Run the agent with LlamaIndex\n",
        "            response = agent.chat(query, chat_history=[])\n",
        "            output = response.response\n",
        "    \n",
        "            landmark_demo_results.append({\n",
        "                \"query\": query,\n",
        "                \"response\": output,\n",
        "                \"query_type\": f\"landmark_demo_{i}\",\n",
        "                \"success\": True\n",
        "            })\n",
        "            \n",
        "            logger.info(f\"✅ Query {i} completed successfully\")\n",
        "    \n",
        "        except Exception as e:\n",
        "            logger.exception(f\"❌ Query {i} failed: {e}\")\n",
        "            landmark_demo_results.append({\n",
        "                \"query\": query,\n",
        "                \"response\": f\"Error: {e!s}\",\n",
        "                \"query_type\": f\"landmark_demo_{i}\",\n",
        "                \"success\": False\n",
        "            })\n",
        "    \n",
        "    # Convert to DataFrame for evaluation\n",
        "    landmark_results_df = pd.DataFrame(landmark_demo_results)\n",
        "    logger.info(f\"📊 Collected {len(landmark_results_df)} responses for evaluation\")\n",
        "    \n",
        "    # Display results summary\n",
        "    for _, row in landmark_results_df.iterrows():\n",
        "        logger.info(f\"Query: {row['query']}\")\n",
        "        logger.info(f\"Response: {row['response'][:200]}...\")\n",
        "        logger.info(f\"Success: {row['success']}\")\n",
        "        logger.info(\"-\" * 50)\n",
        "    \n",
        "    logger.info(\"💡 Visit Phoenix UI at http://localhost:6006/ to see detailed traces and evaluations\")\n",
        "    logger.info(\"💡 Use the evaluation script at evals/eval_arize.py for comprehensive evaluation\")\n",
        "\n",
        "else:\n",
        "    logger.info(\"Arize evaluation not available - install phoenix-evals to enable evaluation\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if ARIZE_AVAILABLE and len(landmark_demo_results) > 0:\n",
        "    logger.info(\"🔍 Running comprehensive Phoenix evaluations...\")\n",
        "    \n",
        "    # Setup evaluator LLM (using OpenAI for consistency)\n",
        "    evaluator_llm = OpenAIModel(model=\"gpt-4o\", temperature=0.1)\n",
        "    \n",
        "    # Prepare evaluation data with proper column names for Phoenix evaluators\n",
        "    landmark_eval_data = []\n",
        "    for _, row in landmark_results_df.iterrows():\n",
        "        landmark_eval_data.append({\n",
        "            \"input\": row[\"query\"],\n",
        "            \"output\": row[\"response\"],\n",
        "            \"reference\": \"A helpful and accurate response about landmarks with specific location information and practical details\",\n",
        "            \"text\": row[\"response\"]  # For toxicity evaluation\n",
        "        })\n",
        "    \n",
        "    landmark_eval_df = pd.DataFrame(landmark_eval_data)\n",
        "    \n",
        "    try:\n",
        "        # 1. Relevance Evaluation\n",
        "        logger.info(\"🔍 Running Relevance Evaluation...\")\n",
        "        landmark_relevance_results = llm_classify(\n",
        "            dataframe=landmark_eval_df[[\"input\", \"reference\"]],\n",
        "            model=evaluator_llm,\n",
        "            template=RAG_RELEVANCY_PROMPT_TEMPLATE,\n",
        "            rails=list(RAG_RELEVANCY_PROMPT_RAILS_MAP.values())\n",
        "        )\n",
        "        \n",
        "        logger.info(\"✅ Relevance Evaluation Results:\")\n",
        "        for i, result in enumerate(landmark_relevance_results):\n",
        "            query = landmark_eval_data[i][\"input\"]\n",
        "            logger.info(f\"   Query: {query}\")\n",
        "            logger.info(f\"   Relevance: {result.label}\")\n",
        "            logger.info(\"   \" + \"-\"*30)\n",
        "        \n",
        "        # 2. QA Evaluation\n",
        "        logger.info(\"🔍 Running QA Evaluation...\")\n",
        "        landmark_qa_results = llm_classify(\n",
        "            dataframe=landmark_eval_df[[\"input\", \"output\", \"reference\"]],\n",
        "            model=evaluator_llm,\n",
        "            template=QA_PROMPT_TEMPLATE,\n",
        "            rails=list(QA_PROMPT_RAILS_MAP.values())\n",
        "        )\n",
        "        \n",
        "        logger.info(\"✅ QA Evaluation Results:\")\n",
        "        for i, result in enumerate(landmark_qa_results):\n",
        "            query = landmark_eval_data[i][\"input\"]\n",
        "            logger.info(f\"   Query: {query}\")\n",
        "            logger.info(f\"   QA Score: {result.label}\")\n",
        "            logger.info(\"   \" + \"-\"*30)\n",
        "        \n",
        "        # 3. Hallucination Evaluation\n",
        "        logger.info(\"🔍 Running Hallucination Evaluation...\")\n",
        "        landmark_hallucination_results = llm_classify(\n",
        "            dataframe=landmark_eval_df[[\"input\", \"reference\", \"output\"]],\n",
        "            model=evaluator_llm,\n",
        "            template=HALLUCINATION_PROMPT_TEMPLATE,\n",
        "            rails=list(HALLUCINATION_PROMPT_RAILS_MAP.values())\n",
        "        )\n",
        "        \n",
        "        logger.info(\"✅ Hallucination Evaluation Results:\")\n",
        "        for i, result in enumerate(landmark_hallucination_results):\n",
        "            query = landmark_eval_data[i][\"input\"]\n",
        "            logger.info(f\"   Query: {query}\")\n",
        "            logger.info(f\"   Hallucination: {result.label}\")\n",
        "            logger.info(\"   \" + \"-\"*30)\n",
        "        \n",
        "        # 4. Toxicity Evaluation\n",
        "        logger.info(\"🔍 Running Toxicity Evaluation...\")\n",
        "        landmark_toxicity_results = llm_classify(\n",
        "            dataframe=landmark_eval_df[[\"text\"]],\n",
        "            model=evaluator_llm,\n",
        "            template=TOXICITY_PROMPT_TEMPLATE,\n",
        "            rails=list(TOXICITY_PROMPT_RAILS_MAP.values())\n",
        "        )\n",
        "        \n",
        "        logger.info(\"✅ Toxicity Evaluation Results:\")\n",
        "        for i, result in enumerate(landmark_toxicity_results):\n",
        "            query = landmark_eval_data[i][\"input\"]\n",
        "            logger.info(f\"   Query: {query}\")\n",
        "            logger.info(f\"   Toxicity: {result.label}\")\n",
        "            logger.info(\"   \" + \"-\"*30)\n",
        "        \n",
        "        # Summary of all evaluations\n",
        "        logger.info(\"📊 EVALUATION SUMMARY\")\n",
        "        logger.info(\"=\" * 50)\n",
        "        \n",
        "        for i, query in enumerate([item[\"input\"] for item in landmark_eval_data]):\n",
        "            logger.info(f\"Query {i+1}: {query}\")\n",
        "            logger.info(f\"  Relevance: {landmark_relevance_results[i].label}\")\n",
        "            logger.info(f\"  QA Score: {landmark_qa_results[i].label}\")\n",
        "            logger.info(f\"  Hallucination: {landmark_hallucination_results[i].label}\")\n",
        "            logger.info(f\"  Toxicity: {landmark_toxicity_results[i].label}\")\n",
        "            logger.info(\"  \" + \"-\"*40)\n",
        "        \n",
        "        logger.info(\"✅ All Phoenix evaluations completed successfully!\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.exception(f\"❌ Phoenix evaluation failed: {e}\")\n",
        "        logger.info(\"💡 This might be due to API rate limits or model availability\")\n",
        "        \n",
        "else:\n",
        "    if not ARIZE_AVAILABLE:\n",
        "        logger.info(\"❌ Phoenix evaluations skipped - Arize dependencies not available\")\n",
        "    else:\n",
        "        logger.info(\"❌ Phoenix evaluations skipped - No demo results to evaluate\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrates a complete landmark search agent implementation using:\n",
        "\n",
        "1. **Agent Catalog Integration**: Using agentc to find tools and prompts\n",
        "2. **LlamaIndex Framework**: ReAct agent pattern with semantic search capabilities\n",
        "3. **Couchbase Vector Store**: Storing and searching landmark data from travel-sample bucket\n",
        "4. **NVIDIA NIMs + Capella AI**: NVIDIA NIMs for LLM, Capella AI for embeddings\n",
        "5. **Single Tool Architecture**: Focused on `search_landmarks` for landmark discovery\n",
        "6. **Comprehensive Evaluation**: Phoenix-based evaluation with LlamaIndex instrumentation\n",
        "\n",
        "The agent can handle various landmark-related queries including:\n",
        "- Landmark search by location (Tokyo, London, Paris)\n",
        "- Finding specific types of attractions (museums, parks, monuments)\n",
        "- Cultural and historical site discovery\n",
        "- Tourist attraction recommendations\n",
        "\n",
        "## Phoenix Evaluation Metrics\n",
        "\n",
        "The notebook demonstrates all four key Phoenix evaluation types:\n",
        "\n",
        "1. **Relevance Evaluation**: Measures how relevant responses are to landmark queries\n",
        "2. **QA Evaluation**: Assesses the quality and accuracy of landmark information\n",
        "3. **Hallucination Detection**: Identifies fabricated or incorrect landmark information\n",
        "4. **Toxicity Detection**: Screens for harmful or inappropriate content\n",
        "\n",
        "Each evaluation provides:\n",
        "- Binary or categorical labels (e.g., \"relevant\"/\"irrelevant\", \"correct\"/\"incorrect\")\n",
        "- Detailed explanations of the evaluation reasoning\n",
        "- Confidence scores for the assessments\n",
        "\n",
        "## Key Features\n",
        "\n",
        "This landmark search agent implementation:\n",
        "- **Uses LlamaIndex**: Advanced RAG framework with ReAct agent pattern\n",
        "- **Uses travel-sample bucket**: Leverages existing Couchbase landmark data\n",
        "- **NVIDIA NIMs integration**: High-performance LLM inference\n",
        "- **Capella AI embeddings**: High-quality vector embeddings for semantic search\n",
        "- **OpenAI fallback**: Graceful fallback when Capella AI is unavailable\n",
        "- **Single focused tool**: Simplified architecture with one search tool\n",
        "- **Comprehensive evaluation**: Full Phoenix evaluation pipeline\n",
        "- **LlamaIndex instrumentation**: Integrated observability and tracing\n",
        "\n",
        "## Data Source\n",
        "\n",
        "The agent uses landmark data from the `travel-sample.inventory.landmark` collection, which contains:\n",
        "- Real landmark information with names, locations, and descriptions\n",
        "- Structured data with address, city, country, and type information\n",
        "- Rich text descriptions suitable for vector embedding\n",
        "- Global coverage of tourist attractions and points of interest\n",
        "\n",
        "## Architecture Differences\n",
        "\n",
        "This landmark search agent differs from the other agents:\n",
        "- **LlamaIndex** (not LangChain or LangGraph) - advanced RAG framework\n",
        "- **NVIDIA NIMs LLM**: High-performance inference instead of OpenAI/Capella LLM\n",
        "- **ReAct Pattern**: Built-in reasoning and action capabilities\n",
        "- **Landmark-specific**: Optimized for tourism and travel use cases\n",
        "- **Global Settings**: Uses LlamaIndex global settings for LLM and embeddings\n",
        "\n",
        "For production use, consider:\n",
        "- Setting up proper monitoring with Arize Phoenix\n",
        "- Implementing comprehensive evaluation pipelines\n",
        "- Adding error handling and retry logic\n",
        "- Scaling the vector store for larger datasets\n",
        "- Adding more sophisticated query understanding\n",
        "\n",
        "## Usage Instructions\n",
        "\n",
        "To run this notebook:\n",
        "1. Set up the required environment variables (Couchbase connection, API keys)\n",
        "2. Install dependencies: `pip install -r requirements.txt`\n",
        "3. Ensure travel-sample bucket is available in your Couchbase cluster\n",
        "4. Publish your agent catalog: `agentc index . && agentc publish`\n",
        "5. Run the notebook cells sequentially\n",
        "\n",
        "The agent will automatically load landmark data from travel-sample and create embeddings for semantic search capabilities. NVIDIA API key is required for LLM functionality.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
