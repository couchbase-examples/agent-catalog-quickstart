{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Landmark Search Agent Tutorial - Priority 1 Implementation\n",
        "\n",
        "This notebook demonstrates the Agent Catalog landmark search agent using LlamaIndex with Couchbase vector store and Arize Phoenix evaluation. Uses Priority 1 AI services with standard OpenAI wrappers and Capella (simple & fast).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Setup and Imports\n",
        "\n",
        "Import all necessary modules for the landmark search agent using self-contained setup.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import base64\n",
        "import getpass\n",
        "import httpx\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "from datetime import timedelta\n",
        "\n",
        "import agentc\n",
        "import dotenv\n",
        "from couchbase.auth import PasswordAuthenticator\n",
        "from couchbase.cluster import Cluster\n",
        "from couchbase.management.buckets import BucketType, CreateBucketSettings\n",
        "from couchbase.management.search import SearchIndex\n",
        "from couchbase.options import ClusterOptions\n",
        "from llama_index.core import Settings\n",
        "from llama_index.core.agent import ReActAgent\n",
        "from llama_index.core.tools import FunctionTool\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.llms.nvidia import NVIDIA\n",
        "from llama_index.llms.openai_like import OpenAILike\n",
        "from llama_index.vector_stores.couchbase import CouchbaseSearchVectorStore\n",
        "from pydantic import SecretStr\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Reduce noise from various libraries during embedding/vector operations\n",
        "logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n",
        "logging.getLogger(\"httpcore\").setLevel(logging.WARNING)\n",
        "logging.getLogger(\"urllib3\").setLevel(logging.WARNING)\n",
        "\n",
        "# Load environment variables\n",
        "dotenv.load_dotenv(override=True)\n",
        "\n",
        "# Set default values for travel-sample bucket configuration\n",
        "DEFAULT_BUCKET = \"travel-sample\"\n",
        "DEFAULT_SCOPE = \"agentc_data\"\n",
        "DEFAULT_COLLECTION = \"landmark_data\"\n",
        "DEFAULT_INDEX = \"landmark_data_index\"\n",
        "DEFAULT_CAPELLA_API_EMBEDDING_MODEL = \"Snowflake/snowflake-arctic-embed-l-v2.0\"\n",
        "DEFAULT_CAPELLA_API_LLM_MODEL = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
        "DEFAULT_NVIDIA_API_LLM_MODEL = \"meta/llama-3.1-70b-instruct\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Self-Contained Setup Functions\n",
        "\n",
        "Define all necessary setup functions inline for a self-contained notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def setup_environment():\n",
        "    \"\"\"Setup default environment variables for agent operations.\"\"\"\n",
        "    defaults = {\n",
        "        \"CB_BUCKET\": \"travel-sample\",\n",
        "        \"CB_SCOPE\": \"agentc_data\",\n",
        "        \"CB_COLLECTION\": \"landmark_data\",\n",
        "        \"CB_INDEX\": \"landmark_data_index\",\n",
        "        \"NVIDIA_API_EMBEDDING_MODEL\": \"nvidia/nv-embedqa-e5-v5\",\n",
        "        \"NVIDIA_API_LLM_MODEL\": \"meta/llama-3.1-70b-instruct\",\n",
        "        \"CAPELLA_API_EMBEDDING_MODEL\": \"nvidia/nv-embedqa-e5-v5\",\n",
        "        \"CAPELLA_API_LLM_MODEL\": \"meta-llama/Llama-3.1-8B-Instruct\",\n",
        "    }\n",
        "    \n",
        "    for key, value in defaults.items():\n",
        "        if not os.getenv(key):\n",
        "            os.environ[key] = value\n",
        "    \n",
        "    logger.info(\"✅ Environment variables configured\")\n",
        "\n",
        "\n",
        "def test_capella_connectivity(api_key: str = None, endpoint: str = None) -> bool:\n",
        "    \"\"\"Test connectivity to Capella AI services.\"\"\"\n",
        "    try:\n",
        "        test_key = api_key or os.getenv(\"CAPELLA_API_EMBEDDINGS_KEY\") or os.getenv(\"CAPELLA_API_LLM_KEY\")\n",
        "        test_endpoint = endpoint or os.getenv(\"CAPELLA_API_ENDPOINT\")\n",
        "        \n",
        "        if not test_key or not test_endpoint:\n",
        "            return False\n",
        "        \n",
        "        # Simple connectivity test\n",
        "        headers = {\"Authorization\": f\"Bearer {test_key}\"}\n",
        "        \n",
        "        with httpx.Client(timeout=10.0) as client:\n",
        "            response = client.get(f\"{test_endpoint.rstrip('/')}/v1/models\", headers=headers)\n",
        "            return response.status_code < 500\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"⚠️ Capella connectivity test failed: {e}\")\n",
        "        return False\n",
        "\n",
        "\n",
        "def setup_ai_services(framework: str = \"llamaindex\", temperature: float = 0.0, application_span=None):\n",
        "    \"\"\"Priority 1: Capella AI with OpenAI wrappers (simple & fast) for LlamaIndex.\"\"\"\n",
        "    embeddings = None\n",
        "    llm = None\n",
        "    \n",
        "    logger.info(f\"🔧 Setting up Priority 1 AI services for {framework} framework...\")\n",
        "    \n",
        "    # Priority 1: Capella AI with direct API keys and OpenAI wrappers\n",
        "    if not embeddings and os.getenv(\"CAPELLA_API_ENDPOINT\") and os.getenv(\"CAPELLA_API_EMBEDDINGS_KEY\"):\n",
        "        try:\n",
        "            embeddings = OpenAIEmbedding(\n",
        "                api_key=os.getenv(\"CAPELLA_API_EMBEDDINGS_KEY\"),\n",
        "                api_base=f\"{os.getenv('CAPELLA_API_ENDPOINT')}/v1\",\n",
        "                model_name=os.getenv(\"CAPELLA_API_EMBEDDING_MODEL\"),\n",
        "                embed_batch_size=30,\n",
        "                # Note: LlamaIndex doesn't need check_embedding_ctx_length=False\n",
        "            )\n",
        "            logger.info(\"✅ Using Priority 1: Capella AI embeddings (OpenAI wrapper)\")\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"⚠️ Priority 1 Capella AI embeddings failed: {e}\")\n",
        "    \n",
        "    if not llm and os.getenv(\"CAPELLA_API_ENDPOINT\") and os.getenv(\"CAPELLA_API_LLM_KEY\"):\n",
        "        try:\n",
        "            llm = OpenAILike(\n",
        "                model=os.getenv(\"CAPELLA_API_LLM_MODEL\"),\n",
        "                api_base=f\"{os.getenv('CAPELLA_API_ENDPOINT')}/v1\",\n",
        "                api_key=os.getenv(\"CAPELLA_API_LLM_KEY\"),\n",
        "                is_chat_model=True,\n",
        "                temperature=temperature,\n",
        "            )\n",
        "            # Test the LLM works\n",
        "            test_response = llm.complete(\"Hello\")\n",
        "            logger.info(\"✅ Using Priority 1: Capella AI LLM (OpenAI wrapper)\")\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"⚠️ Priority 1 Capella AI LLM failed: {e}\")\n",
        "            llm = None\n",
        "    \n",
        "    # Fallback: OpenAI\n",
        "    if not embeddings and os.getenv(\"OPENAI_API_KEY\"):\n",
        "        try:\n",
        "            embeddings = OpenAIEmbedding(\n",
        "                model_name=\"text-embedding-3-small\",\n",
        "                api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
        "            )\n",
        "            logger.info(\"✅ Using OpenAI embeddings fallback\")\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"⚠️ OpenAI embeddings failed: {e}\")\n",
        "    \n",
        "    if not llm and os.getenv(\"OPENAI_API_KEY\"):\n",
        "        try:\n",
        "            llm = OpenAILike(\n",
        "                model=\"gpt-4o\",\n",
        "                api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
        "                is_chat_model=True,\n",
        "                temperature=temperature,\n",
        "            )\n",
        "            logger.info(\"✅ Using OpenAI LLM fallback\")\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"⚠️ OpenAI LLM failed: {e}\")\n",
        "    \n",
        "    if not embeddings:\n",
        "        raise ValueError(\"❌ No embeddings service could be initialized\")\n",
        "    if not llm:\n",
        "        raise ValueError(\"❌ No LLM service could be initialized\")\n",
        "    \n",
        "    logger.info(f\"✅ Priority 1 AI services setup completed for {framework}\")\n",
        "    return embeddings, llm\n",
        "\n",
        "\n",
        "# Setup environment\n",
        "setup_environment()\n",
        "\n",
        "# Test Capella AI connectivity if configured\n",
        "if os.getenv(\"CAPELLA_API_ENDPOINT\"):\n",
        "    if not test_capella_connectivity():\n",
        "        logger.warning(\"❌ Capella AI connectivity test failed. Will use fallback models.\")\n",
        "else:\n",
        "    logger.info(\"ℹ️ Capella API not configured - will use fallback models\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## CouchbaseClient Class\n",
        "\n",
        "Define the CouchbaseClient for all database operations and LlamaIndex agent creation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CouchbaseClient:\n",
        "    \"\"\"Centralized Couchbase client for all database operations.\"\"\"\n",
        "\n",
        "    def __init__(self, conn_string: str, username: str, password: str, bucket_name: str):\n",
        "        \"\"\"Initialize Couchbase client with connection details.\"\"\"\n",
        "        self.conn_string = conn_string\n",
        "        self.username = username\n",
        "        self.password = password\n",
        "        self.bucket_name = bucket_name\n",
        "        self.cluster = None\n",
        "        self.bucket = None\n",
        "        self._collections = {}\n",
        "\n",
        "    def connect(self):\n",
        "        \"\"\"Establish connection to Couchbase cluster.\"\"\"\n",
        "        try:\n",
        "            auth = PasswordAuthenticator(self.username, self.password)\n",
        "            options = ClusterOptions(auth)\n",
        "\n",
        "            # Use WAN profile for better timeout handling with remote clusters\n",
        "            options.apply_profile(\"wan_development\")\n",
        "            self.cluster = Cluster(self.conn_string, options)\n",
        "            self.cluster.wait_until_ready(timedelta(seconds=20))\n",
        "            logger.info(\"Successfully connected to Couchbase\")\n",
        "            return self.cluster\n",
        "        except Exception as e:\n",
        "            raise ConnectionError(f\"Failed to connect to Couchbase: {e!s}\")\n",
        "\n",
        "    def setup_collection(self, scope_name: str, collection_name: str):\n",
        "        \"\"\"Setup collection - create scope and collection if they don't exist.\"\"\"\n",
        "        try:\n",
        "            # Ensure cluster connection\n",
        "            if not self.cluster:\n",
        "                self.connect()\n",
        "\n",
        "            # For travel-sample bucket, assume it exists\n",
        "            if not self.bucket:\n",
        "                self.bucket = self.cluster.bucket(self.bucket_name)\n",
        "                logger.info(f\"Connected to bucket '{self.bucket_name}'\")\n",
        "\n",
        "            # Setup scope\n",
        "            bucket_manager = self.bucket.collections()\n",
        "            scopes = bucket_manager.get_all_scopes()\n",
        "            scope_exists = any(scope.name == scope_name for scope in scopes)\n",
        "\n",
        "            if not scope_exists and scope_name != \"_default\":\n",
        "                logger.info(f\"Creating scope '{scope_name}'...\")\n",
        "                bucket_manager.create_scope(scope_name)\n",
        "                logger.info(f\"Scope '{scope_name}' created successfully\")\n",
        "\n",
        "            # Setup collection - clear if exists, create if doesn't\n",
        "            collections = bucket_manager.get_all_scopes()\n",
        "            collection_exists = any(\n",
        "                scope.name == scope_name\n",
        "                and collection_name in [col.name for col in scope.collections]\n",
        "                for scope in collections\n",
        "            )\n",
        "\n",
        "            if collection_exists:\n",
        "                logger.info(f\"Collection '{collection_name}' exists, clearing data...\")\n",
        "                # Clear existing data\n",
        "                self.clear_collection_data(scope_name, collection_name)\n",
        "            else:\n",
        "                logger.info(f\"Creating collection '{collection_name}'...\")\n",
        "                bucket_manager.create_collection(scope_name, collection_name)\n",
        "                logger.info(f\"Collection '{collection_name}' created successfully\")\n",
        "\n",
        "            time.sleep(3)\n",
        "\n",
        "            # Create primary index\n",
        "            try:\n",
        "                self.cluster.query(\n",
        "                    f\"CREATE PRIMARY INDEX IF NOT EXISTS ON `{self.bucket_name}`.`{scope_name}`.`{collection_name}`\"\n",
        "                ).execute()\n",
        "                logger.info(\"Primary index created successfully\")\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"Error creating primary index: {e}\")\n",
        "\n",
        "            logger.info(\"Collection setup complete\")\n",
        "            return self.bucket.scope(scope_name).collection(collection_name)\n",
        "\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Error setting up collection: {e!s}\")\n",
        "\n",
        "    def clear_collection_data(self, scope_name: str, collection_name: str):\n",
        "        \"\"\"Clear all data from a collection.\"\"\"\n",
        "        try:\n",
        "            logger.info(f\"Clearing data from {self.bucket_name}.{scope_name}.{collection_name}...\")\n",
        "\n",
        "            # Use N1QL to delete all documents with explicit execution\n",
        "            delete_query = f\"DELETE FROM `{self.bucket_name}`.`{scope_name}`.`{collection_name}`\"\n",
        "            result = self.cluster.query(delete_query)\n",
        "\n",
        "            # Execute the query and get the results\n",
        "            rows = list(result)\n",
        "\n",
        "            # Wait a moment for the deletion to propagate\n",
        "            time.sleep(2)\n",
        "\n",
        "            # Verify collection is empty\n",
        "            count_query = f\"SELECT COUNT(*) as count FROM `{self.bucket_name}`.`{scope_name}`.`{collection_name}`\"\n",
        "            count_result = self.cluster.query(count_query)\n",
        "            count_row = list(count_result)[0]\n",
        "            remaining_count = count_row[\"count\"]\n",
        "\n",
        "            if remaining_count == 0:\n",
        "                logger.info(f\"Collection cleared successfully, {remaining_count} documents remaining\")\n",
        "            else:\n",
        "                logger.warning(f\"Collection clear incomplete, {remaining_count} documents remaining\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Error clearing collection data: {e}\")\n",
        "            # If N1QL fails, try to continue anyway\n",
        "            pass\n",
        "\n",
        "    def get_collection(self, scope_name: str, collection_name: str):\n",
        "        \"\"\"Get a collection object.\"\"\"\n",
        "        key = f\"{scope_name}.{collection_name}\"\n",
        "        if key not in self._collections:\n",
        "            self._collections[key] = self.bucket.scope(scope_name).collection(collection_name)\n",
        "        return self._collections[key]\n",
        "\n",
        "    def setup_vector_search_index(self, index_definition: dict, scope_name: str):\n",
        "        \"\"\"Setup vector search index for the specified scope.\"\"\"\n",
        "        try:\n",
        "            if not self.bucket:\n",
        "                raise RuntimeError(\"Bucket not initialized. Call setup_collection first.\")\n",
        "\n",
        "            scope_index_manager = self.bucket.scope(scope_name).search_indexes()\n",
        "            existing_indexes = scope_index_manager.get_all_indexes()\n",
        "            index_name = index_definition[\"name\"]\n",
        "\n",
        "            if index_name not in [index.name for index in existing_indexes]:\n",
        "                logger.info(f\"Creating vector search index '{index_name}'...\")\n",
        "                search_index = SearchIndex.from_json(index_definition)\n",
        "                scope_index_manager.upsert_index(search_index)\n",
        "                logger.info(f\"Vector search index '{index_name}' created successfully\")\n",
        "            else:\n",
        "                logger.info(f\"Vector search index '{index_name}' already exists\")\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Error setting up vector search index: {e!s}\")\n",
        "\n",
        "    def load_landmark_data(self, scope_name, collection_name, index_name, embeddings):\n",
        "        \"\"\"Load landmark data into Couchbase.\"\"\"\n",
        "        try:\n",
        "            # Import landmark data loading function\n",
        "            sys.path.append(os.path.join(os.path.dirname(__file__), \"data\"))\n",
        "            from landmark_data import load_landmark_data_to_couchbase\n",
        "\n",
        "            # Load landmark data using the data loading script\n",
        "            load_landmark_data_to_couchbase(\n",
        "                cluster=self.cluster,\n",
        "                bucket_name=self.bucket_name,\n",
        "                scope_name=scope_name,\n",
        "                collection_name=collection_name,\n",
        "                embeddings=embeddings,\n",
        "                index_name=index_name,\n",
        "            )\n",
        "            logger.info(\"Landmark data loaded into vector store successfully\")\n",
        "\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Error loading landmark data: {e!s}\")\n",
        "\n",
        "    def setup_vector_store_and_agent(self, catalog, span):\n",
        "        \"\"\"Setup vector store with landmark data and create agent.\"\"\"\n",
        "        # Setup AI services using Priority 1: Capella AI + OpenAI wrappers\n",
        "        embeddings, llm = setup_ai_services(framework=\"llamaindex\", temperature=0.1, application_span=span)\n",
        "        \n",
        "        # Set global LlamaIndex settings\n",
        "        Settings.llm = llm\n",
        "        Settings.embed_model = embeddings\n",
        "        \n",
        "        # Setup collection\n",
        "        self.setup_collection(os.environ[\"CB_SCOPE\"], os.environ[\"CB_COLLECTION\"])\n",
        "        \n",
        "        # Setup vector search index\n",
        "        try:\n",
        "            with open(\"agentcatalog_index.json\") as file:\n",
        "                index_definition = json.load(file)\n",
        "            logger.info(\"Loaded vector search index definition from agentcatalog_index.json\")\n",
        "            self.setup_vector_search_index(index_definition, os.environ[\"CB_SCOPE\"])\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Error loading index definition: {e!s}\")\n",
        "            logger.info(\"Continuing without vector search index...\")\n",
        "        \n",
        "        # Load landmark data\n",
        "        self.load_landmark_data(\n",
        "            os.environ[\"CB_SCOPE\"],\n",
        "            os.environ[\"CB_COLLECTION\"],\n",
        "            os.environ[\"CB_INDEX\"],\n",
        "            embeddings,\n",
        "        )\n",
        "        \n",
        "        # Create LlamaIndex ReAct agent\n",
        "        agent = self.create_llamaindex_agent(catalog, span)\n",
        "        \n",
        "        return agent\n",
        "\n",
        "    def create_llamaindex_agent(self, catalog, span):\n",
        "        \"\"\"Create LlamaIndex ReAct agent with landmark search tool from Agent Catalog.\"\"\"\n",
        "        try:\n",
        "            # Get tools from Agent Catalog\n",
        "            tools = []\n",
        "\n",
        "            # Search landmarks tool\n",
        "            search_tool_result = catalog.find(\"tool\", name=\"search_landmarks\")\n",
        "            if search_tool_result:\n",
        "                tools.append(\n",
        "                    FunctionTool.from_defaults(\n",
        "                        fn=search_tool_result.func,\n",
        "                        name=\"search_landmarks\",\n",
        "                        description=getattr(search_tool_result.meta, \"description\", None)\n",
        "                        or \"Search for landmark information using semantic vector search. Use for finding attractions, monuments, museums, parks, and other points of interest.\",\n",
        "                    )\n",
        "                )\n",
        "                logger.info(\"Loaded search_landmarks tool from AgentC\")\n",
        "\n",
        "            if not tools:\n",
        "                logger.warning(\"No tools found in Agent Catalog\")\n",
        "            else:\n",
        "                logger.info(f\"Loaded {len(tools)} tools from Agent Catalog\")\n",
        "\n",
        "            # Get prompt from Agent Catalog - REQUIRED, no fallbacks\n",
        "            prompt_result = catalog.find(\"prompt\", name=\"landmark_search_assistant\")\n",
        "            if not prompt_result:\n",
        "                raise RuntimeError(\"Prompt 'landmark_search_assistant' not found in Agent Catalog\")\n",
        "\n",
        "            # Try different possible attributes for the prompt content\n",
        "            system_prompt = (\n",
        "                getattr(prompt_result, \"content\", None)\n",
        "                or getattr(prompt_result, \"template\", None)\n",
        "                or getattr(prompt_result, \"text\", None)\n",
        "            )\n",
        "            if not system_prompt:\n",
        "                raise RuntimeError(\n",
        "                    \"Could not access prompt content from AgentC - prompt content is None or empty\"\n",
        "                )\n",
        "\n",
        "            logger.info(\"Loaded system prompt from Agent Catalog\")\n",
        "\n",
        "            # Create ReAct agent with limits to prevent excessive iterations\n",
        "            agent = ReActAgent.from_tools(\n",
        "                tools=tools,\n",
        "                llm=Settings.llm,\n",
        "                verbose=True,\n",
        "                system_prompt=system_prompt,\n",
        "                max_iterations=12,\n",
        "            )\n",
        "\n",
        "            logger.info(\"LlamaIndex ReAct agent created successfully\")\n",
        "            return agent\n",
        "\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Error creating LlamaIndex agent: {e!s}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Landmark Search Agent Setup\n",
        "\n",
        "Setup the complete landmark search agent infrastructure using LlamaIndex.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def setup_landmark_agent():\n",
        "    \"\"\"Setup the complete landmark search agent infrastructure and return the agent.\"\"\"\n",
        "    # Initialize Agent Catalog\n",
        "    catalog = agentc.Catalog()\n",
        "    span = catalog.Span(name=\"Landmark Search Agent Setup\")\n",
        "\n",
        "    # Setup database client\n",
        "    client = CouchbaseClient(\n",
        "        conn_string=os.environ[\"CB_CONN_STRING\"],\n",
        "        username=os.environ[\"CB_USERNAME\"],\n",
        "        password=os.environ[\"CB_PASSWORD\"],\n",
        "        bucket_name=os.environ[\"CB_BUCKET\"],\n",
        "    )\n",
        "\n",
        "    client.connect()\n",
        "\n",
        "    # Setup vector store and agent\n",
        "    agent = client.setup_vector_store_and_agent(catalog, span)\n",
        "\n",
        "    return agent, client\n",
        "\n",
        "\n",
        "# Inline evaluation templates for lenient evaluation\n",
        "LENIENT_QA_PROMPT_TEMPLATE = \"\"\"\n",
        "You are an expert evaluator assessing if an AI assistant's response correctly answers the user's question about landmarks and attractions.\n",
        "\n",
        "FOCUS ON FUNCTIONAL SUCCESS, NOT EXACT MATCHING:\n",
        "1. Did the agent provide the requested landmark information?\n",
        "2. Is the core information accurate and helpful to the user?\n",
        "3. Would the user be satisfied with what they received?\n",
        "\n",
        "DYNAMIC DATA IS EXPECTED AND CORRECT:\n",
        "- Landmark search results vary based on current database state\n",
        "- Different search queries may return different but valid landmarks\n",
        "- Order of results may vary (this is normal for search results)\n",
        "- Formatting differences are acceptable\n",
        "\n",
        "IGNORE THESE DIFFERENCES:\n",
        "- Format differences, duplicate searches, system messages\n",
        "- Different result ordering or landmark selection\n",
        "- Reference mismatches due to dynamic search results\n",
        "\n",
        "MARK AS CORRECT IF:\n",
        "- Agent successfully found landmarks matching the request\n",
        "- User received useful, accurate landmark information\n",
        "- Core functionality worked as expected (search worked, results filtered properly)\n",
        "\n",
        "MARK AS INCORRECT ONLY IF:\n",
        "- Agent completely failed to provide landmark information\n",
        "- Response is totally irrelevant to the landmark search request\n",
        "- Agent provided clearly wrong or nonsensical information\n",
        "\n",
        "**Question:** {input}\n",
        "\n",
        "**Reference Answer:** {reference}\n",
        "\n",
        "**AI Response:** {output}\n",
        "\n",
        "Based on the criteria above, is the AI response correct?\n",
        "\n",
        "Answer: [correct/incorrect]\n",
        "\n",
        "Explanation: [Provide a brief explanation focusing on functional success]\n",
        "\"\"\"\n",
        "\n",
        "# Lenient hallucination evaluation template  \n",
        "LENIENT_HALLUCINATION_PROMPT_TEMPLATE = \"\"\"\n",
        "You are evaluating whether an AI assistant's response about landmarks contains hallucinated (fabricated) information.\n",
        "\n",
        "DYNAMIC DATA IS EXPECTED AND FACTUAL:\n",
        "- Landmark search results are pulled from a real database\n",
        "- Different searches return different valid landmarks (this is correct behavior)\n",
        "- Landmark details like addresses, descriptions, and activities come from actual data\n",
        "- Search result variations are normal and factual\n",
        "\n",
        "MARK AS FACTUAL IF:\n",
        "- Response contains \"iteration limit\" or \"time limit\" (system issue, not hallucination)\n",
        "- Agent provides plausible landmark data from search results\n",
        "- Information is consistent with typical landmark search functionality\n",
        "- Results differ from reference due to dynamic search (this is expected!)\n",
        "\n",
        "ONLY MARK AS HALLUCINATED IF:\n",
        "- Response contains clearly impossible landmark information\n",
        "- Agent makes up fake landmark names, addresses, or details\n",
        "- Response contradicts fundamental facts about landmark search\n",
        "- Agent claims to have data it cannot access\n",
        "\n",
        "REMEMBER: Different search results are EXPECTED dynamic behavior, not hallucinations!\n",
        "\n",
        "**Question:** {input}\n",
        "\n",
        "**Reference Answer:** {reference}\n",
        "\n",
        "**AI Response:** {output}\n",
        "\n",
        "Based on the criteria above, does the response contain hallucinated information?\n",
        "\n",
        "Answer: [factual/hallucinated]\n",
        "\n",
        "Explanation: [Focus on whether information is plausible vs clearly fabricated]\n",
        "\"\"\"\n",
        "\n",
        "# Setup the landmark search agent\n",
        "agent, client = setup_landmark_agent()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Test Functions\n",
        "\n",
        "Define test functions to demonstrate the landmark search agent functionality.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_landmark_query(query: str, agent):\n",
        "    \"\"\"Run a single landmark query with error handling.\"\"\"\n",
        "    logger.info(f\"🏛️ Landmark Query: {query}\")\n",
        "    \n",
        "    try:\n",
        "        # Run the agent with LlamaIndex chat interface\n",
        "        response = agent.chat(query, chat_history=[])\n",
        "        result = response.response\n",
        "        \n",
        "        logger.info(f\"🤖 AI Response: {result}\")\n",
        "        logger.info(\"✅ Query completed successfully\")\n",
        "        \n",
        "        return result\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.exception(f\"❌ Query failed: {e}\")\n",
        "        return f\"Error: {str(e)}\"\n",
        "\n",
        "\n",
        "def test_landmark_data_loading():\n",
        "    \"\"\"Test landmark data loading from travel-sample independently.\"\"\"\n",
        "    logger.info(\"Testing Landmark Data Loading from travel-sample\")\n",
        "    logger.info(\"=\" * 50)\n",
        "    \n",
        "    try:\n",
        "        # Import landmark data functions\n",
        "        sys.path.append(os.path.join(os.path.dirname(__file__), \"data\"))\n",
        "        from landmark_data import get_landmark_count, get_landmark_texts\n",
        "        \n",
        "        # Test landmark count\n",
        "        count = get_landmark_count()\n",
        "        logger.info(f\"✅ Landmark count in travel-sample.inventory.landmark: {count}\")\n",
        "        \n",
        "        # Test landmark text generation\n",
        "        texts = get_landmark_texts()\n",
        "        logger.info(f\"✅ Generated {len(texts)} landmark texts for embeddings\")\n",
        "        \n",
        "        if texts:\n",
        "            logger.info(f\"✅ First landmark text sample: {texts[0][:200]}...\")\n",
        "        \n",
        "        logger.info(\"✅ Data loading test completed successfully\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.exception(f\"❌ Data loading test failed: {e}\")\n",
        "\n",
        "\n",
        "# Test landmark data loading\n",
        "test_landmark_data_loading()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Test 1: Landmarks in Tokyo\n",
        "\n",
        "Search for landmarks and attractions in Tokyo, Japan.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result1 = run_landmark_query(\"Find me landmarks in Tokyo\", agent)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Test 2: Museums in London\n",
        "\n",
        "Search for museums and cultural attractions in London, UK.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result2 = run_landmark_query(\"Show me museums in London\", agent)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "xml"
        }
      },
      "source": [
        "## Test 3: Parks in Paris\n",
        "\n",
        "Search for parks and outdoor spaces in Paris, France.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result3 = run_landmark_query(\"What parks can I visit in Paris?\", agent)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Arize Phoenix Evaluation\n",
        "\n",
        "This section demonstrates how to evaluate the landmark search agent using Arize Phoenix observability platform. The evaluation includes:\n",
        "\n",
        "- **Relevance Scoring**: Using Phoenix RelevanceEvaluator to score how relevant responses are to queries\n",
        "- **QA Scoring**: Using Phoenix QAEvaluator to score answer quality\n",
        "- **Hallucination Detection**: Using Phoenix HallucinationEvaluator to detect fabricated information  \n",
        "- **Toxicity Detection**: Using Phoenix ToxicityEvaluator to detect harmful content\n",
        "- **Phoenix UI**: Real-time observability dashboard at `http://localhost:6006/`\n",
        "\n",
        "We'll run landmark search queries and evaluate the responses for quality and safety using LlamaIndex instrumentation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import Phoenix evaluation components\n",
        "try:\n",
        "    import phoenix as px\n",
        "    from openinference.instrumentation.llama_index import LlamaIndexInstrumentor\n",
        "    from phoenix.evals import (\n",
        "        HALLUCINATION_PROMPT_RAILS_MAP,\n",
        "        HALLUCINATION_PROMPT_TEMPLATE,\n",
        "        QA_PROMPT_RAILS_MAP,\n",
        "        QA_PROMPT_TEMPLATE,\n",
        "        RAG_RELEVANCY_PROMPT_RAILS_MAP,\n",
        "        RAG_RELEVANCY_PROMPT_TEMPLATE,\n",
        "        TOXICITY_PROMPT_RAILS_MAP,\n",
        "        TOXICITY_PROMPT_TEMPLATE,\n",
        "        OpenAIModel,\n",
        "        llm_classify,\n",
        "    )\n",
        "    from phoenix.otel import register\n",
        "    import pandas as pd\n",
        "    \n",
        "    ARIZE_AVAILABLE = True\n",
        "    logger.info(\"✅ Arize Phoenix evaluation components available\")\n",
        "except ImportError as e:\n",
        "    logger.warning(f\"Arize dependencies not available: {e}\")\n",
        "    logger.warning(\"Skipping evaluation section...\")\n",
        "    ARIZE_AVAILABLE = False\n",
        "\n",
        "if ARIZE_AVAILABLE:\n",
        "    # Start Phoenix session for observability\n",
        "    try:\n",
        "        px.launch_app(port=6006)\n",
        "        logger.info(\"🚀 Phoenix UI available at http://localhost:6006/\")\n",
        "        \n",
        "        # Register LlamaIndex instrumentation\n",
        "        tracer_provider = register(\n",
        "            project_name=\"landmark-search-agent-evaluation\",\n",
        "            endpoint=\"http://localhost:6006/v1/traces\"\n",
        "        )\n",
        "        \n",
        "        # Instrument LlamaIndex\n",
        "        LlamaIndexInstrumentor().instrument(tracer_provider=tracer_provider)\n",
        "        logger.info(\"✅ LlamaIndex instrumentation enabled\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.warning(f\"Could not start Phoenix UI: {e}\")\n",
        "\n",
        "    # Demo queries for evaluation\n",
        "    landmark_demo_queries = [\n",
        "        \"Find me landmarks in Tokyo\",\n",
        "        \"Show me museums in London\"\n",
        "    ]\n",
        "    \n",
        "    # Run demo queries and collect responses for evaluation\n",
        "    landmark_demo_results = []\n",
        "    \n",
        "    for i, query in enumerate(landmark_demo_queries, 1):\n",
        "        try:\n",
        "            logger.info(f\"🔍 Running evaluation query {i}: {query}\")\n",
        "            \n",
        "            # Run the agent with LlamaIndex\n",
        "            response = agent.chat(query, chat_history=[])\n",
        "            output = response.response\n",
        "    \n",
        "            landmark_demo_results.append({\n",
        "                \"query\": query,\n",
        "                \"response\": output,\n",
        "                \"query_type\": f\"landmark_demo_{i}\",\n",
        "                \"success\": True\n",
        "            })\n",
        "            \n",
        "            logger.info(f\"✅ Query {i} completed successfully\")\n",
        "    \n",
        "        except Exception as e:\n",
        "            logger.exception(f\"❌ Query {i} failed: {e}\")\n",
        "            landmark_demo_results.append({\n",
        "                \"query\": query,\n",
        "                \"response\": f\"Error: {e!s}\",\n",
        "                \"query_type\": f\"landmark_demo_{i}\",\n",
        "                \"success\": False\n",
        "            })\n",
        "    \n",
        "    # Convert to DataFrame for evaluation\n",
        "    landmark_results_df = pd.DataFrame(landmark_demo_results)\n",
        "    logger.info(f\"📊 Collected {len(landmark_results_df)} responses for evaluation\")\n",
        "    \n",
        "    # Display results summary\n",
        "    for _, row in landmark_results_df.iterrows():\n",
        "        logger.info(f\"Query: {row['query']}\")\n",
        "        logger.info(f\"Response: {row['response'][:200]}...\")\n",
        "        logger.info(f\"Success: {row['success']}\")\n",
        "        logger.info(\"-\" * 50)\n",
        "    \n",
        "    logger.info(\"💡 Visit Phoenix UI at http://localhost:6006/ to see detailed traces and evaluations\")\n",
        "    logger.info(\"💡 Use the evaluation script at evals/eval_arize.py for comprehensive evaluation\")\n",
        "\n",
        "else:\n",
        "    logger.info(\"Arize evaluation not available - install phoenix-evals to enable evaluation\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if ARIZE_AVAILABLE and len(landmark_demo_results) > 0:\n",
        "    logger.info(\"🔍 Running comprehensive Phoenix evaluations...\")\n",
        "    \n",
        "    # Setup evaluator LLM (using OpenAI for consistency)\n",
        "    evaluator_llm = OpenAIModel(model=\"gpt-4o\", temperature=0.1)\n",
        "    \n",
        "    # Prepare evaluation data with proper column names for Phoenix evaluators\n",
        "    landmark_eval_data = []\n",
        "    for _, row in landmark_results_df.iterrows():\n",
        "        landmark_eval_data.append({\n",
        "            \"input\": row[\"query\"],\n",
        "            \"output\": row[\"response\"],\n",
        "            \"reference\": \"A helpful and accurate response about landmarks with specific location information and practical details\",\n",
        "            \"text\": row[\"response\"]  # For toxicity evaluation\n",
        "        })\n",
        "    \n",
        "    landmark_eval_df = pd.DataFrame(landmark_eval_data)\n",
        "    \n",
        "    try:\n",
        "        # 1. Relevance Evaluation\n",
        "        logger.info(\"🔍 Running Relevance Evaluation...\")\n",
        "        landmark_relevance_results = llm_classify(\n",
        "            dataframe=landmark_eval_df[[\"input\", \"reference\"]],\n",
        "            model=evaluator_llm,\n",
        "            template=RAG_RELEVANCY_PROMPT_TEMPLATE,\n",
        "            rails=list(RAG_RELEVANCY_PROMPT_RAILS_MAP.values())\n",
        "        )\n",
        "        \n",
        "        logger.info(\"✅ Relevance Evaluation Results:\")\n",
        "        for i, result in enumerate(landmark_relevance_results):\n",
        "            query = landmark_eval_data[i][\"input\"]\n",
        "            logger.info(f\"   Query: {query}\")\n",
        "            logger.info(f\"   Relevance: {result.label}\")\n",
        "            logger.info(\"   \" + \"-\"*30)\n",
        "        \n",
        "        # 2. QA Evaluation\n",
        "        logger.info(\"🔍 Running QA Evaluation...\")\n",
        "        landmark_qa_results = llm_classify(\n",
        "            dataframe=landmark_eval_df[[\"input\", \"output\", \"reference\"]],\n",
        "            model=evaluator_llm,\n",
        "            template=QA_PROMPT_TEMPLATE,\n",
        "            rails=list(QA_PROMPT_RAILS_MAP.values())\n",
        "        )\n",
        "        \n",
        "        logger.info(\"✅ QA Evaluation Results:\")\n",
        "        for i, result in enumerate(landmark_qa_results):\n",
        "            query = landmark_eval_data[i][\"input\"]\n",
        "            logger.info(f\"   Query: {query}\")\n",
        "            logger.info(f\"   QA Score: {result.label}\")\n",
        "            logger.info(\"   \" + \"-\"*30)\n",
        "        \n",
        "        # 3. Hallucination Evaluation\n",
        "        logger.info(\"🔍 Running Hallucination Evaluation...\")\n",
        "        landmark_hallucination_results = llm_classify(\n",
        "            dataframe=landmark_eval_df[[\"input\", \"reference\", \"output\"]],\n",
        "            model=evaluator_llm,\n",
        "            template=HALLUCINATION_PROMPT_TEMPLATE,\n",
        "            rails=list(HALLUCINATION_PROMPT_RAILS_MAP.values())\n",
        "        )\n",
        "        \n",
        "        logger.info(\"✅ Hallucination Evaluation Results:\")\n",
        "        for i, result in enumerate(landmark_hallucination_results):\n",
        "            query = landmark_eval_data[i][\"input\"]\n",
        "            logger.info(f\"   Query: {query}\")\n",
        "            logger.info(f\"   Hallucination: {result.label}\")\n",
        "            logger.info(\"   \" + \"-\"*30)\n",
        "        \n",
        "        # 4. Toxicity Evaluation\n",
        "        logger.info(\"🔍 Running Toxicity Evaluation...\")\n",
        "        landmark_toxicity_results = llm_classify(\n",
        "            dataframe=landmark_eval_df[[\"text\"]],\n",
        "            model=evaluator_llm,\n",
        "            template=TOXICITY_PROMPT_TEMPLATE,\n",
        "            rails=list(TOXICITY_PROMPT_RAILS_MAP.values())\n",
        "        )\n",
        "        \n",
        "        logger.info(\"✅ Toxicity Evaluation Results:\")\n",
        "        for i, result in enumerate(landmark_toxicity_results):\n",
        "            query = landmark_eval_data[i][\"input\"]\n",
        "            logger.info(f\"   Query: {query}\")\n",
        "            logger.info(f\"   Toxicity: {result.label}\")\n",
        "            logger.info(\"   \" + \"-\"*30)\n",
        "        \n",
        "        # Summary of all evaluations\n",
        "        logger.info(\"📊 EVALUATION SUMMARY\")\n",
        "        logger.info(\"=\" * 50)\n",
        "        \n",
        "        for i, query in enumerate([item[\"input\"] for item in landmark_eval_data]):\n",
        "            logger.info(f\"Query {i+1}: {query}\")\n",
        "            logger.info(f\"  Relevance: {landmark_relevance_results[i].label}\")\n",
        "            logger.info(f\"  QA Score: {landmark_qa_results[i].label}\")\n",
        "            logger.info(f\"  Hallucination: {landmark_hallucination_results[i].label}\")\n",
        "            logger.info(f\"  Toxicity: {landmark_toxicity_results[i].label}\")\n",
        "            logger.info(\"  \" + \"-\"*40)\n",
        "        \n",
        "        logger.info(\"✅ All Phoenix evaluations completed successfully!\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.exception(f\"❌ Phoenix evaluation failed: {e}\")\n",
        "        logger.info(\"💡 This might be due to API rate limits or model availability\")\n",
        "        \n",
        "else:\n",
        "    if not ARIZE_AVAILABLE:\n",
        "        logger.info(\"❌ Phoenix evaluations skipped - Arize dependencies not available\")\n",
        "    else:\n",
        "        logger.info(\"❌ Phoenix evaluations skipped - No demo results to evaluate\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrates a complete landmark search agent implementation using:\n",
        "\n",
        "1. **Agent Catalog Integration**: Using agentc to find tools and prompts\n",
        "2. **LlamaIndex Framework**: ReAct agent pattern with semantic search capabilities\n",
        "3. **Couchbase Vector Store**: Storing and searching landmark data from travel-sample bucket\n",
        "4. **NVIDIA NIMs + Capella AI**: NVIDIA NIMs for LLM, Capella AI for embeddings\n",
        "5. **Single Tool Architecture**: Focused on `search_landmarks` for landmark discovery\n",
        "6. **Comprehensive Evaluation**: Phoenix-based evaluation with LlamaIndex instrumentation\n",
        "\n",
        "The agent can handle various landmark-related queries including:\n",
        "- Landmark search by location (Tokyo, London, Paris)\n",
        "- Finding specific types of attractions (museums, parks, monuments)\n",
        "- Cultural and historical site discovery\n",
        "- Tourist attraction recommendations\n",
        "\n",
        "## Phoenix Evaluation Metrics\n",
        "\n",
        "The notebook demonstrates all four key Phoenix evaluation types:\n",
        "\n",
        "1. **Relevance Evaluation**: Measures how relevant responses are to landmark queries\n",
        "2. **QA Evaluation**: Assesses the quality and accuracy of landmark information\n",
        "3. **Hallucination Detection**: Identifies fabricated or incorrect landmark information\n",
        "4. **Toxicity Detection**: Screens for harmful or inappropriate content\n",
        "\n",
        "Each evaluation provides:\n",
        "- Binary or categorical labels (e.g., \"relevant\"/\"irrelevant\", \"correct\"/\"incorrect\")\n",
        "- Detailed explanations of the evaluation reasoning\n",
        "- Confidence scores for the assessments\n",
        "\n",
        "## Key Features\n",
        "\n",
        "This landmark search agent implementation:\n",
        "- **Uses LlamaIndex**: Advanced RAG framework with ReAct agent pattern\n",
        "- **Uses travel-sample bucket**: Leverages existing Couchbase landmark data\n",
        "- **NVIDIA NIMs integration**: High-performance LLM inference\n",
        "- **Capella AI embeddings**: High-quality vector embeddings for semantic search\n",
        "- **OpenAI fallback**: Graceful fallback when Capella AI is unavailable\n",
        "- **Single focused tool**: Simplified architecture with one search tool\n",
        "- **Comprehensive evaluation**: Full Phoenix evaluation pipeline\n",
        "- **LlamaIndex instrumentation**: Integrated observability and tracing\n",
        "\n",
        "## Data Source\n",
        "\n",
        "The agent uses landmark data from the `travel-sample.inventory.landmark` collection, which contains:\n",
        "- Real landmark information with names, locations, and descriptions\n",
        "- Structured data with address, city, country, and type information\n",
        "- Rich text descriptions suitable for vector embedding\n",
        "- Global coverage of tourist attractions and points of interest\n",
        "\n",
        "## Architecture Differences\n",
        "\n",
        "This landmark search agent differs from the other agents:\n",
        "- **LlamaIndex** (not LangChain or LangGraph) - advanced RAG framework\n",
        "- **NVIDIA NIMs LLM**: High-performance inference instead of OpenAI/Capella LLM\n",
        "- **ReAct Pattern**: Built-in reasoning and action capabilities\n",
        "- **Landmark-specific**: Optimized for tourism and travel use cases\n",
        "- **Global Settings**: Uses LlamaIndex global settings for LLM and embeddings\n",
        "\n",
        "For production use, consider:\n",
        "- Setting up proper monitoring with Arize Phoenix\n",
        "- Implementing comprehensive evaluation pipelines\n",
        "- Adding error handling and retry logic\n",
        "- Scaling the vector store for larger datasets\n",
        "- Adding more sophisticated query understanding\n",
        "\n",
        "## Usage Instructions\n",
        "\n",
        "To run this notebook:\n",
        "1. Set up the required environment variables (Couchbase connection, API keys)\n",
        "2. Install dependencies: `pip install -r requirements.txt`\n",
        "3. Ensure travel-sample bucket is available in your Couchbase cluster\n",
        "4. Publish your agent catalog: `agentc index . && agentc publish`\n",
        "5. Run the notebook cells sequentially\n",
        "\n",
        "The agent will automatically load landmark data from travel-sample and create embeddings for semantic search capabilities. NVIDIA API key is required for LLM functionality.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
