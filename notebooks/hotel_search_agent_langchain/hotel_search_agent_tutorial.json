{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Hotel Support Agent Tutorial - Complete Self-Contained Version\n",
        "\n",
        "This notebook demonstrates the Agent Catalog hotel support agent using LangChain with Couchbase vector store and Arize Phoenix evaluation. This is a complete, self-contained implementation that includes all necessary code inline.\n",
        "\n",
        "## Key Features:\n",
        "- **Priority 1 AI Services**: Uses standard OpenAI wrappers with Capella (simple & fast)\n",
        "- **Latest Fixes**: Includes check_embedding_ctx_length=False fix for asymmetric models\n",
        "- **Complete Hotel Data**: Full implementation of travel-sample hotel data loading\n",
        "- **Working Agent Setup**: Uses the tested and working agent configuration\n",
        "- **Phoenix Evaluation**: Comprehensive evaluation with lenient scoring\n",
        "\n",
        "## Prerequisites:\n",
        "- Couchbase Capella cluster with travel-sample bucket\n",
        "- Agent Catalog tools and prompts indexed with `agentc index`\n",
        "- Environment variables configured in `.env` file\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Setup and Imports\n",
        "\n",
        "Import all necessary modules and setup logging for the hotel support agent.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import getpass\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "import time\n",
        "from datetime import timedelta\n",
        "from typing import List, Optional\n",
        "\n",
        "import agentc\n",
        "import agentc_langchain\n",
        "import dotenv\n",
        "from couchbase.auth import PasswordAuthenticator\n",
        "from couchbase.cluster import Cluster\n",
        "from couchbase.management.search import SearchIndex\n",
        "from couchbase.options import ClusterOptions\n",
        "from langchain.agents import AgentExecutor, create_react_agent\n",
        "from langchain_core.messages import HumanMessage\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.tools import Tool\n",
        "from langchain_couchbase.vectorstores import CouchbaseVectorStore\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Suppress verbose logging\n",
        "logging.getLogger(\"openai\").setLevel(logging.WARNING)\n",
        "logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n",
        "logging.getLogger(\"httpcore\").setLevel(logging.WARNING)\n",
        "logging.getLogger(\"agentc_core\").setLevel(logging.WARNING)\n",
        "\n",
        "# Load environment variables\n",
        "dotenv.load_dotenv(override=True)\n",
        "\n",
        "# Constants\n",
        "DEFAULT_BUCKET = \"travel-sample\"\n",
        "DEFAULT_SCOPE = \"agentc_data\"\n",
        "DEFAULT_COLLECTION = \"hotel_data\"\n",
        "DEFAULT_INDEX = \"hotel_data_index\"\n",
        "\n",
        "logger.info(\"‚úÖ All imports completed successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Environment Setup\n",
        "\n",
        "Setup environment variables and configuration with all the latest fixes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _set_if_undefined(env_var: str, default_value: str = None):\n",
        "    \"\"\"Set environment variable if not already defined.\"\"\"\n",
        "    if not os.getenv(env_var):\n",
        "        if default_value is None:\n",
        "            value = getpass.getpass(f\"Enter {env_var}: \")\n",
        "        else:\n",
        "            value = default_value\n",
        "        os.environ[env_var] = value\n",
        "\n",
        "\n",
        "def setup_environment():\n",
        "    \"\"\"Setup required environment variables with defaults and latest fixes.\"\"\"\n",
        "    logger.info(\"Setting up environment variables...\")\n",
        "\n",
        "    # Set default bucket configuration\n",
        "    _set_if_undefined(\"CB_BUCKET\", DEFAULT_BUCKET)\n",
        "    _set_if_undefined(\"CB_SCOPE\", DEFAULT_SCOPE)\n",
        "    _set_if_undefined(\"CB_COLLECTION\", DEFAULT_COLLECTION)\n",
        "    _set_if_undefined(\"CB_INDEX\", DEFAULT_INDEX)\n",
        "\n",
        "    # Set AI service defaults with updated token limits\n",
        "    _set_if_undefined(\"CAPELLA_API_EMBEDDING_MAX_TOKENS\", \"4096\")\n",
        "    _set_if_undefined(\"CAPELLA_API_EMBEDDING_MODEL\", \"nvidia/llama-3.2-nv-embedqa-1b-v2\")\n",
        "    _set_if_undefined(\"CAPELLA_API_LLM_MODEL\", \"meta-llama/Llama-3.1-8B-Instruct\")\n",
        "\n",
        "    # Required Couchbase connection variables\n",
        "    _set_if_undefined(\"CB_CONN_STRING\")\n",
        "    _set_if_undefined(\"CB_USERNAME\")\n",
        "    _set_if_undefined(\"CB_PASSWORD\")\n",
        "\n",
        "    # Apply latest fixes\n",
        "    # Fix 1: Add ?tls_verify=none for SSL issues with Capella\n",
        "    conn_string = os.getenv(\"CB_CONN_STRING\")\n",
        "    if conn_string and conn_string.startswith(\"couchbases://\") and \"?tls_verify=none\" not in conn_string:\n",
        "        conn_string += \"?tls_verify=none\"\n",
        "        os.environ[\"CB_CONN_STRING\"] = conn_string\n",
        "        logger.info(\"‚úÖ Added ?tls_verify=none to Couchbase connection string for SSL compatibility\")\n",
        "\n",
        "    # Fix 2: Ensure Capella endpoint has /v1 suffix for compatibility\n",
        "    endpoint = os.getenv(\"CAPELLA_API_ENDPOINT\")\n",
        "    if endpoint and not endpoint.endswith(\"/v1\"):\n",
        "        endpoint = endpoint.rstrip(\"/\") + \"/v1\"\n",
        "        os.environ[\"CAPELLA_API_ENDPOINT\"] = endpoint\n",
        "        logger.info(f\"‚úÖ Updated Capella endpoint to: {endpoint}\")\n",
        "\n",
        "    logger.info(\"‚úÖ Environment setup completed\")\n",
        "\n",
        "\n",
        "# Setup environment\n",
        "setup_environment()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Priority 1 AI Services Setup\n",
        "\n",
        "Implementation of Priority 1 using standard OpenAI wrappers with Capella (simple & fast).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Priority 1 AI Services - Simple & Fast OpenAI Wrappers with Capella\n",
        "def setup_ai_services(temperature: float = 0.0, callbacks: Optional[List] = None):\n",
        "    \"\"\"\n",
        "    Setup AI services using Priority 1: Standard OpenAI wrappers with Capella.\n",
        "    \n",
        "    This uses the confirmed working approach with check_embedding_ctx_length=False fix.\n",
        "    \"\"\"\n",
        "    embeddings = None\n",
        "    llm = None\n",
        "    \n",
        "    logger.info(\"üîß Setting up AI services using Priority 1 (OpenAI wrappers + Capella)...\")\n",
        "    \n",
        "    # Priority 1: Capella with OpenAI wrappers (WORKING with fix)\n",
        "    if (\n",
        "        not embeddings \n",
        "        and os.getenv(\"CAPELLA_API_ENDPOINT\") \n",
        "        and os.getenv(\"CAPELLA_API_EMBEDDINGS_KEY\")\n",
        "    ):\n",
        "        try:\n",
        "            endpoint = os.getenv(\"CAPELLA_API_ENDPOINT\")\n",
        "            api_key = os.getenv(\"CAPELLA_API_EMBEDDINGS_KEY\")\n",
        "            model = os.getenv(\"CAPELLA_API_EMBEDDING_MODEL\")\n",
        "            \n",
        "            # Handle endpoint that may or may not already have /v1 suffix\n",
        "            if endpoint.endswith('/v1'):\n",
        "                base_url = endpoint\n",
        "            else:\n",
        "                base_url = f\"{endpoint}/v1\"\n",
        "            \n",
        "            # Debug logging - same pattern as working test\n",
        "            logger.info(f\"üîß Endpoint: {endpoint}\")\n",
        "            logger.info(f\"üîß Model: {model}\")\n",
        "            logger.info(f\"üîß Base URL: {base_url}\")\n",
        "            \n",
        "            embeddings = OpenAIEmbeddings(\n",
        "                model=model,\n",
        "                api_key=api_key,\n",
        "                base_url=base_url,\n",
        "                check_embedding_ctx_length=False,  # KEY FIX for asymmetric models\n",
        "            )\n",
        "            logger.info(\"‚úÖ Using Priority 1: Capella AI embeddings (OpenAI wrapper)\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"‚ùå Priority 1 Capella AI embeddings failed: {type(e).__name__}: {e}\")\n",
        "\n",
        "    if (\n",
        "        not llm \n",
        "        and os.getenv(\"CAPELLA_API_ENDPOINT\") \n",
        "        and os.getenv(\"CAPELLA_API_LLM_KEY\")\n",
        "    ):\n",
        "        try:\n",
        "            endpoint = os.getenv(\"CAPELLA_API_ENDPOINT\")\n",
        "            llm_key = os.getenv(\"CAPELLA_API_LLM_KEY\")\n",
        "            llm_model = os.getenv(\"CAPELLA_API_LLM_MODEL\")\n",
        "            \n",
        "            # Handle endpoint that may or may not already have /v1 suffix\n",
        "            if endpoint.endswith('/v1'):\n",
        "                base_url = endpoint\n",
        "            else:\n",
        "                base_url = f\"{endpoint}/v1\"\n",
        "            \n",
        "            # Debug logging\n",
        "            logger.info(f\"üîß LLM Endpoint: {endpoint}\")\n",
        "            logger.info(f\"üîß LLM Model: {llm_model}\")\n",
        "            logger.info(f\"üîß LLM Base URL: {base_url}\")\n",
        "            \n",
        "            # Use direct parameters like our working test\n",
        "            llm = ChatOpenAI(\n",
        "                api_key=llm_key,\n",
        "                base_url=base_url,\n",
        "                model=llm_model,\n",
        "                temperature=temperature,\n",
        "                callbacks=callbacks if callbacks else None,\n",
        "            )\n",
        "            \n",
        "            # Test the LLM works\n",
        "            test_response = llm.invoke([HumanMessage(content=\"Hello\")])\n",
        "            logger.info(f\"‚úÖ Using Priority 1: Capella AI LLM (OpenAI wrapper) - {test_response.content}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"‚ùå Priority 1 Capella AI LLM failed: {type(e).__name__}: {e}\")\n",
        "            llm = None\n",
        "\n",
        "    # Fallback to OpenAI if Capella fails\n",
        "    if not embeddings and os.getenv(\"OPENAI_API_KEY\"):\n",
        "        try:\n",
        "            embeddings = OpenAIEmbeddings(\n",
        "                model=\"text-embedding-3-small\",\n",
        "                api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
        "            )\n",
        "            logger.info(\"‚úÖ Using OpenAI embeddings (fallback)\")\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"‚ö†Ô∏è OpenAI embeddings failed: {e}\")\n",
        "    \n",
        "    if not llm and os.getenv(\"OPENAI_API_KEY\"):\n",
        "        try:\n",
        "            chat_kwargs = {\n",
        "                \"model\": \"gpt-4o\",\n",
        "                \"api_key\": os.getenv(\"OPENAI_API_KEY\"),\n",
        "                \"temperature\": temperature,\n",
        "            }\n",
        "            if callbacks:\n",
        "                chat_kwargs[\"callbacks\"] = callbacks\n",
        "            \n",
        "            llm = ChatOpenAI(**chat_kwargs)\n",
        "            logger.info(\"‚úÖ Using OpenAI LLM (fallback)\")\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"‚ö†Ô∏è OpenAI LLM failed: {e}\")\n",
        "    \n",
        "    if not embeddings or not llm:\n",
        "        raise RuntimeError(\"‚ùå Failed to setup AI services - check your API keys\")\n",
        "    \n",
        "    logger.info(\"‚úÖ Priority 1 AI services setup completed successfully\")\n",
        "    return embeddings, llm\n",
        "\n",
        "\n",
        "logger.info(\"‚úÖ Priority 1 AI services setup function defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## AI Services Integration\n",
        "\n",
        "The Priority 1 AI services are already defined above and integrated into the agent setup below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# AI services are set up using the Priority 1 function defined above\n",
        "logger.info(\"‚úÖ AI services integration ready\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## CouchbaseClient Class\n",
        "\n",
        "Complete implementation of the CouchbaseClient with all latest fixes and retry logic.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CouchbaseClient:\n",
        "    \"\"\"Centralized Couchbase client for all database operations with latest fixes.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        conn_string: str,\n",
        "        username: str,\n",
        "        password: str,\n",
        "        bucket_name: str,\n",
        "        wan_profile: bool = True,\n",
        "        timeout_seconds: int = 60,\n",
        "    ):\n",
        "        \"\"\"Initialize Couchbase client with enhanced configuration.\"\"\"\n",
        "        self.conn_string = conn_string\n",
        "        self.username = username\n",
        "        self.password = password\n",
        "        self.bucket_name = bucket_name\n",
        "        self.wan_profile = wan_profile\n",
        "        self.timeout_seconds = timeout_seconds\n",
        "        self.cluster = None\n",
        "        self.bucket = None\n",
        "        self._collections = {}\n",
        "\n",
        "    def connect(self, max_retries: int = 3):\n",
        "        \"\"\"Establish connection to Couchbase cluster with retry logic and SSL fixes.\"\"\"\n",
        "        last_exception = None\n",
        "        \n",
        "        for attempt in range(max_retries + 1):\n",
        "            try:\n",
        "                if attempt > 0:\n",
        "                    delay = 2 ** attempt  # Exponential backoff\n",
        "                    logger.info(f\"üîÑ Retry attempt {attempt + 1}/{max_retries + 1}, waiting {delay}s...\")\n",
        "                    time.sleep(delay)\n",
        "                \n",
        "                auth = PasswordAuthenticator(self.username, self.password)\n",
        "                options = ClusterOptions(auth)\n",
        "\n",
        "                # Enhanced WAN profile for remote clusters\n",
        "                if self.wan_profile:\n",
        "                    options.apply_profile(\"wan_development\")\n",
        "                    logger.info(f\"üåê Applied WAN profile with {self.timeout_seconds}s timeout\")\n",
        "\n",
        "                self.cluster = Cluster(self.conn_string, options)\n",
        "                self.cluster.wait_until_ready(timedelta(seconds=self.timeout_seconds))\n",
        "                logger.info(\"‚úÖ Successfully connected to Couchbase\")\n",
        "                return self.cluster\n",
        "                \n",
        "            except Exception as e:\n",
        "                last_exception = e\n",
        "                logger.warning(f\"‚ö†Ô∏è Connection attempt {attempt + 1} failed: {e}\")\n",
        "                \n",
        "                if attempt == max_retries:\n",
        "                    break\n",
        "                \n",
        "        raise ConnectionError(f\"‚ùå Failed to connect after {max_retries + 1} attempts. Last error: {last_exception!s}\")\n",
        "\n",
        "    def setup_collection(self, scope_name: str, collection_name: str):\n",
        "        \"\"\"Setup collection with proper error handling.\"\"\"\n",
        "        try:\n",
        "            if not self.cluster:\n",
        "                self.connect()\n",
        "\n",
        "            if not self.bucket:\n",
        "                self.bucket = self.cluster.bucket(self.bucket_name)\n",
        "                logger.info(f\"‚úÖ Connected to bucket '{self.bucket_name}'\")\n",
        "\n",
        "            # Setup scope and collection\n",
        "            bucket_manager = self.bucket.collections()\n",
        "            scopes = bucket_manager.get_all_scopes()\n",
        "            scope_exists = any(scope.name == scope_name for scope in scopes)\n",
        "\n",
        "            if not scope_exists and scope_name != \"_default\":\n",
        "                logger.info(f\"Creating scope '{scope_name}'...\")\n",
        "                bucket_manager.create_scope(scope_name)\n",
        "                logger.info(f\"‚úÖ Scope '{scope_name}' created\")\n",
        "\n",
        "            collections = bucket_manager.get_all_scopes()\n",
        "            collection_exists = any(\n",
        "                scope.name == scope_name\n",
        "                and collection_name in [col.name for col in scope.collections]\n",
        "                for scope in collections\n",
        "            )\n",
        "\n",
        "            if collection_exists:\n",
        "                logger.info(f\"‚ÑπÔ∏è Collection '{collection_name}' exists, keeping existing data\")\n",
        "            else:\n",
        "                logger.info(f\"Creating collection '{collection_name}'...\")\n",
        "                bucket_manager.create_collection(scope_name, collection_name)\n",
        "                logger.info(f\"‚úÖ Collection '{collection_name}' created\")\n",
        "\n",
        "            time.sleep(2)  # Wait for collection to be ready\n",
        "\n",
        "            # Create primary index\n",
        "            try:\n",
        "                self.cluster.query(\n",
        "                    f\"CREATE PRIMARY INDEX IF NOT EXISTS ON `{self.bucket_name}`.`{scope_name}`.`{collection_name}`\"\n",
        "                ).execute()\n",
        "                logger.info(\"‚úÖ Primary index created successfully\")\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"‚ö†Ô∏è Primary index creation: {e}\")\n",
        "\n",
        "            logger.info(f\"‚úÖ Collection setup complete: {scope_name}.{collection_name}\")\n",
        "            return self.bucket.scope(scope_name).collection(collection_name)\n",
        "\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"‚ùå Collection setup failed: {e!s}\")\n",
        "\n",
        "    def setup_vector_search_index(self, index_definition: dict, scope_name: str):\n",
        "        \"\"\"Setup vector search index with error handling.\"\"\"\n",
        "        try:\n",
        "            scope_index_manager = self.bucket.scope(scope_name).search_indexes()\n",
        "            existing_indexes = scope_index_manager.get_all_indexes()\n",
        "            index_name = index_definition[\"name\"]\n",
        "\n",
        "            if index_name not in [index.name for index in existing_indexes]:\n",
        "                logger.info(f\"Creating vector search index '{index_name}'...\")\n",
        "                search_index = SearchIndex.from_json(index_definition)\n",
        "                scope_index_manager.upsert_index(search_index)\n",
        "                logger.info(f\"‚úÖ Vector search index '{index_name}' created\")\n",
        "            else:\n",
        "                logger.info(f\"‚ÑπÔ∏è Vector search index '{index_name}' already exists\")\n",
        "                \n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"‚ùå Vector search index setup failed: {e!s}\")\n",
        "\n",
        "    def disconnect(self):\n",
        "        \"\"\"Safely disconnect from Couchbase.\"\"\"\n",
        "        if self.cluster:\n",
        "            # Couchbase SDK handles cleanup automatically\n",
        "            logger.info(\"‚úÖ Couchbase connection closed\")\n",
        "\n",
        "\n",
        "def create_couchbase_client(\n",
        "    conn_string: str = None,\n",
        "    username: str = None,\n",
        "    password: str = None,\n",
        "    bucket_name: str = None,\n",
        "    **kwargs\n",
        ") -> CouchbaseClient:\n",
        "    \"\"\"Factory function to create CouchbaseClient with environment defaults.\"\"\"\n",
        "    return CouchbaseClient(\n",
        "        conn_string=conn_string or os.getenv(\"CB_CONN_STRING\"),\n",
        "        username=username or os.getenv(\"CB_USERNAME\"),\n",
        "        password=password or os.getenv(\"CB_PASSWORD\"),\n",
        "        bucket_name=bucket_name or os.getenv(\"CB_BUCKET\", DEFAULT_BUCKET),\n",
        "        **kwargs\n",
        "    )\n",
        "\n",
        "\n",
        "logger.info(\"‚úÖ CouchbaseClient class defined successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Hotel Data Module\n",
        "\n",
        "Complete implementation of hotel data loading from travel-sample.inventory.hotel.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hotel search queries and reference answers\n",
        "HOTEL_SEARCH_QUERIES = [\n",
        "    \"Find hotels in Giverny with free breakfast\",\n",
        "    \"I need a hotel in Glossop with free internet access\",\n",
        "    \"Show me hotels in Helensburgh with free breakfast\",\n",
        "]\n",
        "\n",
        "HOTEL_REFERENCE_ANSWERS = [\n",
        "    # Query 1: Giverny with free breakfast\n",
        "    \"\"\"I found one hotel in Giverny that offers free breakfast:\n",
        "\n",
        "**Le Clos Fleuri**\n",
        "- **Location:** Giverny, France  \n",
        "- **Address:** 5 rue de la D√Æme, 27620 Giverny\n",
        "- **Amenities:** Free breakfast ‚úÖ, Free internet ‚úÖ, Free parking ‚úÖ\n",
        "- **Description:** Situated near the church and just a few minutes walking distance from Monet's gardens.\"\"\",\n",
        "    \n",
        "    # Query 2: Glossop with free internet\n",
        "    \"\"\"Here are hotels in Glossop that offer free internet access:\n",
        "\n",
        "1. **The George Hotel** - Norfolk Street, Glossop\n",
        "2. **Avondale Guest House** - 28 Woodhead Road, Glossop\n",
        "3. **The Bulls Head** - 102 Church Street, Old Glossop\n",
        "4. **Windy Harbour Farm Hotel** - Woodhead Road, Padfield, Glossop\n",
        "\n",
        "All offer free internet access as requested.\"\"\",\n",
        "    \n",
        "    # Query 3: Helensburgh with free breakfast\n",
        "    \"\"\"Here are hotels in Helensburgh that offer free breakfast:\n",
        "\n",
        "1. **County Lodge Hotel** - Old Luss Road, Helensburgh\n",
        "2. **Commodore Hotel** - 112-117 West Clyde Street, Helensburgh\n",
        "\n",
        "Both hotels offer free breakfast along with additional amenities.\"\"\",\n",
        "]\n",
        "\n",
        "QUERY_REFERENCE_ANSWERS = {\n",
        "    query: answer for query, answer in zip(HOTEL_SEARCH_QUERIES, HOTEL_REFERENCE_ANSWERS)\n",
        "}\n",
        "\n",
        "\n",
        "def get_evaluation_queries():\n",
        "    \"\"\"Get queries for evaluation.\"\"\"\n",
        "    return HOTEL_SEARCH_QUERIES\n",
        "\n",
        "\n",
        "def get_reference_answer(query: str) -> str:\n",
        "    \"\"\"Get the reference answer for a query.\"\"\"\n",
        "    return QUERY_REFERENCE_ANSWERS.get(query, f\"No reference answer for: {query}\")\n",
        "\n",
        "\n",
        "def retry_with_backoff(func, retries=3):\n",
        "    \"\"\"Simple retry with exponential backoff.\"\"\"\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            return func()\n",
        "        except Exception as e:\n",
        "            if attempt == retries - 1:\n",
        "                raise\n",
        "            delay = 2 ** attempt\n",
        "            logger.warning(f\"Attempt {attempt + 1} failed, retrying in {delay}s...\")\n",
        "            time.sleep(delay)\n",
        "\n",
        "\n",
        "def get_cluster_connection():\n",
        "    \"\"\"Get a fresh cluster connection.\"\"\"\n",
        "    try:\n",
        "        auth = PasswordAuthenticator(\n",
        "            username=os.getenv(\"CB_USERNAME\"),\n",
        "            password=os.getenv(\"CB_PASSWORD\"),\n",
        "        )\n",
        "        options = ClusterOptions(authenticator=auth)\n",
        "        options.apply_profile(\"wan_development\")\n",
        "        \n",
        "        cluster = Cluster(os.getenv(\"CB_CONN_STRING\"), options)\n",
        "        cluster.wait_until_ready(timedelta(seconds=60))\n",
        "        return cluster\n",
        "    except Exception as e:\n",
        "        logger.error(f\"‚ùå Failed to connect to cluster: {e}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "def get_hotel_count():\n",
        "    \"\"\"Get count of hotels in travel-sample.inventory.hotel.\"\"\"\n",
        "    def _get_count():\n",
        "        cluster = get_cluster_connection()\n",
        "        result = cluster.query(\n",
        "            \"SELECT COUNT(*) as count FROM `travel-sample`.inventory.hotel WHERE type='hotel'\"\n",
        "        )\n",
        "        return list(result)[0]['count']\n",
        "    \n",
        "    return retry_with_backoff(_get_count)\n",
        "\n",
        "\n",
        "def load_hotel_data_from_travel_sample():\n",
        "    \"\"\"Load hotel data from travel-sample.inventory.hotel collection.\"\"\"\n",
        "    try:\n",
        "        cluster = get_cluster_connection()\n",
        "        if not cluster:\n",
        "            raise ConnectionError(\"Could not connect to Couchbase cluster\")\n",
        "\n",
        "        # Query to get all hotel documents from travel-sample.inventory.hotel\n",
        "        query = \"\"\"\n",
        "            SELECT h.*, META(h).id as doc_id\n",
        "            FROM `travel-sample`.inventory.hotel h\n",
        "            ORDER BY h.name\n",
        "        \"\"\"\n",
        "\n",
        "        logger.info(\"Loading hotel data from travel-sample.inventory.hotel...\")\n",
        "        result = cluster.query(query)\n",
        "\n",
        "        hotels = []\n",
        "        for row in result:\n",
        "            hotel = row\n",
        "            hotels.append(hotel)\n",
        "\n",
        "        logger.info(f\"Loaded {len(hotels)} hotels from travel-sample.inventory.hotel\")\n",
        "        return hotels\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading hotel data: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "def get_hotel_texts():\n",
        "    \"\"\"Returns formatted hotel texts for vector store embedding from travel-sample data.\"\"\"\n",
        "    hotels = load_hotel_data_from_travel_sample()\n",
        "    hotel_texts = []\n",
        "\n",
        "    for hotel in tqdm(hotels, desc=\"Processing hotels\"):\n",
        "        # Start with basic info\n",
        "        name = hotel.get(\"name\", \"Unknown Hotel\")\n",
        "        city = hotel.get(\"city\", \"Unknown City\")\n",
        "        country = hotel.get(\"country\", \"Unknown Country\")\n",
        "\n",
        "        # Build text with PRIORITIZED information for search\n",
        "        text_parts = [f\"{name} in {city}, {country}\"]\n",
        "\n",
        "        # PRIORITY 1: Location details (critical for search)\n",
        "        location_fields = [\"address\", \"state\", \"directions\"]\n",
        "        for field in location_fields:\n",
        "            value = hotel.get(field)\n",
        "            if value and value != \"None\":\n",
        "                text_parts.append(f\"{field.title()}: {value}\")\n",
        "\n",
        "        # PRIORITY 2: Key amenities (most searched features)\n",
        "        amenity_fields = [\n",
        "            (\"free_breakfast\", \"Free breakfast\"),\n",
        "            (\"free_internet\", \"Free internet\"), \n",
        "            (\"free_parking\", \"Free parking\"),\n",
        "            (\"pets_ok\", \"Pets allowed\")\n",
        "        ]\n",
        "        for field, label in amenity_fields:\n",
        "            value = hotel.get(field)\n",
        "            if value is not None:\n",
        "                text_parts.append(f\"{label}: {'Yes' if value else 'No'}\")\n",
        "\n",
        "        # PRIORITY 3: Hotel description and type\n",
        "        description_fields = [\n",
        "            (\"description\", \"Description\"),\n",
        "            (\"type\", \"Type\"),\n",
        "            (\"title\", \"Title\")\n",
        "        ]\n",
        "        for field, label in description_fields:\n",
        "            value = hotel.get(field)\n",
        "            if value and value != \"None\":\n",
        "                text_parts.append(f\"{label}: {value}\")\n",
        "\n",
        "        # PRIORITY 4: Other details (less critical for search)\n",
        "        other_fields = [\n",
        "            (\"price\", \"Price\"),\n",
        "            (\"checkin\", \"Check-in\"),\n",
        "            (\"checkout\", \"Check-out\"),\n",
        "            (\"phone\", \"Phone\"),\n",
        "            (\"email\", \"Email\"),\n",
        "            (\"vacancy\", \"Vacancy\"),\n",
        "            (\"alias\", \"Also known as\")\n",
        "        ]\n",
        "        for field, label in other_fields:\n",
        "            value = hotel.get(field)\n",
        "            if value and value != \"None\":\n",
        "                if isinstance(value, bool):\n",
        "                    text_parts.append(f\"{label}: {'Yes' if value else 'No'}\")\n",
        "                else:\n",
        "                    text_parts.append(f\"{label}: {value}\")\n",
        "\n",
        "        # Add geographic coordinates if available\n",
        "        if hotel.get(\"geo\"):\n",
        "            geo = hotel[\"geo\"]\n",
        "            if geo.get(\"lat\") and geo.get(\"lon\"):\n",
        "                text_parts.append(f\"Coordinates: {geo['lat']}, {geo['lon']}\")\n",
        "\n",
        "        # Add review summary if available\n",
        "        if hotel.get(\"reviews\") and isinstance(hotel[\"reviews\"], list):\n",
        "            review_count = len(hotel[\"reviews\"])\n",
        "            if review_count > 0:\n",
        "                text_parts.append(f\"Reviews: {review_count} customer reviews available\")\n",
        "\n",
        "                # Include a sample of review content for better search matching\n",
        "                sample_reviews = hotel[\"reviews\"][:2]  # First 2 reviews\n",
        "                for i, review in enumerate(sample_reviews):\n",
        "                    if review.get(\"content\"):\n",
        "                        # Truncate long reviews for embedding efficiency\n",
        "                        content = (\n",
        "                            review[\"content\"][:200] + \"...\"\n",
        "                            if len(review[\"content\"]) > 200\n",
        "                            else review[\"content\"]\n",
        "                        )\n",
        "                        text_parts.append(f\"Review {i + 1}: {content}\")\n",
        "\n",
        "        # Add public likes if available\n",
        "        if hotel.get(\"public_likes\") and isinstance(hotel[\"public_likes\"], list):\n",
        "            likes_count = len(hotel[\"public_likes\"])\n",
        "            if likes_count > 0:\n",
        "                text_parts.append(f\"Public likes: {likes_count} likes\")\n",
        "\n",
        "        # Join all parts with \". \"\n",
        "        text = \". \".join(text_parts)\n",
        "        hotel_texts.append(text)\n",
        "\n",
        "    logger.info(f\"Generated {len(hotel_texts)} hotel text embeddings\")\n",
        "    return hotel_texts\n",
        "\n",
        "\n",
        "def load_hotel_data_to_couchbase(\n",
        "    cluster,\n",
        "    bucket_name: str,\n",
        "    scope_name: str,\n",
        "    collection_name: str,\n",
        "    embeddings,\n",
        "    index_name: str,\n",
        "):\n",
        "    \"\"\"Load hotel data into Couchbase vector store.\"\"\"\n",
        "    logger.info(\"üîÑ Loading data into vector store...\")\n",
        "    \n",
        "    try:\n",
        "        # Get hotel data\n",
        "        logger.info(\"Loading hotel data from travel-sample.inventory.hotel...\")\n",
        "        hotel_count = get_hotel_count()\n",
        "        logger.info(f\"Loaded {hotel_count} hotels from travel-sample.inventory.hotel\")\n",
        "        \n",
        "        hotel_texts = get_hotel_texts()\n",
        "        logger.info(f\"Generated {len(hotel_texts)} hotel text embeddings\")\n",
        "        \n",
        "        # Create vector store and add documents\n",
        "        vector_store = CouchbaseVectorStore(\n",
        "            cluster=cluster,\n",
        "            bucket_name=bucket_name,\n",
        "            scope_name=scope_name,\n",
        "            collection_name=collection_name,\n",
        "            embedding=embeddings,\n",
        "            index_name=index_name,\n",
        "        )\n",
        "        \n",
        "        logger.info(f\"Loading {len(hotel_texts)} hotel embeddings to {bucket_name}.{scope_name}.{collection_name}\")\n",
        "        \n",
        "        # Add documents in batches\n",
        "        batch_size = 50\n",
        "        for i in tqdm(range(0, len(hotel_texts), batch_size), desc=\"Loading hotel embeddings\"):\n",
        "            batch = hotel_texts[i:i + batch_size]\n",
        "            metadatas = [{'source': f'hotel_{j}', 'batch': i//batch_size} for j in range(len(batch))]\n",
        "            \n",
        "            try:\n",
        "                vector_store.add_texts(batch, metadatas=metadatas)\n",
        "                time.sleep(0.1)  # Rate limiting\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"‚ö†Ô∏è Batch {i//batch_size} failed: {e}\")\n",
        "                continue\n",
        "        \n",
        "        logger.info(\"‚úÖ Hotel data loaded successfully\")\n",
        "        return vector_store\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"‚ùå Failed to load hotel data: {e}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "logger.info(\"‚úÖ Hotel data module functions defined successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Hotel Support Agent Setup\n",
        "\n",
        "Complete setup of the hotel support agent with Agent Catalog integration using all working components.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def setup_hotel_support_agent():\n",
        "    \"\"\"Setup the complete hotel support agent with all working components.\"\"\"\n",
        "    try:\n",
        "        logger.info(\"üöÄ Setting up hotel support agent...\")\n",
        "        \n",
        "        # Initialize Agent Catalog\n",
        "        catalog = agentc.catalog.Catalog()\n",
        "        application_span = catalog.Span(name=\"Hotel Support Agent\")\n",
        "        \n",
        "        # Setup AI services using Priority 1 (OpenAI wrappers + Capella)\n",
        "        embeddings, llm = setup_ai_services(\n",
        "            temperature=0.0,\n",
        "            callbacks=[agentc_langchain.chat.Callback(span=application_span)]\n",
        "        )\n",
        "        \n",
        "        # Setup Couchbase connection\n",
        "        couchbase_client = create_couchbase_client()\n",
        "        couchbase_client.connect()\n",
        "        \n",
        "        # Setup collection\n",
        "        couchbase_client.setup_collection(\n",
        "            os.getenv(\"CB_SCOPE\", DEFAULT_SCOPE),\n",
        "            os.getenv(\"CB_COLLECTION\", DEFAULT_COLLECTION)\n",
        "        )\n",
        "        \n",
        "        # Setup vector search index - MUST have agentcatalog_index.json\n",
        "        with open(\"agentcatalog_index.json\", \"r\") as file:\n",
        "            index_definition = json.load(file)\n",
        "        logger.info(\"Loaded vector search index definition from agentcatalog_index.json\")\n",
        "        \n",
        "        couchbase_client.setup_vector_search_index(\n",
        "            index_definition, os.getenv(\"CB_SCOPE\", DEFAULT_SCOPE)\n",
        "        )\n",
        "        logger.info(\"‚úÖ Vector search index setup completed\")\n",
        "        \n",
        "        # Load hotel data into vector store\n",
        "        vector_store = load_hotel_data_to_couchbase(\n",
        "            cluster=couchbase_client.cluster,\n",
        "            bucket_name=couchbase_client.bucket_name,\n",
        "            scope_name=os.getenv(\"CB_SCOPE\", DEFAULT_SCOPE),\n",
        "            collection_name=os.getenv(\"CB_COLLECTION\", DEFAULT_COLLECTION),\n",
        "            embeddings=embeddings,\n",
        "            index_name=os.getenv(\"CB_INDEX\", DEFAULT_INDEX),\n",
        "        )\n",
        "        \n",
        "        # Load tools from Agent Catalog\n",
        "        tool_search = catalog.find(\"tool\", name=\"search_vector_database\")\n",
        "        if not tool_search:\n",
        "            raise ValueError(\n",
        "                \"Could not find search_vector_database tool. Make sure it's indexed with 'agentc index tools/'\"\n",
        "            )\n",
        "\n",
        "        tools = [\n",
        "            Tool(\n",
        "                name=tool_search.meta.name,\n",
        "                description=tool_search.meta.description,\n",
        "                func=tool_search.func,\n",
        "            ),\n",
        "        ]\n",
        "        \n",
        "        # Load prompt from Agent Catalog\n",
        "        hotel_prompt = catalog.find(\"prompt\", name=\"hotel_search_assistant\")\n",
        "        if not hotel_prompt:\n",
        "            raise ValueError(\n",
        "                \"Could not find hotel_search_assistant prompt. Make sure it's indexed with 'agentc index prompts/'\"\n",
        "            )\n",
        "\n",
        "        custom_prompt = PromptTemplate(\n",
        "            template=hotel_prompt.content.strip(),\n",
        "            input_variables=[\"input\", \"agent_scratchpad\"],\n",
        "            partial_variables={\n",
        "                \"tools\": \"\\n\".join([f\"{tool.name}: {tool.description}\" for tool in tools]),\n",
        "                \"tool_names\": \", \".join([tool.name for tool in tools]),\n",
        "            },\n",
        "        )\n",
        "        \n",
        "        # Create agent with enhanced error handling\n",
        "        def handle_parsing_error(error) -> str:\n",
        "            \"\"\"Enhanced error handler for parsing errors.\"\"\"\n",
        "            logger.warning(f\"Parsing error occurred: {error}\")\n",
        "            return \"\"\"I need to use the correct format. Let me search for hotels:\n",
        "\n",
        "Thought: I need to search for hotels using the search_vector_database tool\n",
        "Action: search_vector_database\n",
        "Action Input: \"\"\"\n",
        "\n",
        "        agent = create_react_agent(llm, tools, custom_prompt)\n",
        "\n",
        "        agent_executor = AgentExecutor(\n",
        "            agent=agent,\n",
        "            tools=tools,\n",
        "            verbose=True,\n",
        "            handle_parsing_errors=handle_parsing_error,\n",
        "            max_iterations=8,\n",
        "            max_execution_time=120,\n",
        "            early_stopping_method=\"force\",\n",
        "            return_intermediate_steps=True,\n",
        "        )\n",
        "\n",
        "        logger.info(\"‚úÖ Hotel support agent setup completed successfully\")\n",
        "        return agent_executor, application_span, couchbase_client\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.exception(f\"‚ùå Error setting up hotel support agent: {e}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "# Setup the hotel support agent\n",
        "logger.info(\"üöÄ Initializing hotel support agent...\")\n",
        "agent_executor, application_span, couchbase_client = setup_hotel_support_agent()\n",
        "logger.info(\"‚úÖ Hotel support agent ready!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Test Functions\n",
        "\n",
        "Define test functions to demonstrate the hotel support agent functionality.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_hotel_query(query: str, agent_executor, application_span):\n",
        "    \"\"\"Run a single hotel query with comprehensive error handling.\"\"\"\n",
        "    logger.info(f\"üîç Hotel Query: {query}\")\n",
        "    \n",
        "    try:\n",
        "        with application_span.new(f\"Hotel Query: {query}\") as query_span:\n",
        "            query_span[\"query\"] = query\n",
        "            \n",
        "            # Run the agent\n",
        "            response = agent_executor.invoke({\"input\": query})\n",
        "            result = response.get(\"output\", \"No response generated\")\n",
        "            \n",
        "            query_span[\"result\"] = result\n",
        "            logger.info(f\"ü§ñ AI Response: {result}\")\n",
        "            logger.info(\"‚úÖ Query completed successfully\")\n",
        "            \n",
        "            return result\n",
        "            \n",
        "    except Exception as e:\n",
        "        logger.exception(f\"‚ùå Query failed: {e}\")\n",
        "        return f\"Error: {str(e)}\"\n",
        "\n",
        "\n",
        "def test_hotel_data_loading():\n",
        "    \"\"\"Test hotel data loading capabilities.\"\"\"\n",
        "    logger.info(\"üß™ Testing Hotel Data Loading\")\n",
        "    logger.info(\"=\" * 50)\n",
        "    \n",
        "    try:\n",
        "        # Test hotel count\n",
        "        count = get_hotel_count()\n",
        "        logger.info(f\"‚úÖ Hotel count in travel-sample.inventory.hotel: {count}\")\n",
        "        \n",
        "        # Test hotel text generation (sample)\n",
        "        texts = get_hotel_texts()\n",
        "        logger.info(f\"‚úÖ Generated {len(texts)} hotel texts for embeddings\")\n",
        "        \n",
        "        if texts:\n",
        "            logger.info(f\"‚úÖ Sample hotel text: {texts[0][:200]}...\")\n",
        "        \n",
        "        logger.info(\"‚úÖ Data loading test completed successfully\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.exception(f\"‚ùå Data loading test failed: {e}\")\n",
        "\n",
        "\n",
        "# Run data loading test\n",
        "test_hotel_data_loading()\n",
        "\n",
        "logger.info(\"‚úÖ Test functions ready\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Test 1: Hotel Search in Giverny\n",
        "\n",
        "Search for hotels in Giverny with free breakfast.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test query 1: Giverny with free breakfast\n",
        "eval_queries = get_evaluation_queries()\n",
        "\n",
        "result1 = run_hotel_query(\n",
        "    eval_queries[0],  # \"Find hotels in Giverny with free breakfast\"\n",
        "    agent_executor,\n",
        "    application_span\n",
        ")\n",
        "\n",
        "print(f\"\\nüìã Query Result 1:\\n{result1}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Test 2: Hotel Search in Glossop\n",
        "\n",
        "Search for hotels in Glossop with free internet access.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test query 2: Glossop with free internet\n",
        "result2 = run_hotel_query(\n",
        "    eval_queries[1],  # \"I need a hotel in Glossop with free internet access\"\n",
        "    agent_executor,\n",
        "    application_span\n",
        ")\n",
        "\n",
        "print(f\"\\nüìã Query Result 2:\\n{result2}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Test 3: Hotel Search in Helensburgh\n",
        "\n",
        "Search for hotels in Helensburgh with free breakfast.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test query 3: Helensburgh with free breakfast\n",
        "result3 = run_hotel_query(\n",
        "    eval_queries[2],  # \"Show me hotels in Helensburgh with free breakfast\"\n",
        "    agent_executor,\n",
        "    application_span\n",
        ")\n",
        "\n",
        "print(f\"\\nüìã Query Result 3:\\n{result3}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Arize Phoenix Evaluation\n",
        "\n",
        "Comprehensive evaluation using Arize Phoenix with lenient scoring templates optimized for hotel search scenarios.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Import Phoenix evaluation components - comprehensive imports from eval_arize.py\n",
        "try:\n",
        "    import pandas as pd\n",
        "    import phoenix as px\n",
        "    from openinference.instrumentation.langchain import LangChainInstrumentor\n",
        "    from openinference.instrumentation.openai import OpenAIInstrumentor\n",
        "    from phoenix.evals import (\n",
        "        RAG_RELEVANCY_PROMPT_RAILS_MAP,\n",
        "        RAG_RELEVANCY_PROMPT_TEMPLATE,\n",
        "        TOXICITY_PROMPT_RAILS_MAP,\n",
        "        TOXICITY_PROMPT_TEMPLATE,\n",
        "        HallucinationEvaluator,\n",
        "        OpenAIModel,\n",
        "        QAEvaluator,\n",
        "        RelevanceEvaluator,\n",
        "        ToxicityEvaluator,\n",
        "        llm_classify,\n",
        "    )\n",
        "    from phoenix.otel import register\n",
        "    \n",
        "    import nest_asyncio\n",
        "    nest_asyncio.apply()\n",
        "\n",
        "    # Define lenient evaluation templates for hotel search\n",
        "    # Lenient templates imported from templates.py (exact copy from source)\n",
        "    LENIENT_QA_PROMPT_TEMPLATE = \"\"\"\n",
        "You are an expert evaluator assessing if an AI assistant's response correctly answers the user's question about hotels.\n",
        "\n",
        "FOCUS ON FUNCTIONAL SUCCESS, NOT EXACT MATCHING:\n",
        "1. Did the agent provide the requested hotel information?\n",
        "2. Is the core information accurate and helpful to the user?\n",
        "3. Would the user be satisfied with what they received?\n",
        "\n",
        "DYNAMIC DATA IS EXPECTED AND CORRECT:\n",
        "- Hotel search results vary based on current database state\n",
        "- Different search queries may return different but valid hotels\n",
        "- Order of results may vary (this is normal for search results)\n",
        "- Formatting differences are acceptable\n",
        "\n",
        "IGNORE THESE DIFFERENCES:\n",
        "- Format differences, duplicate searches, system messages\n",
        "- Different result ordering or hotel selection\n",
        "- Reference mismatches due to dynamic search results\n",
        "\n",
        "MARK AS CORRECT IF:\n",
        "- Agent successfully found hotels matching the request\n",
        "- User received useful, accurate hotel information\n",
        "- Core functionality worked as expected (search worked, results filtered properly)\n",
        "\n",
        "MARK AS INCORRECT ONLY IF:\n",
        "- Agent completely failed to provide hotel information\n",
        "- Response is totally irrelevant to the hotel search request\n",
        "- Agent provided clearly wrong or nonsensical information\n",
        "\n",
        "**Question:** {input}\n",
        "\n",
        "**Reference Answer:** {reference}\n",
        "\n",
        "**AI Response:** {output}\n",
        "\n",
        "Based on the criteria above, is the AI response correct?\n",
        "\n",
        "Answer: [correct/incorrect]\n",
        "\n",
        "Explanation: [Provide a brief explanation focusing on functional success]\n",
        "\"\"\"\n",
        "\n",
        "    # Lenient hallucination evaluation template  \n",
        "    LENIENT_HALLUCINATION_PROMPT_TEMPLATE = \"\"\"\n",
        "You are evaluating whether an AI assistant's response about hotels contains hallucinated (fabricated) information.\n",
        "\n",
        "DYNAMIC DATA IS EXPECTED AND FACTUAL:\n",
        "- Hotel search results are pulled from a real database\n",
        "- Different searches return different valid hotels (this is correct behavior)\n",
        "- Hotel details like addresses, amenities, and descriptions come from actual data\n",
        "- Search result variations are normal and factual\n",
        "\n",
        "MARK AS FACTUAL IF:\n",
        "- Response contains \"iteration limit\" or \"time limit\" (system issue, not hallucination)\n",
        "- Agent provides plausible hotel data from search results\n",
        "- Information is consistent with typical hotel search functionality\n",
        "- Results differ from reference due to dynamic search (this is expected!)\n",
        "\n",
        "ONLY MARK AS HALLUCINATED IF:\n",
        "- Response contains clearly impossible hotel information\n",
        "- Agent makes up fake hotel names, addresses, or amenities\n",
        "- Response contradicts fundamental facts about hotel search\n",
        "- Agent claims to have data it cannot access\n",
        "\n",
        "REMEMBER: Different search results are EXPECTED dynamic behavior, not hallucinations!\n",
        "\n",
        "**Question:** {input}\n",
        "\n",
        "**Reference Answer:** {reference}\n",
        "\n",
        "**AI Response:** {output}\n",
        "\n",
        "Based on the criteria above, does the response contain hallucinated information?\n",
        "\n",
        "Answer: [factual/hallucinated]\n",
        "\n",
        "Explanation: [Focus on whether information is plausible vs clearly fabricated]\n",
        "\"\"\"\n",
        "\n",
        "    # Lenient evaluation rails (classification options)\n",
        "    LENIENT_QA_RAILS = [\"correct\", \"incorrect\"]\n",
        "    LENIENT_HALLUCINATION_RAILS = [\"factual\", \"hallucinated\"]\n",
        "    \n",
        "    ARIZE_AVAILABLE = True\n",
        "    logger.info(\"‚úÖ Arize Phoenix evaluation components available\")\n",
        "\n",
        "    # Initialize Phoenix evaluators from eval_arize.py\n",
        "    evaluator_llm = OpenAIModel(model=\"gpt-4o\")\n",
        "    phoenix_evaluators = {\n",
        "        \"relevance\": RelevanceEvaluator(evaluator_llm),\n",
        "        \"qa_correctness\": QAEvaluator(evaluator_llm),\n",
        "        \"hallucination\": HallucinationEvaluator(evaluator_llm),\n",
        "        \"toxicity\": ToxicityEvaluator(evaluator_llm),\n",
        "    }\n",
        "    logger.info(\"‚úÖ Phoenix evaluators initialized\")\n",
        "\n",
        "    # Setup Phoenix observability from eval_arize.py\n",
        "    try:\n",
        "        tracer_provider = register(\n",
        "            project_name=\"hotel-support-agent-evaluation\",\n",
        "            endpoint=\"http://localhost:6006/v1/traces\",\n",
        "        )\n",
        "        \n",
        "        # Setup instrumentation\n",
        "        langchain_instrumentor = LangChainInstrumentor()\n",
        "        langchain_instrumentor.instrument(tracer_provider=tracer_provider)\n",
        "        \n",
        "        openai_instrumentor = OpenAIInstrumentor()\n",
        "        openai_instrumentor.instrument(tracer_provider=tracer_provider)\n",
        "        \n",
        "        logger.info(\"‚úÖ Phoenix observability and instrumentation setup completed\")\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"‚ö†Ô∏è Phoenix instrumentation setup failed: {e}\")\n",
        "\n",
        "    # ArizeHotelSupportEvaluator class from eval_arize.py\n",
        "    class ArizeHotelSupportEvaluator:\n",
        "        \"\"\"Comprehensive hotel support agent evaluator using Arize Phoenix evaluators.\"\"\"\n",
        "\n",
        "        def __init__(self):\n",
        "            \"\"\"Initialize the evaluator with Phoenix components.\"\"\"\n",
        "            self.evaluator_llm = evaluator_llm\n",
        "            self.phoenix_evaluators = phoenix_evaluators\n",
        "\n",
        "        def _extract_response_content(self, result):\n",
        "            \"\"\"Extract clean response content from agent result.\"\"\"\n",
        "            try:\n",
        "                response_parts = []\n",
        "                \n",
        "                # Check for intermediate_steps (AgentExecutor format) first\n",
        "                if isinstance(result, dict) and \"intermediate_steps\" in result:\n",
        "                    for step in result[\"intermediate_steps\"]:\n",
        "                        if isinstance(step, tuple) and len(step) >= 2:\n",
        "                            # step[0] is the action, step[1] is the tool output/observation\n",
        "                            tool_output = str(step[1])\n",
        "                            if tool_output and tool_output.strip():\n",
        "                                response_parts.append(tool_output)\n",
        "                \n",
        "                # Then check standard output fields\n",
        "                if isinstance(result, dict):\n",
        "                    if \"output\" in result:\n",
        "                        output_content = str(result[\"output\"])\n",
        "                        # Filter out generic system messages that confuse evaluators\n",
        "                        if not any(msg in output_content.lower() for msg in [\n",
        "                            \"agent stopped due to iteration limit\",\n",
        "                            \"agent stopped due to time limit\",\n",
        "                            \"parsing error\",\n",
        "                            \"could not parse\"\n",
        "                        ]):\n",
        "                            response_parts.append(output_content)\n",
        "                    elif \"response\" in result:\n",
        "                        response_parts.append(str(result[\"response\"]))\n",
        "                \n",
        "                # Return the best available content\n",
        "                if response_parts:\n",
        "                    return \"\\n\".join(response_parts)\n",
        "                \n",
        "                # Fallback to original result\n",
        "                result_str = str(result)\n",
        "                if result_str and result_str.strip():\n",
        "                    return result_str\n",
        "                \n",
        "                return \"No response content found\"\n",
        "                \n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error extracting response content: {e}\")\n",
        "                return f\"Error extracting response: {e}\"\n",
        "\n",
        "        def _create_reference_text(self, query):\n",
        "            \"\"\"Create reference text for evaluation based on query.\"\"\"\n",
        "            reference_answer = get_reference_answer(query)\n",
        "            \n",
        "            if reference_answer.startswith(\"No reference answer available\"):\n",
        "                raise ValueError(f\"No reference answer available for query: '{query}'. \"\n",
        "                               f\"Please add this query to QUERY_REFERENCE_ANSWERS in data/queries.py\")\n",
        "            \n",
        "            return reference_answer\n",
        "\n",
        "        def run_phoenix_evaluations(self, results_df):\n",
        "            \"\"\"Run Phoenix evaluations on the results with comprehensive logic.\"\"\"\n",
        "            if not ARIZE_AVAILABLE or not self.phoenix_evaluators:\n",
        "                logger.warning(\"‚ö†Ô∏è Phoenix evaluators not available - skipping evaluations\")\n",
        "                return results_df\n",
        "\n",
        "            logger.info(f\"üß† Running Phoenix evaluations on {len(results_df)} responses...\")\n",
        "            logger.info(\"üìã Evaluation criteria:\")\n",
        "            logger.info(\"   üîç Relevance: Does the response address the hotel search query?\")\n",
        "            logger.info(\"   üéØ QA Correctness: Is the hotel information accurate and helpful?\")\n",
        "            logger.info(\"   üö® Hallucination: Does the response contain fabricated information?\")\n",
        "            logger.info(\"   ‚ò†Ô∏è Toxicity: Is the response harmful or inappropriate?\")\n",
        "\n",
        "            try:\n",
        "                # Prepare evaluation data\n",
        "                evaluation_data = []\n",
        "                for _, row in results_df.iterrows():\n",
        "                    query = row[\"query\"]\n",
        "                    response = row[\"response\"]\n",
        "\n",
        "                    # Create reference text based on query type\n",
        "                    reference = self._create_reference_text(str(query))\n",
        "\n",
        "                    evaluation_data.append({\n",
        "                        \"input\": query,\n",
        "                        \"output\": response,\n",
        "                        \"reference\": reference,\n",
        "                        \"query\": query,  # For hallucination evaluation\n",
        "                        \"response\": response,  # For hallucination evaluation\n",
        "                        \"text\": response,  # For toxicity evaluation\n",
        "                    })\n",
        "\n",
        "                eval_df = pd.DataFrame(evaluation_data)\n",
        "\n",
        "                # Run individual Phoenix evaluations\n",
        "                self._run_individual_phoenix_evaluations(eval_df, results_df)\n",
        "\n",
        "                logger.info(\"‚úÖ Phoenix evaluations completed\")\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.exception(f\"‚ùå Error running Phoenix evaluations: {e}\")\n",
        "                # Add error indicators\n",
        "                for eval_type in [\"relevance\", \"qa_correctness\", \"hallucination\", \"toxicity\"]:\n",
        "                    results_df[eval_type] = \"error\"\n",
        "                    results_df[f\"{eval_type}_explanation\"] = f\"Error: {e}\"\n",
        "\n",
        "            return results_df\n",
        "\n",
        "        def _run_individual_phoenix_evaluations(self, eval_df, results_df):\n",
        "            \"\"\"Run individual Phoenix evaluations.\"\"\"\n",
        "            for eval_name, evaluator in self.phoenix_evaluators.items():\n",
        "                try:\n",
        "                    logger.info(f\"   üìä Running {eval_name} evaluation...\")\n",
        "\n",
        "                    # Prepare data based on evaluator requirements\n",
        "                    if eval_name == \"relevance\":\n",
        "                        data = eval_df[[\"input\", \"reference\"]].copy()\n",
        "                        eval_results = llm_classify(\n",
        "                            data=data,\n",
        "                            model=self.evaluator_llm,\n",
        "                            template=RAG_RELEVANCY_PROMPT_TEMPLATE,\n",
        "                            rails=list(RAG_RELEVANCY_PROMPT_RAILS_MAP.values()),\n",
        "                            provide_explanation=True,\n",
        "                        )\n",
        "                    elif eval_name == \"qa_correctness\":\n",
        "                        data = eval_df[[\"input\", \"output\", \"reference\"]].copy()\n",
        "                        eval_results = llm_classify(\n",
        "                            data=data,\n",
        "                            model=self.evaluator_llm,\n",
        "                            template=LENIENT_QA_PROMPT_TEMPLATE,\n",
        "                            rails=LENIENT_QA_RAILS,\n",
        "                            provide_explanation=True,\n",
        "                        )\n",
        "                    elif eval_name == \"hallucination\":\n",
        "                        data = eval_df[[\"input\", \"reference\", \"output\"]].copy()\n",
        "                        eval_results = llm_classify(\n",
        "                            data=data,\n",
        "                            model=self.evaluator_llm,\n",
        "                            template=LENIENT_HALLUCINATION_PROMPT_TEMPLATE,\n",
        "                            rails=LENIENT_HALLUCINATION_RAILS,\n",
        "                            provide_explanation=True,\n",
        "                        )\n",
        "                    elif eval_name == \"toxicity\":\n",
        "                        data = eval_df[[\"input\"]].copy()\n",
        "                        eval_results = llm_classify(\n",
        "                            data=data,\n",
        "                            model=self.evaluator_llm,\n",
        "                            template=TOXICITY_PROMPT_TEMPLATE,\n",
        "                            rails=list(TOXICITY_PROMPT_RAILS_MAP.values()),\n",
        "                            provide_explanation=True,\n",
        "                        )\n",
        "                    else:\n",
        "                        logger.warning(f\"‚ö†Ô∏è Unknown evaluator: {eval_name}\")\n",
        "                        continue\n",
        "\n",
        "                    # Process results\n",
        "                    self._process_evaluation_results(eval_results, eval_name, results_df)\n",
        "\n",
        "                except Exception as e:\n",
        "                    logger.warning(f\"‚ö†Ô∏è {eval_name} evaluation failed: {e}\")\n",
        "                    results_df[eval_name] = \"error\"\n",
        "                    results_df[f\"{eval_name}_explanation\"] = f\"Error: {e}\"\n",
        "\n",
        "        def _process_evaluation_results(self, eval_results, eval_name, results_df):\n",
        "            \"\"\"Process evaluation results and add to results DataFrame.\"\"\"\n",
        "            try:\n",
        "                if eval_results is None:\n",
        "                    logger.warning(f\"‚ö†Ô∏è {eval_name} evaluation returned None\")\n",
        "                    results_df[eval_name] = \"unknown\"\n",
        "                    results_df[f\"{eval_name}_explanation\"] = \"Evaluation returned None\"\n",
        "                    return\n",
        "\n",
        "                # Handle DataFrame results\n",
        "                if hasattr(eval_results, \"columns\"):\n",
        "                    if \"label\" in eval_results.columns:\n",
        "                        results_df[eval_name] = eval_results[\"label\"].tolist()\n",
        "                    elif \"classification\" in eval_results.columns:\n",
        "                        results_df[eval_name] = eval_results[\"classification\"].tolist()\n",
        "                    else:\n",
        "                        results_df[eval_name] = \"unknown\"\n",
        "\n",
        "                    if \"explanation\" in eval_results.columns:\n",
        "                        results_df[f\"{eval_name}_explanation\"] = eval_results[\"explanation\"].tolist()\n",
        "                    elif \"reason\" in eval_results.columns:\n",
        "                        results_df[f\"{eval_name}_explanation\"] = eval_results[\"reason\"].tolist()\n",
        "                    else:\n",
        "                        results_df[f\"{eval_name}_explanation\"] = \"No explanation provided\"\n",
        "\n",
        "                    logger.info(f\"   ‚úÖ {eval_name} evaluation completed\")\n",
        "\n",
        "                # Handle list results\n",
        "                elif isinstance(eval_results, list) and len(eval_results) > 0:\n",
        "                    if isinstance(eval_results[0], dict):\n",
        "                        results_df[eval_name] = [item.get(\"label\", \"unknown\") for item in eval_results]\n",
        "                        results_df[f\"{eval_name}_explanation\"] = [\n",
        "                            item.get(\"explanation\", \"No explanation\") for item in eval_results\n",
        "                        ]\n",
        "                    else:\n",
        "                        results_df[eval_name] = eval_results\n",
        "                        results_df[f\"{eval_name}_explanation\"] = \"List evaluation result\"\n",
        "\n",
        "                    logger.info(f\"   ‚úÖ {eval_name} evaluation completed (list format)\")\n",
        "\n",
        "                else:\n",
        "                    logger.warning(f\"‚ö†Ô∏è {eval_name} evaluation returned unexpected format\")\n",
        "                    results_df[eval_name] = \"unknown\"\n",
        "                    results_df[f\"{eval_name}_explanation\"] = f\"Unexpected format: {type(eval_results)}\"\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"‚ö†Ô∏è Error processing {eval_name} results: {e}\")\n",
        "                results_df[eval_name] = \"error\"\n",
        "                results_df[f\"{eval_name}_explanation\"] = f\"Processing error: {e}\"\n",
        "\n",
        "        def _log_evaluation_summary(self, results_df):\n",
        "            \"\"\"Log evaluation summary using Phoenix results.\"\"\"\n",
        "            logger.info(\"\\nüìä Phoenix Evaluation Summary:\")\n",
        "            logger.info(f\"  Total queries: {len(results_df)}\")\n",
        "            logger.info(f\"  Successful executions: {results_df['success'].sum()}\")\n",
        "            logger.info(f\"  Failed executions: {(~results_df['success']).sum()}\")\n",
        "            if 'execution_time' in results_df.columns:\n",
        "                logger.info(f\"  Average execution time: {results_df['execution_time'].mean():.2f}s\")\n",
        "            else:\n",
        "                logger.info(\"  Execution time: Not available (using demo data)\")\n",
        "\n",
        "            # Phoenix evaluation results\n",
        "            if ARIZE_AVAILABLE and self.phoenix_evaluators:\n",
        "                # Create evaluation results dictionary for user-friendly formatting\n",
        "                evaluation_results = {}\n",
        "                for eval_type in [\"relevance\", \"qa_correctness\", \"hallucination\", \"toxicity\"]:\n",
        "                    if eval_type in results_df.columns:\n",
        "                        counts = results_df[eval_type].value_counts()\n",
        "                        evaluation_results[eval_type] = dict(counts)\n",
        "                \n",
        "                # Display results in user-friendly format\n",
        "                if evaluation_results:\n",
        "                    self._format_evaluation_results(evaluation_results, len(results_df))\n",
        "\n",
        "            # Sample results with FULL detailed explanations\n",
        "            if len(results_df) > 0:\n",
        "                logger.info(\"\\nüìù DETAILED EVALUATION RESULTS (FULL EXPLANATIONS):\")\n",
        "                logger.info(\"=\"*80)\n",
        "                for i in range(len(results_df)):\n",
        "                    row = results_df.iloc[i]\n",
        "                    logger.info(f\"\\nüîç QUERY {i+1}: {row['query']}\")\n",
        "                    logger.info(\"-\"*60)\n",
        "\n",
        "                    for eval_type in [\"relevance\", \"qa_correctness\", \"hallucination\", \"toxicity\"]:\n",
        "                        if eval_type in row:\n",
        "                            result = row[eval_type]\n",
        "                            # Show FULL explanation instead of truncated version\n",
        "                            full_explanation = str(row.get(f\"{eval_type}_explanation\", \"No explanation provided\"))\n",
        "                            logger.info(f\"\\nüìä {eval_type.upper()}: {result}\")\n",
        "                            logger.info(f\"üí≠ FULL REASONING:\")\n",
        "                            logger.info(f\"{full_explanation}\")\n",
        "                            logger.info(\"-\"*40)\n",
        "                    logger.info(\"=\"*80)\n",
        "\n",
        "        def _format_evaluation_results(self, results, total_queries):\n",
        "            \"\"\"Format evaluation results in a user-friendly way from eval_arize.py.\"\"\"\n",
        "            logger.info(\"\\n\" + \"=\"*50)\n",
        "            logger.info(\"üìä EVALUATION RESULTS SUMMARY\")\n",
        "            logger.info(\"=\"*50)\n",
        "            \n",
        "            # Create a mapping of metric names to user-friendly descriptions\n",
        "            metric_descriptions = {\n",
        "                'relevance': {\n",
        "                    'name': 'üîç Relevance',\n",
        "                    'description': 'Does the response address the user query?',\n",
        "                    'good_values': ['relevant']\n",
        "                },\n",
        "                'qa_correctness': {\n",
        "                    'name': 'üéØ QA Correctness', \n",
        "                    'description': 'Is the response factually correct?',\n",
        "                    'good_values': ['correct']\n",
        "                },\n",
        "                'hallucination': {\n",
        "                    'name': 'üö® Hallucination',\n",
        "                    'description': 'Does the response contain fabricated info?',\n",
        "                    'good_values': ['factual']\n",
        "                },\n",
        "                'toxicity': {\n",
        "                    'name': '‚ò†Ô∏è Toxicity',\n",
        "                    'description': 'Is the response harmful or inappropriate?',\n",
        "                    'good_values': ['non-toxic']\n",
        "                }\n",
        "            }\n",
        "            \n",
        "            for metric_name, metric_data in results.items():\n",
        "                if metric_name in metric_descriptions:\n",
        "                    desc = metric_descriptions[metric_name]\n",
        "                    logger.info(f\"\\n{desc['name']}: {desc['description']}\")\n",
        "                    logger.info(\"-\" * 40)\n",
        "                    \n",
        "                    # Calculate percentages for each category\n",
        "                    for category, count in metric_data.items():\n",
        "                        percentage = (int(count) / total_queries) * 100\n",
        "                        \n",
        "                        # Add status indicator\n",
        "                        if category in desc['good_values']:\n",
        "                            status = \"‚úÖ\"\n",
        "                        else:\n",
        "                            status = \"‚ùå\"\n",
        "                        \n",
        "                        logger.info(f\"  {status} {category.title()}: {count}/{total_queries} ({percentage:.1f}%)\")\n",
        "            \n",
        "            logger.info(\"\\n\" + \"=\"*50)\n",
        "\n",
        "    # Initialize the comprehensive evaluator\n",
        "    hotel_evaluator = ArizeHotelSupportEvaluator()\n",
        "    logger.info(\"‚úÖ ArizeHotelSupportEvaluator class initialized with comprehensive evaluation methods\")\n",
        "\n",
        "except ImportError as e:\n",
        "    logger.warning(f\"Arize dependencies not available: {e}\")\n",
        "    logger.warning(\"Running in local evaluation mode only...\")\n",
        "    ARIZE_AVAILABLE = False\n",
        "    phoenix_evaluators = {}\n",
        "    hotel_evaluator = None\n",
        "\n",
        "if ARIZE_AVAILABLE:\n",
        "    # Start Phoenix session\n",
        "    try:\n",
        "        px.launch_app(port=6006)\n",
        "        logger.info(\"üöÄ Phoenix UI available at http://localhost:6006/\")\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"Could not start Phoenix UI: {e}\")\n",
        "\n",
        "    # Collect results from previous tests\n",
        "    demo_results = [\n",
        "        {\"query\": eval_queries[0], \"response\": result1, \"success\": \"Error\" not in result1},\n",
        "        {\"query\": eval_queries[1], \"response\": result2, \"success\": \"Error\" not in result2},\n",
        "        {\"query\": eval_queries[2], \"response\": result3, \"success\": \"Error\" not in result3},\n",
        "    ]\n",
        "    \n",
        "    # Convert to DataFrame for evaluation\n",
        "    results_df = pd.DataFrame(demo_results)\n",
        "    logger.info(f\"üìä Collected {len(results_df)} responses for evaluation\")\n",
        "    \n",
        "    # Prepare evaluation data\n",
        "    eval_data = []\n",
        "    for _, row in results_df.iterrows():\n",
        "        query = row[\"query\"]\n",
        "        reference = get_reference_answer(query)\n",
        "        eval_data.append({\n",
        "            \"input\": query,\n",
        "            \"output\": row[\"response\"],\n",
        "            \"reference\": reference,\n",
        "            \"text\": row[\"response\"]  # For toxicity evaluation\n",
        "        })\n",
        "    \n",
        "    eval_df = pd.DataFrame(eval_data)\n",
        "    \n",
        "    # Display summary\n",
        "    logger.info(\"\\nüìã EVALUATION SUMMARY\")\n",
        "    logger.info(\"=\" * 50)\n",
        "    for i, row in enumerate(demo_results):\n",
        "        logger.info(f\"Query {i+1}: {row['query']}\")\n",
        "        logger.info(f\"Success: {row['success']}\")\n",
        "        logger.info(f\"Response: {row['response'][:100]}...\")\n",
        "        logger.info(\"-\" * 30)\n",
        "    \n",
        "    logger.info(\"üí° Visit Phoenix UI at http://localhost:6006/ for detailed traces\")\n",
        "    logger.info(\"‚úÖ Basic evaluation completed - Phoenix integration ready\")\n",
        "\n",
        "    # Run comprehensive Phoenix evaluations using ArizeHotelSupportEvaluator from eval_arize.py\n",
        "    if hotel_evaluator and phoenix_evaluators:\n",
        "        logger.info(\"\\nüöÄ Starting comprehensive Phoenix evaluation with full explanations...\")\n",
        "        \n",
        "        # Run comprehensive Phoenix evaluations on results\n",
        "        results_with_comprehensive_evals = hotel_evaluator.run_phoenix_evaluations(results_df)\n",
        "        \n",
        "        # Log comprehensive evaluation summary with detailed explanations\n",
        "        hotel_evaluator._log_evaluation_summary(results_with_comprehensive_evals)\n",
        "        \n",
        "        logger.info(\"‚úÖ Comprehensive Phoenix evaluation with detailed explanations completed!\")\n",
        "        logger.info(\"üí° Visit Phoenix UI at http://localhost:6006/ for interactive analysis and traces\")\n",
        "\n",
        "else:\n",
        "    logger.info(\"‚ùå Phoenix evaluation not available - install phoenix-evals to enable\")\n",
        "    \n",
        "    # Still show basic results\n",
        "    logger.info(\"\\nüìã BASIC RESULTS SUMMARY\")\n",
        "    logger.info(\"=\" * 50)\n",
        "    logger.info(f\"Query 1: {eval_queries[0]}\")\n",
        "    logger.info(f\"Result 1: {'‚úÖ Success' if 'Error' not in result1 else '‚ùå Failed'}\")\n",
        "    logger.info(f\"Query 2: {eval_queries[1]}\")\n",
        "    logger.info(f\"Result 2: {'‚úÖ Success' if 'Error' not in result2 else '‚ùå Failed'}\")\n",
        "    logger.info(f\"Query 3: {eval_queries[2]}\")\n",
        "    logger.info(f\"Result 3: {'‚úÖ Success' if 'Error' not in result3 else '‚ùå Failed'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Cleanup\n",
        "\n",
        "Clean up resources and connections.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
