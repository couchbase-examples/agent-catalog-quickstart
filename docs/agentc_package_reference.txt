agentc Command Documentation
The agentc command line tool acts as an interface for your Agent Catalog instance.

$ agentc
Usage: agentc [OPTIONS] COMMAND [ARGS]...

  The Couchbase Agent Catalog command line tool.

Options:
  -v, --verbose  Flag to enable verbose output.  [default: 0; 0<=x<=2]
  -i, --interactive / -ni, --no-interactive
                 Flag to enable interactive mode.  [default: i]
  -h, --help     Show this message and exit.

Commands:
  add      Interactively create a new tool or prompt and save it to the...
  clean    Delete all or specific (catalog and/or activity) Agent Catalog...
  env      Return all Agent Catalog related environment and configuration...
  execute  Search for and subsequently execute a specific tool.
  find     Find items from the catalog based on a natural language string...
  index    Walk the source directory trees (sources) to index source files...
  init     Initialize the necessary files/collections for your working Agent...
  ls       List all indexed tools and/or prompts in the catalog.
  publish  Upload the local catalog and/or logs to a Couchbase instance.
  status   Show the (aggregate) status of your Agent Catalog environment.
  version  Show the current version of Agent Catalog.

See https://couchbaselabs.github.io/agent-catalog/index.html for more
information.
All sub-commands have a verbosity option (-v, --verbose, by default verbosity is 0) and an interactive option (-i, --interactive / -ni, --no-interactive, by default interactivity is enabled).

add Command
The primary purpose of the add command is to import templates for declarative Agent Catalog tools (and prompts).

$ agentc add --help
Usage: agentc add [OPTIONS]

  Interactively create a new tool or prompt and save it to the filesystem
  (output). You MUST edit the generated file as per your requirements!

Options:
  -o, --output DIRECTORY  Location to save the generated tool / prompt to.
                          Defaults to your current working directory.
  --kind [python_function|sqlpp_query|semantic_search|http_request|prompt]
  -h, --help              Show this message and exit.
See here for more information on the types of catalog entries Agent Catalog currently recognizes.

Note

The add command does not index the generated tool / prompt for you. You are responsible for populating the remaining fields from the generated output and running the subsequent agentc index command.

Tip

For tools authored in Python, we recommend adding the agentc.catalog.tool decorator to your function and specifying the containing directory on agentc index instead of using the add command. For example, if you have an existing tool named positive_sentiment_analysis_tool that is properly documented (i.e., has a docstring and descriptive names in the function signature), simply add @agentc.catalog.tool to the top of your function.

import agentc

@agentc.catalog.tool
def positive_sentiment_analysis_tool(text_to_analyze: str) -> float:
    """ Using the given text, return a number between 0 and 1.
        A value of 0 means the text is not positive.
        A value of 1 means the text is positive.
        A vale of 0.5 means the text is slightly positive. """
    ...
clean Command
The purpose of the clean command is to "clean" / prune data from your local and / or remote catalog.

$ agentc clean --help
Usage: agentc clean [OPTIONS]
                    [[catalog|activity]]...

  Delete all or specific (catalog and/or activity) Agent Catalog related files /
  collections.

Options:
  --db / --no-db            Flag to perform / not-perform a DB clean.  [default:
                            db]
  --local / --no-local      Flag to perform / not-perform a local FS clean.
                            [default: local]
  --tools / --no-tools      Flag to clean / avoid-cleaning the tool-catalog.
                            [default: tools]
  --prompts / --no-prompts  Flag to clean / avoid-cleaning the prompt-catalog.
                            [default: prompts]
  --bucket TEXT             Name of the Couchbase bucket to remove Agent Catalog
                            from.
  -cid, --catalog-id TEXT   Catalog ID used to remove a specific catalog version
                            from the DB catalog.
  -y, --yes                 Flag to delete local-FS and DB catalog data without
                            confirmation.
  -d, --date TEXT           Datetime of the oldest log entry to keep (older log
                            entries will be deleted).
  -h, --help                Show this message and exit.
For remote catalogs possessing multiple catalog versions, you can specify a set of catalog IDs (i.e., Git SHAs) via the --catalog-id flag to delete their associated entries from the remote catalog. For example, running the command below will delete all tool + prompt + metadata entries associated with catalog versions GS53S and 14dFDD:

$ agentc clean -cid GS53S -cid 14dFDD
env Command
The env command displays the current configuration of Agent Catalog as a JSON object (see here for all configuration fields).

$ agentc env --help
Usage: agentc env [OPTIONS]

  Return all Agent Catalog related environment and configuration parameters as a
  JSON object.

Options:
  -h, --help  Show this message and exit.
execute Command
The execute command is a helper command that allows users to directly invoke tools indexed by Agent Catalog.

$ agentc execute --help
Usage: agentc execute [OPTIONS]

  Search for and subsequently execute a specific tool.

Options:
  --query TEXT             User query describing the task for which tools /
                           prompts are needed. This field or --name must be
                           specified.
  --name TEXT              Name of catalog item to retrieve from the catalog
                           directly. This field or --query must be specified.
  --bucket TEXT            Name of the Couchbase bucket to search.
  --dirty / --no-dirty     Flag to process and search amongst dirty source
                           files.  [default: dirty]
  --refiner [ClosestCluster|None]
                           Class to post-process (rerank, prune, etc...) find
                           results.
  -an, --annotations TEXT  Tool-specific annotations to filter by, specified
                           using KEY="VALUE" (AND|OR KEY="VALUE")*.
  -cid, --catalog-id TEXT  Catalog ID that uniquely specifies a catalog version
                           / snapshot (git commit id).  [default: __LATEST__]
  --db / --no-db           Flag to include / exclude items from the DB-catalog
                           while searching.
  --local / --no-local     Flag to include / exclude items from the local-FS-
                           catalog while searching.  [default: local]
  -h, --help               Show this message and exit.
The arguments for agentc execute are identical to that of agentc find (with the exception of the {tools|prompts} argument). execute is useful for verifying declarative tools before running them in your application (e.g., validating the results of your SQL++ query, checking the results of your semantic search, etc...).

find Command
The primary purpose of the find command is to validate the query or name arguments used by a call to agentc.Catalog:find.

$ agentc find --help
Usage: agentc find [OPTIONS] {tools|prompts}

  Find items from the catalog based on a natural language string (query) or by
  name.

Options:
  --query TEXT             User query describing the task for which tools /
                           prompts are needed. This field or --name must be
                           specified.
  --name TEXT              Name of catalog item to retrieve from the catalog
                           directly. This field or --query must be specified.
  --bucket TEXT            Name of the Couchbase bucket to search.
  --limit INTEGER          Maximum number of results to show.  [default: 1]
  --dirty / --no-dirty     Flag to process and search amongst dirty source
                           files.  [default: dirty]
  --refiner [ClosestCluster|None]
                           Class to post-process (rerank, prune, etc...) find
                           results.
  -an, --annotations TEXT  Tool-specific annotations to filter by, specified
                           using KEY="VALUE" (AND|OR KEY="VALUE")*.
  -cid, --catalog-id TEXT  Catalog ID that uniquely specifies a catalog version
                           / snapshot (git commit id).  [default: __LATEST__]
  --db / --no-db           Flag to include / exclude items from the DB-catalog
                           while searching.
  --local / --no-local     Flag to include / exclude items from the local-FS-
                           catalog while searching.  [default: local]
  -h, --help               Show this message and exit.
Search By Name
To find a tool or prompt directly by name, use the --name option. For example, to search for the latest version of the tool "find_user_by_id", you would write / author the following:


Command Line
$ agentc find tool --name find_user_by_id

Python Program
Search By Query
On agentc index, descriptions of tools and prompts are forwarded through an embedding model to enable semantic search of tools and prompts at find time. This is useful for authoring prompts in a tool-agnostic manner (see here for more information). To find 3 tools semantically related to "finding users", you would write / author the following:


Command Line
$ agentc find tool --query "finding users" --limit 3

Python Program
Filter By Annotations
Annotations can be added to tools and prompts, which can serve as optional filters for --query at find time. For example, to search for the most related prompt to "frontdesk agent" tailored towards healthcare (domain="healthcare"), you would write / author the following:


Command Line
$ # Use single quotes to interpret the annotations string as a literal here!
$ agentc find prompt --query "frontdesk agent" --annotations 'domain="healthcare"'

Python Program
Tip

Annotations on the command line must (generally) follow the regex KEY="VALUE" (AND|OR KEY="VALUE")*. This string must be specified in between single quotes to properly interpret the double quote.

Local Only Search
By default, Agent Catalog will search the local catalog and attempt (in a best-effort fashion) to search your Couchbase-backed catalog. To search for local-only entries for a single find command, use the --no-db flag. There is no equivalent flag using a agentc.Catalog:find call, but you can force local-only searches for a agentc.Catalog instance by explicitly setting the conn_string attribute to None. For example, to find the tool named "get_most_popular_categories" from the local catalog, write / author the following:


Command Line
$ agentc find tool --name get_most_popular_categories --no-db

Python Program
index Command
The purpose of the index command is to build a local index of all tools and prompts.

$ agentc index --help
Usage: agentc index [OPTIONS] [SOURCES]...

  Walk the source directory trees (sources) to index source files into the local
  catalog. Source files that will be scanned include *.py, *.sqlpp, *.yaml, etc.

Options:
  --prompts / --no-prompts  Flag to look for / ignore prompts when indexing
                            source files into the local catalog.  [default:
                            prompts]
  --tools / --no-tools      Flag to look for / ignore tools when indexing source
                            files into the local catalog.  [default: tools]
  --dry-run                 Flag to prevent catalog changes.
  -h, --help                Show this message and exit.
By default, the index command will look for both tools and prompts in the given SOURCES. To avoid searching for tools or prompts specifically, use the --no-tools and --no-prompts flags respectively. In the example below, Agent Catalog will scan the directories my_tools_1, my_tools_2, and my_tools_3 for only tools.

$ agentc index --no-prompts my_tools_1 my_tools_2 my_tools_3
init Command
The purpose of the init command is to initialize your Agent Catalog environment (both locally and on Couchbase).

$ agentc init --help
Usage: agentc init [OPTIONS]
                   [[catalog|activity]]...

  Initialize the necessary files/collections for your working Agent Catalog
  environment.

Options:
  --db / --no-db        Flag to enable / disable DB initialization.  [default:
                        db]
  --local / --no-local  Flag to enable / disable local FS initialization.
                        [default: local]
  --bucket TEXT         Name of the Couchbase bucket to initialize in.
  -h, --help            Show this message and exit.
The init command must be run at least once for each Agent Catalog environment. By default, init will run both locally and on Couchbase, initializing your catalog environment and your activity environment on each. For instances where init has already been run on Couchbase (or vice-versa), use the --no-db and --no-local flags respectively. In the example below, the catalog environment and the activity environment is only locally initialized:

$ agentc init --no-db
ls Command
The purpose of the ls command is to list out all items in the latest version of your Agent Catalog instance.

$ agentc ls --help
Usage: agentc ls [OPTIONS] [[tools|prompts]]...

  List all indexed tools and/or prompts in the catalog.

Options:
  --db / --no-db        Flag to force a DB-only search.  [default: no-db]
  --local / --no-local  Flag to force a local-only search.  [default: local]
  --dirty / --no-dirty  Flag to process and search amongst dirty source files.
                        [default: dirty]
  --bucket TEXT         Name of Couchbase bucket that is being used for Agent
                        Catalog.
  -h, --help            Show this message and exit.
By default, the ls command will only list items in a local and potentially dirty catalog instance. To list out all items in your Couchbase instance requires the --db flag. A common use case for this command involves running this command after index to see whether or not a certain tool exists in your local catalog.

Warning

This command is the equivalent to a SELECT * FROM agent_catalog.[tools|prompts]; and should be used sparingly with the --db flag. To view aggregate information about your catalog, use the status command instead.

publish Command
The primary purpose of the publish command is to "snapshot" your local catalog instance and persist this snapshot to Couchbase.

$ agentc publish --help
Usage: agentc publish [OPTIONS]
                      [[tools|prompts|logs]]...

  Upload the local catalog and/or logs to a Couchbase instance. By default, only
  tools and prompts are published unless log is explicitly specified.

Options:
  --bucket TEXT  Name of the Couchbase bucket to publish to.
  -an, --annotations <TEXT TEXT>...
                 Snapshot level annotations to be added while publishing
                 catalogs.
  -h, --help     Show this message and exit.
The publish command, similar to all other Couchbase-interfacing commands (e.g., clean, find, etc...) reads Couchbase authentication details from your environment. To override the bucket being published to, users can specify the bucket name directly via the --bucket option. In the example below, both tools and prompts are published to a bucket named test_bucket:

$ agentc publish --bucket test_bucket
Users can also choose to selectively publish tools, prompts, or logs to Couchbase. By default, only tools and prompts are published -- logs are expected to be continuously pushed to Couchbase while your application is running (thus, the logs choice is primarily for recovery operations). In the example below, only tools are published:

$ agentc publish tools
status Command
The purpose of the status command is to view aggregate information about your Agent Catalog instance.

$ agentc status --help
Usage: agentc status [OPTIONS]
                     [[tools|prompts]]...

  Show the (aggregate) status of your Agent Catalog environment.

Options:
  --dirty / --no-dirty  Flag to process and compare against dirty source files.
                        [default: dirty]
  --db / --no-db        Flag to include / exclude items from the DB-catalog
                        while displaying status.
  --local / --no-local  Flag to include / exclude items from the local-FS-
                        catalog while displaying status.  [default: local]
  --bucket TEXT         Name of the Couchbase bucket hosting the Agent Catalog.
  -h, --help            Show this message and exit.
TODO

version Command
The purpose of the version command is to display the current version of the agentc package.

$ agentc version --help
Usage: agentc version [OPTIONS]

  Show the current version of Agent Catalog.

Options:
  -h, --help  Show this message and exit.
$ agentc version
0.2.2a2

Package Documentation
agentc Package
pydantic settings agentc.catalog.Catalog[source]
A provider of indexed "agent building blocks" (e.g., tools, prompts, spans...).

Class Description
A Catalog instance can be configured in three ways (listed in order of precedence):

Directly (as arguments to the constructor).

Via the environment (though environment variables).

Via a .env configuration file.

In most cases, you'll want to configure your catalog via a .env file. This style of configuration means you can instantiate a Catalog instance as such:

import agentc
catalog = agentc.Catalog()
Some custom configurations can only be specified via the constructor (e.g., secrets). For example, if your secrets are managed by some external service (defined below as my_secrets_manager), you can specify them as such:

import agentc
catalog = agentc.Catalog(secrets={
    "CB_CONN_STRING": os.getenv("CB_CONN_STRING"),
    "CB_USERNAME": os.getenv("CB_USERNAME"),
    "CB_PASSWORD": my_secrets_manager.get("THE_CB_PASSWORD"),
    "CB_CERTIFICATE": my_secrets_manager.get("PATH_TO_CERT"),
})
Fields:
Span(
name,
session=None,
state=None,
**kwargs
)[source]
A factory method to initialize a Span (more specifically, a GlobalSpan) instance.

Parameters:
name (str) -- Name to bind to each message logged within this span.

session (str) -- The run that this tree of spans is associated with. By default, this is a UUID.

state (Any) -- A JSON-serializable object that will be logged on entering and exiting this span.

kwargs -- Additional keyword arguments to pass to the Span constructor.

Return type:
Span

find(
kind,
query=None,
name=None,
annotations=None,
catalog_id='__LATEST__',
limit=1
)[source]
Return a list of tools or prompts based on the specified search criteria.

Method Description
This method is meant to act as the programmatic equivalent of the agentc find command. Whether (or not) the results are fetched from the local catalog or the remote catalog depends on the configuration of this agentc_core.catalog.Catalog instance.

For example, to find a tool named "get_sentiment_of_text", you would author:

results = catalog.find(kind="tool", name="get_sentiment_of_text")
sentiment_score = results[0].func("I love this product!")
To find a prompt named "summarize_article_instructions", you would author:

results = catalog.find(kind="prompt", name="summarize_article_instructions")
prompt_for_agent = summarize_article_instructions.content
Parameters:
kind (Literal['tool', 'prompt']) -- The type of item to search for, either 'tool' or 'prompt'.

query (str) -- A query string (natural language) to search the catalog with.

name (str) -- The specific name of the catalog entry to search for.

annotations (str) -- An annotation query string in the form of KEY="VALUE" (AND|OR KEY="VALUE")*.

catalog_id (str) -- The snapshot version to find the tools for. By default, we use the latest snapshot.

limit (int | None) -- The maximum number of results to return (ignored if name is specified).

Returns:
One of the following:

None if no results are found by name.

"tools" if kind is "tool" (see find_tools() for details).

"prompts" if kind is "prompt" (see find_prompts() for details).

Return type:
list[ToolResult | T] | list[PromptResult] | ToolResult | T | PromptResult | None

find_prompts(
query=None,
name=None,
annotations=None,
catalog_id='__LATEST__',
limit=1
)[source]
Return a list of prompts based on the specified search criteria.

Parameters:
query (str) -- A query string (natural language) to search the catalog with.

name (str) -- The specific name of the catalog entry to search for.

annotations (str) -- An annotation query string in the form of KEY="VALUE" (AND|OR KEY="VALUE")*.

catalog_id (str) -- The snapshot version to find the tools for. By default, we use the latest snapshot.

limit (int | None) -- The maximum number of results to return (ignored if name is specified).

Returns:
A list of Prompt instances, with the following attributes:

content (str | dict): The content to be served to the model.

tools (list): The list containing the tool functions associated with prompt.

output (dict): The output type of the prompt, if it exists.

meta (RecordDescriptor): The metadata associated with the prompt.

Return type:
list[PromptResult] | PromptResult | None

find_tools(
query=None,
name=None,
annotations=None,
catalog_id='__LATEST__',
limit=1
)[source]
Return a list of tools based on the specified search criteria.

Parameters:
query (str) -- A query string (natural language) to search the catalog with.

name (str) -- The specific name of the catalog entry to search for.

annotations (str) -- An annotation query string in the form of KEY="VALUE" (AND|OR KEY="VALUE")*.

catalog_id (str) -- The snapshot version to find the tools for. By default, we use the latest snapshot.

limit (int | None) -- The maximum number of results to return (ignored if name is specified).

Returns:
By default, a list of Tool instances with the following attributes:

func (typing.Callable): A Python callable representing the function.

meta (RecordDescriptor): The metadata associated with the tool.

If a tool_decorator is present, this method will return a list of objects decorated accordingly.

Return type:
list[ToolResult | T] | ToolResult | T | None

property version: VersionDescriptor
The version of the catalog currently being served (i.e., the latest version).

Returns:
An agentc_core.version.VersionDescriptor instance.

pydantic model agentc.span.Span[source]
A structured logging context for agent activity.

Class Description
A Span instance belongs to a tree of other Span instances, whose root is a GlobalSpan instance that is constructed using the Catalog.Span() method.

Attention

Spans should never be created directly (via constructor), as logs generated by the span must always be associated with a catalog version and some application structure.

Below we illustrate how a tree of Span instances is created:

import agentc
catalog = agentc.Catalog()
root_span = catalog.Span(name="root")
child_1_span = root_span.new(name="child_1")
child_2_span = root_span.new(name="child_2")
In practice, you'll likely use different spans for different agents and/or different tasks. Below we give a small LangGraph example using spans for different agents:

import agentc
import langgraph.graph

catalog = agentc.Catalog()
root_span = catalog.Span(name="flight_planner")

def front_desk_agent(...):
    with root_span.new(name="front_desk_agent") as front_desk_span:
        ...

def route_finding_agent(...):
    with root_span.new(name="route_finding_agent") as route_finding_span:
        ...

workflow = langgraph.graph.StateGraph()
workflow.add_node("front_desk_agent", front_desk_agent)
workflow.add_node("route_finding_agent", route_finding_agent)
workflow.set_entry_point("front_desk_agent")
workflow.add_edge("front_desk_agent", "route_finding_agent")
...
Fields:
iterable (bool | None)

kwargs (dict[str, Any] | None)

logger (Callable[[...], agentc_core.activity.models.log.Log])

name (str)

parent (agentc_core.activity.span.Span)

state (Any)

field iterable: bool | None = False
Flag to indicate whether or not this span should be iterable.

Validated by:
_initialize_iterable_logger

field kwargs: dict[str, Any] | None = None
Annotations to apply to all messages logged within this span.

Validated by:
_initialize_iterable_logger

field name: str [Required]
Name to bind to each message logged within this span.

Validated by:
_initialize_iterable_logger

field parent: Span = None
Parent span of this span (i.e., the span that had new() called on it).

Validated by:
_initialize_iterable_logger

field state: Any = None
A JSON-serializable object that will be logged on entering and exiting this span.

Validated by:
_initialize_iterable_logger

pydantic model Identifier[source]
The unique identifier for a Span.

Class Description
A Span is uniquely identified by two parts:

an application-defined multipart name and...

a session identifier unique to each run of the application.

Fields:
name (list[str])

session (str)

field name: list[str] [Required]
The name of the Span.

Names are built up from the root of the span tree to the leaf, thus the first element of name is the name of the root and the last element is the name of the current span (i.e., the leaf).

field session: str [Required]
The session identifier of the Span.

Sessions must be unique to each run of the application. By default, we generate these as UUIDs (see GlobalSpan.session).

enter()[source]
Record a BeginContent log entry for this span.

Method Description
The enter() method is to denote the start of the span (optionally logging the incoming state if specified). This method is also called when entering the span using the with statement. In the example below, enter() is called (implicitly).

import agentc

catalog = agentc.Catalog()
incoming_state = {"flights": []}
with catalog.Span(name="flight_planner", state=incoming_state) as span:
    flight_planner_implementation()
On entering the context, one log is generated possessing the content below:

{ "kind": "begin", "state": {"flights": []} }
Return type:
Self

exit()[source]
Record a EndContent log entry for this span.

Method Description
The exit() method is to denote the end of the span (optionally logging the outgoing state if specified). This method is also called when exiting the span using the with statement successfully. In the example below, exit() is called (implicitly).

import agentc

catalog = agentc.Catalog()
incoming_state = {"flights": []}
with catalog.Span(name="flight_planner", state=incoming_state) as span:
    ... = flight_planner_implementation(...)
    incoming_state["flights"] = [{"flight_number": "AA123", "status": "on_time"}]
On exiting the context, one log is generated possessing the content below:

{ "kind": "end", "state": {"flights": [{"flight_number": "AA123", "status": "on_time"}]} }
Note

The state of the span must be JSON-serializable and must be mutated in-place. If you are working with immutable state objects, you must set the state attribute before exiting the span (i.e., before the with statement exits or with exit() explicitly).

import agentc

catalog = agentc.Catalog()
immutable_incoming_state = {"flights": []}
with catalog.Span(name="flight_planner", state=incoming_state) as span:
    ... = flight_planner_implementation(...)
    span.state = {"flights": [{"flight_number": "AA123", "status": "on_time"}]}
log(
content,
**kwargs
)[source]
Accept some content (with optional annotations specified by kwargs) and generate a corresponding log entry.

Method Description
The heart of the Span class is the log() method. This method is used to log events that occur within the span. Users can capture events that occur in popular frameworks like LangChain and LlamaIndex using our helper packages (see agentc_langchain, agentc_langgraph, and agentc_llamaindex) but must use those packages in conjunction with this log() method to capture the full breadth of their application's activity. See here for a list of all available log content types.

Users can also use Python's [] syntax to write arbitrary JSON-serializable content as a key-value (KeyValueContent) pair. This is useful for logging arbitrary data like metrics during evaluations. In the example below, we illustrate an example of a system-wide evaluation suite that uses this [] syntax:

import my_agent_app
import my_output_evaluator
import agentc

catalog = agentc.Catalog()
evaluation_span = catalog.Span(name="evaluation_suite")
with open("my-evaluation-suite.json") as fp:
    for i, line in enumerate(fp):
        with evaluation_span.new(name=f"evaluation{i}") as span:
            output = my_agent_app(span)
            span["positive_sentiment"] = my_output_evaluator.positive(output)
            span.log(
                content={
                    "kind": "key-value",
                    "key": "negative_sentiment",
                    "value": my_output_evaluator.negative(output)
                    },
                alpha="SDGD"
            )
All keywords passed to the log() method will be applied as annotations to the log entry. In the example above, the alpha annotation is applied only to the second log entry. For span-wide annotations, use the kwargs attribute on new().

Parameters:
content (SystemContent | ToolCallContent | ToolResultContent | ChatCompletionContent | RequestHeaderContent | UserContent | AssistantContent | BeginContent | EndContent | EdgeContent | KeyValueContent) -- The content to log.

kwargs -- Additional annotations to apply to the log.

logs()[source]
Return the logs generated by the tree of Span nodes rooted from this Span instance.

Method Description
The logs() method returns an iterable of all logs generated within the span. This method is also called (implicitly) when iterating over the span (e.g., using a for loop). To use this method, you must set the iterable attribute to True when instantiating the span:

import agentc

catalog = agentc.Catalog()
span = catalog.Span(name="flight_planner", iterable=True)
for log in span:
    match log.content.kind:
        case "begin":
            ...
Tip

Generally, this method should only be used for debugging purposes. This method will keep all logs generated by the span in memory. To perform efficient aggregate analysis of your logs, consider querying the agent_activity.logs collection in your Couchbase cluster using SQL++ instead.

Return type:
Iterable[Log]

new(
name,
state=None,
iterable=False,
**kwargs
)[source]
Create a new span under the current Span.

Method Description
Spans require a name and a session (see identifier). Aside from name, state, and iterable, you can also pass additional keywords that will be applied as annotations to each log() call within a span. As an example, the following code illustrates the use of kwargs to add a span-wide "alpha" annotation:

import agentc
catalog = agentc.Catalog()
root_span = catalog.Span(name="flight_planner")
with root_span.new(name="find_airports_task", alpha="SDGD") as child_span:
    child_span.log(content=agentc.span.UserContent(value="Hello, world!", "beta": "412d"))
The example code above will generate the three logs below (for brevity, we only show the content and
annotations fields):

{ "content": { "kind": "begin" }, "annotations": { "alpha": "SDGD"} }
{ "content": { "kind": "user", "value": "Hello, world!" },
  "annotations": { "alpha": "SDGD", "beta": "412d" } }
{ "content" : { "kind": "end" }, "annotations": { "alpha": "SDGD" } }
Parameters:
name (str) -- The name of the span.

state (Any) -- The starting state of the span. This will be recorded upon entering and exiting the span.

iterable (bool) -- Whether this new span should be iterable. By default, this is False.

kwargs -- Additional annotations to apply to the span.

Returns:
A new Span instance.

Return type:
Span

property identifier: Identifier
A unique identifier for this span.

Integration Packages
LangChain
class agentc_langchain.chat.Callback(
span,
tools=None,
output=None
)[source]
A callback that will log all LLM calls using the given span as the root.

Class Description
This class is a callback that will log all LLM calls using the given span as the root. This class will record all messages used to generated ChatCompletionContent and ToolCallContent. ToolResultContent is not logged by this class, as it is not generated by a BaseChatModel instance.

Below, we illustrate a minimal example of how to use this class:

import langchain_openai
import langchain_core.messages
import agentc_langchain.chat
import agentc

# Create a span to bind to the chat model messages.
catalog = agentc.Catalog()
root_span = catalog.Span(name="root_span")

# Create a chat model.
chat_model = langchain_openai.chat_models.ChatOpenAI(model="gpt-4o", callbacks=[])

# Create a callback with the appropriate span, and attach it to the chat model.
my_agent_span = root_span.new(name="my_agent")
callback = agentc_langchain.chat.Callback(span=my_agent_span)
chat_model.callbacks.append(callback)
result = chat_model.invoke(messages=[
    langchain_core.messages.SystemMessage(content="Hello, world!")
])
To record the exact tools and output used by the chat model, you can pass in the tools and output to the agentc_langchain.chat.Callback constructor. For example:

import langchain_openai
import langchain_core.messages
import langchain_core.tools
import agentc_langchain.chat
import agentc

# Create a span to bind to the chat model messages.
catalog = agentc.Catalog()
root_span = catalog.Span(name="root_span")

# Create a chat model.
chat_model = langchain_openai.chat_models.ChatOpenAI(model="gpt-4o", callbacks=[])

# Grab the correct tools and output from the catalog.
my_agent_prompt = catalog.find("prompt", name="my_agent")
my_agent_tools = [
    langchain_core.tools.StructuredTool.from_function(tool.func) for tool in my_agent_prompt.tools
]
my_agent_output = my_agent_prompt.output

# Create a callback with the appropriate span, tools, and output, and attach it to the chat model.
my_agent_span = root_span.new(name="my_agent")
callback = agentc_langchain.chat.Callback(
    span=my_agent_span,
    tools=my_agent_tools,
    output=my_agent_output
)
chat_model.callbacks.append(callback)
result = chat_model.with_structured_output(my_agent_output).invoke(messages=[
    langchain_core.messages.SystemMessage(content=my_agent_prompt.content)
])
Parameters:
span (Span)

tools (list[Tool])

output (dict)

agentc_langchain.cache.cache(
chat_model,
kind,
embeddings=None,
options=None,
**kwargs
)[source]
A function to attach a Couchbase-backed exact or semantic cache to a ChatModel.

Function Description
This function is used to set the .cache property of LangChain ChatModel instances. For all options related to this Couchbase-backed cache, see CacheOptions.

Below, we illustrate a minimal working example of how to use this function to store and retrieve LLM responses via exact prompt matching:

import langchain_openai
import agentc_langchain.cache

chat_model = langchain_openai.chat_models.ChatOpenAI(model="gpt-4o")
caching_chat_model = agentc_langchain.cache.cache(
    chat_model=chat_model,
    kind="exact",
    create_if_not_exists=True
)

# Response #2 is served from the cache.
response_1 = caching_chat_model.invoke("Hello there!")
response_2 = caching_chat_model.invoke("Hello there!")
To use this function to store and retrieve LLM responses via semantic similarity, use the kind="semantic" argument with an langchain_core.embeddings.Embeddings instance:

import langchain_openai
import agentc_langchain.cache

chat_model = langchain_openai.chat_models.ChatOpenAI(model="gpt-4o")
embeddings = langchain_openai.OpenAIEmbeddings(model="text-embedding-3-small")
caching_chat_model = agentc_langchain.cache.cache(
    chat_model=chat_model,
    kind="semantic",
    embeddings=embeddings,
    create_if_not_exists=True
)

# Response #2 is served from the cache.
response_1 = caching_chat_model.invoke("Hello there!")
response_2 = caching_chat_model.invoke("Hello there!!")
By default, the Couchbase initialization of the cache is separate from the cache's usage (storage and retrieval). To explicitly initialize the cache yourself, use the initialize() method.

See also

This method uses the langchain_couchbase.cache.CouchbaseCache and langchain_couchbase.cache.CouchbaseSemanticCache classes from the langchain_couchbase package. See here for more details.

Parameters:
chat_model (BaseChatModel) -- The LangChain chat model to cache responses for.

kind (Literal['exact', 'semantic']) -- The type of cache to attach to the chat model.

embeddings (Embeddings) -- The embeddings to use when attaching a 'semantic' cache to the chat model.

options (CacheOptions) -- The options to use when attaching a cache to the chat model.

kwargs -- Keyword arguments to be forwarded to a CacheOptions constructor (ignored if options is present).

Returns:
The same LangChain chat model that was passed in, but with a cache attached.

Return type:
BaseChatModel

agentc_langchain.cache.initialize(
kind,
options=None,
embeddings=None,
**kwargs
)[source]
A function to create the collections and/or indexes required to use the cache() function.

Function Description
This function is a helper function for creating the default collection (and index, in the case of kind="semantic") required for the cache() function. Below, we give a minimal working example of how to use this function to create a semantic cache backed by Couchbase.

import langchain_openai
import agentc_langchain.cache

embeddings = langchain_openai.OpenAIEmbeddings(model="text-embedding-3-small")
agentc_langchain.cache.initialize(
    kind="semantic",
    embeddings=embeddings
)

chat_model = langchain_openai.chat_models.ChatOpenAI(model="gpt-4o")
caching_chat_model = agentc_langchain.cache.cache(
    chat_model=chat_model,
    kind="semantic",
    embeddings=embeddings,
)

# Response #2 is served from the cache.
response_1 = caching_chat_model.invoke("Hello there!")
response_2 = caching_chat_model.invoke("Hello there!!")
Parameters:
kind (Literal['exact', 'semantic']) -- The type of cache to attach to the chat model.

embeddings (Embeddings) -- The embeddings to use when attaching a 'semantic' cache to the chat model.

options (CacheOptions) -- The options to use when attaching a cache to the chat model.

kwargs -- Keyword arguments to be forwarded to a CacheOptions constructor (ignored if options is present).

Return type:
None

pydantic settings agentc_langchain.cache.CacheOptions[source]
Config:
env_prefix: str = AGENT_CATALOG_LANGCHAIN_CACHE_

env_file: str = .env

Fields:
bucket (str | None)

collection (str | None)

conn_root_certificate (str | pathlib.Path | None)

conn_string (str | None)

create_if_not_exists (bool | None)

ddl_retry_attempts (int | None)

ddl_retry_wait_seconds (float | None)

index_name (str | None)

password (pydantic.types.SecretStr | None)

scope (str | None)

score_threshold (float | None)

ttl (datetime.timedelta | None)

username (str | None)

field bucket: str | None = None
The name of the Couchbase bucket hosting the cache.

This field must be specified.

Validated by:
_pull_cluster_from_agent_catalog

field collection: str | None = 'langchain_llm_cache'
The name of the Couchbase collection hosting the cache.

This field is optional and defaults to langchain_llm_cache.

Validated by:
_pull_cluster_from_agent_catalog

field conn_root_certificate: str | Path | None = None
Path to the root certificate file for the Couchbase cluster.

This field is optional and only required if the Couchbase cluster is using a self-signed certificate.

Validated by:
_pull_cluster_from_agent_catalog

field conn_string: str | None = None
The connection string to the Couchbase cluster hosting the cache.

This field must be specified.

Validated by:
_pull_cluster_from_agent_catalog

field create_if_not_exists: bool | None = False
Create the required collections and/or indexes if they do not exist.

When raised (i.e., this value is set to True), the collections and indexes will be created if they do not exist. Lower this flag (set this to False) to instead raise an error if the collections & indexes do not exist.

Validated by:
_pull_cluster_from_agent_catalog

field ddl_retry_attempts: int | None = 3
Maximum number of attempts to retry DDL operations.

This value is only used on setup (i.e., the first time the cache is requested). If the number of attempts is exceeded, the command will fail. By default, this value is 3 attempts.

Validated by:
_pull_cluster_from_agent_catalog

field ddl_retry_wait_seconds: float | None = 5
Wait time (in seconds) between DDL operation retries.

This value is only used on setup (i.e., the first time the cache is requested). By default, this value is 5 seconds.

Validated by:
_pull_cluster_from_agent_catalog

field index_name: str | None = 'langchain_llm_cache_index'
The name of the Couchbase FTS index used to query the cache.

This field will only be used if the cache is of type semantic. If the cache is of type semantic and this field is not specified, this field defaults to langchain_llm_cache_index.

Validated by:
_pull_cluster_from_agent_catalog

field password: SecretStr | None = None
Password associated with the Couchbase instance hosting the cache.

This field must be specified.

Validated by:
_pull_cluster_from_agent_catalog

field scope: str | None = 'agent_activity'
The name of the Couchbase scope hosting the cache.

This field is optional and defaults to agent_activity.

Validated by:
_pull_cluster_from_agent_catalog

field score_threshold: float | None = 0.8
The score threshold used to quantify what constitutes as a "good" match.

This field will only be used if the cache is of type semantic. If the cache is of type semantic and this field is not specified, this field defaults to 0.8.

Validated by:
_pull_cluster_from_agent_catalog

field ttl: timedelta | None = None
The time-to-live (TTL) for the cache.

When specified, the cached documents will be automatically removed after the specified duration. This field is optional and defaults to None.

Validated by:
_pull_cluster_from_agent_catalog

field username: str | None = None
Username associated with the Couchbase instance hosting the cache.

This field must be specified.

Validated by:
_pull_cluster_from_agent_catalog

LangGraph
class agentc_langgraph.tool.ToolNode(
span,
*args,
**kwargs
)[source]
A tool node that logs tool results to a span.

Class Description
This class will record the results of each tool invocation to the span that is passed to it (ultimately generating ToolResultContent log entries). This class does not log tool calls (i.e., ToolCallContent log entries) as these are typically logged with ChatCompletionContent log entries.

Below, we illustrate a minimal working example of how to use this class with agentc_langchain.chat.Callback to record ChatCompletionContent log entries, ToolCallContent log entries, and ToolResultContent log entries.

import langchain_openai
import langchain_core.tools
import langgraph.prebuilt
import agentc_langchain.chat
import agentc_langgraph
import agentc

# Create a span to bind to the chat model messages.
catalog = agentc.Catalog()
root_span = catalog.Span(name="root_span")

# Create a chat model.
chat_model = langchain_openai.chat_models.ChatOpenAI(model="gpt-4o", callbacks=[])

# Create a callback with the appropriate span, and attach it to the chat model.
my_agent_span = root_span.new(name="my_agent")
callback = agentc_langchain.chat.Callback(span=my_agent_span)
chat_model.callbacks.append(callback)

# Grab the correct tools and output from the catalog.
my_agent_prompt = catalog.find("prompt", name="my_agent")
my_agent_tools = agentc_langgraph.tool.ToolNode(
    span=my_agent_span,
    tools=[
        langchain_core.tools.StructuredTool.from_function(tool.func) for tool in my_agent_prompt.tools
    ]
)
my_agent_output = my_agent_prompt.output

# Finally, build your agent.
my_agent = langgraph.prebuilt.create_react_agent(
    model=chat_model,
    tools=my_agent_tools,
    prompt=my_agent_prompt,
    response_format=my_agent_output
)
Note

For all constructor parameters, see the documentation for langgraph.prebuilt.ToolNode here.

Parameters:
span (Span)

class agentc_langgraph.agent.ReActAgent(
chat_model,
catalog,
span,
prompt_name=None
)[source]
A helper ReAct agent base class that integrates with Agent Catalog.

Class Description
This class is meant to handle some of the boilerplate around using Agent Catalog with LangGraph's prebuilt ReAct agent. More specifically, this class performs the following:

Fetches the prompt given the name (prompt_name) in the constructor and supplies the prompt and tools attached to the prompt to the ReAct agent constructor.

Attaches a agentc_langchain.chat.Callback to the given chat_model to record all chat-model related activity (i.e., chat completions and tool calls).

Wraps tools (if present in the prompt) in a agentc_langgraph.tool.ToolNode instance to record the results of tool calls.

Wraps the invocation of this agent in a agentc.Span context manager.

Below, we illustrate an example Agent Catalog prompt and an implementation of this class for our prompt. First, our prompt:

record_kind: prompt
name: endpoint_finding_node
description: All inputs required to assemble the endpoint finding agent.

output:
  title: Endpoints
  description: The source and destination airports for a flight / route.
  type: object
  properties:
    source:
      type: string
      description: "The IATA code for the source airport."
    dest:
      type: string
      description: "The IATA code for the destination airport."
  required: [source, dest]

content:
  agent_instructions: >
    Your task is to find the source and destination airports for a flight.
    The user will provide you with the source and destination cities.
    You need to find the IATA codes for the source and destination airports.
    Another agent will use these IATA codes to find a route between the two airports.
    If a route cannot be found, suggest alternate airports (preferring airports that are more likely to have
    routes between them).

  output_format_instructions: >
    Ensure that each IATA code is a string and is capitalized.
Next, the usage of this prompt in an implementation of this class:

import langchain_core.messages
import agentc_langgraph.agent
import agentc
import typing

class State(agentc_langgraph.state):
    endpoints: typing.Optional[dict]

class EndpointFindingAgent(agentc_langgraph.agent.ReActAgent):
    def __init__(self, catalog: agentc.Catalog, span: agentc.Span, **kwargs):
        chat_model = langchain_openai.chat_models.ChatOpenAI(model="gpt-4o", temperature=0)
        super().__init__(
            chat_model=chat_model,
            catalog=catalog,
            span=span,
            prompt_name="endpoint_finding_node",
             **kwargs
        )

    def _invoke(self, span: agentc.Span, state: State, config) -> State:
        # Give the working state to our agent.
        agent = self.create_react_agent(span)
        response = agent.invoke(input=state, config=config)

        # 'source' and 'dest' comes from the prompt's output format.
        # Note this is a direct mutation on the "state" given to the Span!
        structured_response = response["structured_response"]
        state["endpoints"] = {"source": structured_response["source"], "destination": structured_response["dest"]}
        state["messages"].append(response["messages"][-1])
        return state

if __name__ == '__main__':
    catalog = agentc.Catalog()
    span = catalog.Span(name="root_span")
    my_agent = EndpointFindingAgent(catalog=catalog, span=span)
Note

For all constructor parameters, see the documentation for langgraph.prebuilt.create_react_agent here.

Parameters:
chat_model (BaseChatModel)

catalog (Catalog)

span (Span)

prompt_name (str)

class agentc_langgraph.agent.State[source]
An (optional) state class for use with Agent Catalog's LangGraph helper classes.

Class Description
The primary use for this class to help agentc_langgraph.agent.ReActAgent instances build agentc.span.EdgeContent logs. This class is essentially identical to the default state schema for LangGraph (i.e., messages and is_last_step) but with the inclusion of a new previous_node field.

class agentc_langgraph.graph.GraphRunnable(
*,
catalog,
span=None
)[source]
A helper class that wraps the "Runnable" interface with agentc.Span.

Class Description
This class is meant to handle some of the boilerplate around using agentc.Span instances and LangGraph compiled graphs. Specifically, this class builds a new span on instantiation and wraps all Runnable methods in a Span's context manager.

Below, we illustrate an example implementation of this class for a two-agent system.

import langgraph.prebuilt
import langgraph.graph
import langchain_openai
import langchain_core.messages
import agentc_langgraph
import agentc
import typing

class MyResearcherApp(agentc_langgraph.graph.GraphRunnable):
    def search_web(self, str: search_string) -> str:
        ...

    def summarize_results(self, str: content) -> str:
        ...

    def compile(self):
        research_agent = langgraph.prebuilt.create_react_agent(
            model=langchain_openai.ChatOpenAI(model="gpt-4o"),
            tools=[self.search_web]
        )
        summary_agent = langgraph.prebuilt.create_react_agent(
            model=langchain_openai.ChatOpenAI(model="gpt-4o"),
            tools=[self.summarize_results]
        )
        workflow = langgraph.graph.StateGraph(agentc_langgraph.graph.State)
        workflow.add_node("research_agent", research_agent)
        workflow.add_node("summary_agent", summary_agent)
        workflow.add_edge("research_agent", "summary_agent")
        workflow.add_edge("summary_agent", langgraph.graph.END)
        workflow.set_entry_point("research_agent")
        return workflow.compile()

if __name__ == '__main__':
    catalog = agentc.Catalog()
    state = MyResearchState(messages=[], is_last_step=False)
    MyResearcherApp(catalog=catalog).invoke(input=state)
Note

For more information around LangGraph's (LangChain's) Runnable interface, see LangChain's documentation here.

Tip

The example above does not use tools and prompts managed by Agent Catalog. See agentc_langgraph.agent.ReActAgent for a helper class that handles some of the boilerplate around using LangGraph's prebuilt ReAct agent and Agent Catalog.

Parameters:
catalog (Catalog)

span (Span)

class agentc_langgraph.state.CheckpointSaver(
options=None,
*,
serde=None,
**kwargs
)[source]
Checkpoint saver class to persist LangGraph states in a Couchbase instance.

Class Description
Instances of this class are used by LangGraph (passed in during compile() time) to save checkpoints of agent state.

Below, we give a minimal working example of how to use this class with LangGraph's prebuilt ReAct agent.

import langchain_openai
import langgraph.prebuilt
import agentc_langgraph.state

# Pass our checkpoint saver to the create_react_agent method.
chat_model = langchain_openai.ChatOpenAI(name="gpt-4o")
agent = langgraph.prebuilt.create_react_agent(
    model=chat_model,
    tools=list(),
    checkpointer=CheckpointSaver(create_if_not_exists=True)
)
config = {"configurable": {"thread_id": "1"}}
agent.invoke({"messages": [("human", "Hello!)]}, config)
To use this method with Agent Catalog's agentc_langgraph.graph.GraphRunnable class, pass the checkpoint saver to your workflow's compile() method (see the documentation for LangGraph's Graph.compile() method here for more information.

import langgraph.prebuilt
import langgraph.graph
import langchain_openai
import langchain_core.messages
import agentc_langgraph
import agentc
import typing

class MyResearcherApp(agentc_langgraph.graph.GraphRunnable):
    def search_web(self, str: search_string) -> str:
        ...

    def summarize_results(self, str: content) -> str:
        ...

    def compile(self):
        research_agent = langgraph.prebuilt.create_react_agent(
            model=langchain_openai.ChatOpenAI(model="gpt-4o"),
            tools=[self.search_web]
        )
        summary_agent = langgraph.prebuilt.create_react_agent(
            model=langchain_openai.ChatOpenAI(model="gpt-4o"),
            tools=[self.summarize_results]
        )
        workflow = langgraph.graph.StateGraph(agentc_langgraph.graph.State)
        workflow.add_node("research_agent", research_agent)
        workflow.add_node("summary_agent", summary_agent)
        workflow.add_edge("research_agent", "summary_agent")
        workflow.add_edge("summary_agent", langgraph.graph.END)
        workflow.set_entry_point("research_agent")
        checkpointer = agentc_langgraph.state.CheckpointSaver(create_if_not_exists=True)
        return workflow.compile(checkpointer=checkpointer)
Tip

See here for more information about checkpoints in LangGraph.

See also

This class is a wrapper around the langgraph_checkpointer_couchbase.CouchbaseSaver class. See here for more information.

Parameters:
options (CheckpointOptions)

serde (SerializerProtocol)

agentc_langgraph.state.initialize(
options=None,
**kwargs
)[source]
A function to create the collections required to use the checkpoint savers in this module.

Function Description
This function is a helper function for creating the default collections (the thread and tuple collections) required for the CheckpointSaver and :py:class`AsyncCheckpointSaver` classes. Below, we give a minimal working example of how to use this function to create these collections.

import langchain_openai
import langgraph.prebuilt
import agentc_langgraph.state

# Initialize our collections.
agentc_langgraph.state.initialize()

# Pass our checkpoint saver to the create_react_agent method.
chat_model = langchain_openai.ChatOpenAI(name="gpt-4o")
agent = langgraph.prebuilt.create_react_agent(
    model=chat_model,
    tools=list(),
    checkpointer=CheckpointSaver()
)
config = {"configurable": {"thread_id": "1"}}
agent.invoke({"messages": [("human", "Hello there!")]}, config)
Parameters:
options (CheckpointOptions) -- The options to use when saving checkpoints to Couchbase.

kwargs -- Keyword arguments to be forwarded to a CheckpointOptions constructor (ignored if options is present).

Return type:
None

pydantic settings agentc_langgraph.state.CheckpointOptions[source]
Config:
env_prefix: str = AGENT_CATALOG_LANGGRAPH_CHECKPOINT_

env_file: str = .env

Fields:
bucket (str | None)

checkpoint_collection (str | None)

conn_root_certificate (str | pathlib.Path | None)

conn_string (str | None)

create_if_not_exists (bool | None)

ddl_retry_attempts (int | None)

ddl_retry_wait_seconds (float | None)

password (pydantic.types.SecretStr | None)

scope (str | None)

tuple_collection (str | None)

username (str | None)

field bucket: str | None = None
The name of the Couchbase bucket hosting the checkpoints.

This field must be specified.

Validated by:
_pull_cluster_from_agent_catalog

field checkpoint_collection: str | None = 'langgraph_checkpoint_thread'
The name of the Couchbase collection hosting the checkpoints threads.

This field is optional and defaults to langgraph_checkpoint_thread.

Validated by:
_pull_cluster_from_agent_catalog

field conn_root_certificate: str | Path | None = None
Path to the root certificate file for the Couchbase cluster.

This field is optional and only required if the Couchbase cluster is using a self-signed certificate.

Validated by:
_pull_cluster_from_agent_catalog

field conn_string: str | None = None
The connection string to the Couchbase cluster hosting the cache.

This field must be specified.

Validated by:
_pull_cluster_from_agent_catalog

field create_if_not_exists: bool | None = False
Create the required collections if they do not exist.

When raised (i.e., this value is set to True), the collections will be created if they do not exist. Lower this flag (set this to False) to instead raise an error if the collections do not exist.

Validated by:
_pull_cluster_from_agent_catalog

field ddl_retry_attempts: int | None = 3
Maximum number of attempts to retry DDL operations.

This value is only used on setup (i.e., the first time the checkpointer is requested). If the number of attempts is exceeded, the command will fail. By default, this value is 3 attempts.

Validated by:
_pull_cluster_from_agent_catalog

field ddl_retry_wait_seconds: float | None = 5
Wait time (in seconds) between DDL operation retries.

This value is only used on setup (i.e., the first time the checkpointer is requested). By default, this value is 5 seconds.

Validated by:
_pull_cluster_from_agent_catalog

field password: SecretStr | None = None
Password associated with the Couchbase instance hosting the cache.

This field must be specified.

Validated by:
_pull_cluster_from_agent_catalog

field scope: str | None = 'agent_activity'
The name of the Couchbase scope hosting the checkpoints.

This field is optional and defaults to agent_activity.

Validated by:
_pull_cluster_from_agent_catalog

field tuple_collection: str | None = 'langgraph_checkpoint_tuple'
The name of the Couchbase collection hosting the checkpoints tuples.

This field is optional and defaults to langgraph_checkpoint_tuple.

Validated by:
_pull_cluster_from_agent_catalog

field username: str | None = None
Username associated with the Couchbase instance hosting the cache.

This field must be specified.

Validated by:
_pull_cluster_from_agent_catalog

LlamaIndex
class agentc_llamaindex.chat.Callback(
span,
event_starts_to_ignore=None,
event_ends_to_ignore=None
)[source]
All callback that will log all LlamaIndex events using the given span as the root.

Class Description
This class is a callback handler that will log ChatCompletionContent, ToolCallContent, and ToolResultContent using events yielded from LlamaIndex (with the given span as the root). Below, we provide an example of how to use this class.

import agentc
import llama_index.core.llms
import llama_index.llms.openai

catalog = agentc.Catalog()
root_span = catalog.Span(name="root_span")
my_prompt = catalog.find("prompt", name="talk_like_a_pirate")
chat_model = llama_index.llms.openai.OpenAI(model="gpt-4o")
chat_model.callback_manager.add_handler(Callback(span=span))
result = chat_model.chat(
    [
        llama_index.core.llms.ChatMessage(role="system", content=my_prompt.content),
        llama_index.core.llms.ChatMessage(role="user", content="What is your name"),
    ]
)
Parameters:
span (Span)

event_starts_to_ignore (list[CBEventType])

event_ends_to_ignore (list[CBEventType])



Agent Catalog Record Entries
As of date, Agent Catalog supports five different types of records (four types of tools and the generic prompt).

Tool Catalog Records
Tools are explicit actions that an agent can take to accomplish a task. Agent Catalog currently supports four types of tools: Python function tools, SQL++ query tools, semantic search tools, and HTTP request tools.

Python Function Tools
The most generic tool is the Python function tool, which is associated with a function in .py file. To signal to Agent Catalog that you want to mark a function as a tool, you must use the @tool decorator.

#
# The following file is a template for a Python tool.
#
from agentc.catalog import tool
from pydantic import BaseModel


# Although Python uses duck-typing, the specification of models greatly improves the response quality of LLMs.
# It is highly recommended that all tools specify the models of their bound functions using Pydantic or dataclasses.
class SalesModel(BaseModel):
    input_sources: list[str]
    sales_formula: str


# Only functions decorated with "tool" will be indexed.
# All other functions / module members will be ignored by the indexer.
@tool
def compute_sales_for_this_week(sales_model: SalesModel) -> float:
    """A description for the function bound to the tool. This is mandatory for tools."""

    return 1.0 * 0.99 + 2.00 % 6.0


# You can also specify the name and description of the tool explicitly, as well as any annotations you wish to attach.
@tool(name="compute_sales_for_the_month", annotations={"type": "sales"})
def compute_sales_for_the_month(sales_model: SalesModel) -> float:
    """A description for the function bound to the tool. This is mandatory for tools."""

    return 1.0 * 0.99 + 2.00 % 6.0
SQL++ Query Tools
SQL++ is the query language used by Couchbase to interact with the data stored in the cluster. To create a SQL++ query tool, you must author a .sqlpp file with a header that details various metadata. If you are importing an existing SQL++ query, simply prepend the header to the query.

--
-- The following file is a template for a (Couchbase) SQL++ query tool.
--

-- All SQL++ query tools are specified using a valid SQL++ (.sqlpp) file.
-- The tool metadata must be specified with YAML inside a multi-line C-style comment.
/*
# The name of the tool must be a valid Python identifier (e.g., no spaces).
# This field is mandatory, and will be used as the name of a Python function.
name: find_high_order_item_customers_between_date

# A description for the function bound to this tool.
# This field is mandatory, and will be used in the docstring of a Python function.
description: >
    Given a date range, find the customers that have placed orders where the total number of items is more than 1000.

# The inputs used to resolve the named parameters in the SQL++ query below.
# Inputs are described using a JSON object that follows the JSON schema standard.
# This field is mandatory, and will be used to build a Pydantic model.
# See https://json-schema.org/learn/getting-started-step-by-step for more info.
input: >
    {
      "type": "object",
      "properties": {
        "orderdate_start": { "type": "string" },
        "orderdate_end": { "type": "string" }
      }
    }

# The outputs used describe the structure of the SQL++ query result.
# Outputs are described using a JSON object that follows the JSON schema standard.
# This field is optional, and will be used to build a Pydantic model.
# We recommend using the 'INFER' command to build a JSON schema from your query results.
# See https://docs.couchbase.com/server/current/n1ql/n1ql-language-reference/infer.html.
# In the future, this field will be optional (we will INFER the query automatically for you).
# output: >
#     {
#       "type": "array",
#       "items": {
#         "type": "object",
#         "properties": {
#           "cust_id": { "type": "string" },
#           "first_name": { "type": "string" },
#           "last_name": { "type": "string" },
#           "item_cnt": { "type": "integer" }
#         }
#       }
#     }

# As a supplement to the tool similarity search, users can optionally specify search annotations.
# The values of these annotations MUST be strings (e.g., not 'true', but '"true"').
# This field is optional, and does not have to be present.
annotations:
  gdpr_2016_compliant: "false"
  ccpa_2019_compliant: "true"

# The "secrets" field defines search keys that will be used to query a "secrets" manager.
# Note that these values are NOT the secrets themselves, rather they are used to lookup secrets.
secrets:

    # All Couchbase tools (e.g., semantic search, SQL++) must specify conn_string, username, and password.
    - couchbase:
        conn_string: CB_CONN_STRING
        username: CB_USERNAME
        password: CB_PASSWORD
*/

SELECT
  c.cust_id,
  c.name.first AS first_name,
  c.name.last  AS last_name,
  COUNT(*)     AS item_cnt
FROM
  customers AS c,
  orders    AS o,
  o.items   AS i
WHERE
  -- Parameters specified in the input field of the tool metadata above correspond to named parameters here.
  -- The '$' syntax is used to denote a named parameter.
  -- See https://docs.couchbase.com/server/current/n1ql/n1ql-rest-api/exnamed.html for more details.
  ( o.orderdate BETWEEN $orderdate_start AND $orderdate_end ) AND
  c.cust_id = o.cust_id
GROUP BY
  c.cust_id
HAVING
  COUNT(*) > 1000;
Semantic Search Tools
Semantic search tools are used to search for text that is semantically similar to some query text. To create a semantic search tool, you must author a .yaml file with the record_kind field populated with semantic_search.

#
# The following file is a template for a (Couchbase) semantic search tool.
#
record_kind: semantic_search

# The name of the tool must be a valid Python identifier (e.g., no spaces).
# This field is mandatory, and will be used as the name of a Python function.
name: search_for_relevant_products

# A description for the function bound to this tool.
# This field is mandatory, and will be used in the docstring of a Python function.
description: >
  Find product descriptions that are closely related to a collection of tags.

# The prompts used to build a comparable representation for a semantic search.
# Inputs are described using a JSON object that follows the JSON schema standard.
# This field is mandatory, and will be used to build a Pydantic model.
# See https://json-schema.org/learn/getting-started-step-by-step for more info.
input: >
  {
    "type": "object",
    "properties": {
      "search_tags": {
        "type": "array",
        "items": { "type": "string" }
      }
    }
  }

# As a supplement to the tool similarity search, users can optionally specify search annotations.
# The values of these annotations MUST be strings (e.g., not 'true', but '"true"').
# This field is optional, and does not have to be present.
annotations:
  gdpr_2016_compliant: "false"
  ccpa_2019_compliant: "true"

# The "secrets" field defines search keys that will be used to query a "secrets" manager.
# Note that these values are NOT the secrets themselves, rather they are used to lookup secrets.
secrets:

  # All Couchbase tools (e.g., semantic search, SQL++) must specify conn_string, username, and password.
  - couchbase:
      conn_string: CB_CONN_STRING
      username: CB_USERNAME
      password: CB_PASSWORD

# Couchbase semantic search tools always involve a vector search.
vector_search:

  # A bucket, scope, and collection must be specified.
  # Semantic search across multiple collections is currently not supported.
  bucket: my-bucket
  scope: my-scope
  collection: my-collection

  # All semantic search operations require that a (FTS) vector index is built.
  # In the future, we will relax this constraint.
  index: my-vector-index

  # The vector_field refers to the field the vector index (above) was built on.
  # In the future, we will relax the constraint that an index exists on this field.
  vector_field: vec

  # The text_field is the field name used in the tool output (i.e., the results).
  # In the future, we will support multi-field tool outputs for semantic search.
  text_field: text

  # The embedding model used to generate the vector_field.
  # If a URL is specified, we will assume the URL serves as the base of an OpenAI-client-compatible endpoint.
  # If a URL is not specified (the default), we will assume the embedding model is a sentence-transformers model
  # that can be downloaded from HuggingFace.
  embedding_model:
    name: sentence-transformers/all-MiniLM-L12-v2
    # url:

  # The number of candidates (i.e., the K value) to request for when performing a vector top-k search.
  # This field is optional, and defaults to k=3 if not specified.
  num_candidates: 3
HTTP Request Tools
HTTP request tools are used to interact with external services via REST API calls. The details on how to interface with these external services are detailed in a standard OpenAPI spec (see here for more details). To create an HTTP request tool, you must author a .yaml file with the record_kind field populated with http_request. One tool is generated per specified endpoint.

#
# The following file is a template for a set of HTTP request tools.
#
record_kind: http_request

# As a supplement to the tool similarity search, users can optionally specify search annotations.
# The values of these annotations MUST be strings (e.g., not 'true', but '"true"').
# This field is optional, and does not have to be present.
annotations:
  gdpr_2016_compliant: "false"
  ccpa_2019_compliant: "true"

# HTTP requests must be specified using an OpenAPI spec.
open_api:

  # The path relative to the tool-calling code.
  # The OpenAPI spec can either be in JSON or YAML.
  filename: path_to_openapi_spec.json

  # A URL denoting where to retrieve the OpenAPI spec.
  # The filename or the url must be specified (not both).
  # url: http://url_to_openapi_spec/openapi.json

  # Which OpenAPI operations should be indexed as tools are specified below.
  # This field is mandatory, and each operation is validated against the spec on index.
  operations:

    # All operations must specify a path and a method.
    # 1. The path corresponds to an OpenAPI path object.
    # 2. The method corresponds to GET/POST/PUT/PATCH/DELETE/HEAD/OPTIONS/TRACE.
    # See https://swagger.io/specification/#path-item-object for more information.
    - path: /users/create
      method: post
    - path: /users/delete/{user_id}
      method: delete
To know more on generating your OpenAPI spec, check out the schema here. For an example OpenAPI spec used in the travel-sample agent, see here.

Prompt Records
Prompts in Agent Catalog refer to the aggregation of all all inputs (tool choices, unstructured prompts, output types, etc...) given to an LLM (or an agent framework).

#
# The following file is a template for a prompt.
#
record_kind: prompt

# The name of the prompt must be a valid Python identifier (e.g., no spaces).
# This field is mandatory, and will be used when searching for prompts by name.
name: route_finding_agent

# A description of the prompt's purpose (e.g., where this prompt will be used).
# This field is mandatory, and will be used (indirectly) when performing semantic search for prompts.
description: >
  Instructions on how to find routes between two specific airports.

# As a supplement to the description similarity search, users can optionally specify search annotations.
# The values of these annotations MUST be strings (e.g., not 'true', but '"true"').
# This field is optional, and does not have to be present.
annotations:
  organization: "sequoia"

# The input to an LLM will _generally_ (more often than not) be accompanied by a small collection of tools.
# This field is used at provider time to search the catalog for tools.
# This field is optional, and does not have to be present.
tools:
  # Tools can be specified using the same parameters found in Catalog.find("tool", ...).
  # For instance, we can condition on the tool name...
  - name: "find_indirect_routes"

  # ...the tool name and some annotations...
  - name: "find_direct_routes"
    annotations: gdpr_2016_compliant = "true"

  # ...or even a semantic search via the tool description.
  - query: "finding flights by name"
    limit: 2

# The output type (expressed in JSON-schema) associated with this prompt.
# See https://json-schema.org/understanding-json-schema for more information.
# This field is commonly supplied to an LLM to generate structured responses.
# This field is optional, and does not have to be present.
output:
  type: object
  properties:
    source:
      type: string
      description: "The IATA code for the source airport."
    dest:
      type: string
      description: "The IATA code for the destination airport."

# The textual input to the model.
# This can either be a single string or an arbitrarily nested dictionary.
# Below, we provide an example of a nested dictionary.
content:
  Goal:
    Your goal is to find a sequence of routes between the source and destination airport.

  Examples:
    ...

  Instructions: >
    Try to find a direct routes first between the source airport and the destination airport.
    If there are no direct routes, then find a one-layover route.
    If there are no such routes, then try another source airport that is close.
Tip

The content field of Agent Catalog prompt entries can be either be completely unstructured (e.g., persisted as a single string) or as a YAML object (of arbitrary nesting) structuring specific parts of your prompt. For example, suppose we are given the prompt record below:

name: my_prompt

description: A prompt for validating the output of another agent.

content:
    agent_instructions: |
        Your task is to validate the line of thinking using
        the previous messages.
    format_instructions: |
        You MUST return your answer in all caps.
Upon fetching this prompt from the catalog, we can access the content field as a dictionary. This is useful for agent frameworks that require specific small snippets of text (e.g., "instructions", "objective", etc...)

import agentc
import your_favorite_agent_framework

catalog = agentc.Catalog()
my_prompt = catalog.find("prompt", name="my_prompt")
my_agent = your_favorite_agent_framework.Agent(
    instructions=my_prompt.content["agent_instructions"],
    output={
        "type": [True, False],
        "instructions": my_prompt.content["format_instructions"]
    }
)

Log Records
As of date, Agent Catalog supports 11 different types of log records (all varied by content). Agent Catalog maintains a collection of logs as a series of events.

Schema of Logs
pydantic model agentc_core.activity.models.log.Log[source]
A Log instance represents a single log record that is bound to a part of the application versioned according to catalog_version.

Attention

Log instances are immutable and should not be instantiated directly. Only Content instances should be created directly, and then passed to a Span instance via the agentc.span.Span.log() method.

Show JSON schema
field annotations: Dict | None = None
Additional annotations that can be added to the message.

field catalog_version: VersionDescriptor [Required]
A unique identifier that defines a catalog version / snapshot / commit.

field content: SystemContent | ToolCallContent | ToolResultContent | ChatCompletionContent | RequestHeaderContent | UserContent | AssistantContent | BeginContent | EndContent | EdgeContent | KeyValueContent [Required]
The content of the record. This should be as close to the producer as possible.

field identifier: str [Optional]
A unique identifier for this record. This field is typically a UUID.

field span: Log.Span [Required]
The span (i.e., a list of names and a session ID) that this record is associated with.

field timestamp: AwareDatetime [Required]
Timestamp of the generated record. This field must have a timezone attached as well.

pydantic model Span[source]
Show JSON schema
field name: list[str] [Required]
The name of the span. This is a list of names that represent the span hierarchy.

field session: str [Optional]
The 'session' (a runtime identifier) that this span is associated with.

Content in Logs
enum agentc_core.activity.models.content.Kind(
value
)[source]
The different types of log content that are recognized.

Member Type:
str

Valid values are as follows:

System = <Kind.System: 'system'>
System refers to messages that are generated by (none other than) the system or application. In agent frameworks, these messages are typically templated and instantiated with application-defined objectives / instructions.

ToolCall = <Kind.ToolCall: 'tool-call'>
ToolCall refers to messages that contain (typically LLM generated) arguments for invoking a tool. These logs are not to be confused with ToolResult messages which contain the results of invoking a tool.

ToolResult = <Kind.ToolResult: 'tool-result'>
ToolResult refers to messages containing the results of invoking a tool. These logs are not to be confused with ToolCall messages which are (typically) generated by an LLM.

ChatCompletion = <Kind.ChatCompletion: 'chat-completion'>
ChatCompletion refers to messages that are generated using a language model. Ideally, these messages should be captured immediately after generation (without any post-processing).

RequestHeader = <Kind.RequestHeader: 'request-header'>
RequestHeader refers to messages that specifically capture tools and output types used in a request to a language model.

User = <Kind.User: 'user'>
User refers to messages that are directly sent by (none other than) the user. If the application uses prompt templates, these messages refer to the raw user input (not the templated text).

Assistant = <Kind.Assistant: 'assistant'>
Assistant refers to messages that are directly served back to the user. These messages exclude any ModelOutput or System messages that are used internally by the application.

Begin = <Kind.Begin: 'begin'>
Begin refers to marker messages that are used to indicate the start of a span (e.g., a task, agent, state, etc...). These messages are typically used to trace how one unit of work mutates some state, and are application specific.

End = <Kind.End: 'end'>
End refers to marker messages that are used to indicate the end of a span (e.g., a task, agent, state, etc...). These messages are typically used to trace how one unit of work mutates some state, and are application specific.

Edge = <Kind.Edge: 'edge'>
Edge refers to marker messages that are used to indicate one unit of work (essentially) invoking another unit of work. These messages can be used to trace 'handoffs' and 'agent-to-agent' communication.

KeyValue = <Kind.KeyValue: 'key-value'>
KeyValue refers to messages that contain user-specified data that are to be logged under some span.

pydantic model agentc_core.activity.models.content.SystemContent[source]
System refers to messages that are generated by (none other than) the system or application.

Class Description
In agent frameworks, these messages are typically templated and instantiated with application-defined objectives / instructions. In our integration packages (e.g., agentc_langchain.chat.Callback), system messages are commonly used to record the contents used to generate a chat-completion or tool-call message.

A system message has a single required field, value, which contains the content of the system message. If extra data is associated with the system message (e.g., LangGraph's run_id fields), it can be stored in the optional extra field.

Show JSON schema
Config:
frozen: bool = True

use_enum_values: bool = True

Fields:
extra (dict | None)

kind (Literal[agentc_core.activity.models.content.Kind.System])

value (str)

field extra: Optional[dict] = None
Additional data that is associated with the content. This field is optional.

field kind: Literal[Kind.System] = Kind.System
field value: str [Required]
The content of the system message.

pydantic model agentc_core.activity.models.content.ToolCallContent[source]
ToolCall refers to messages that contain (typically LLM generated) arguments for invoking a tool.

Class Description
LLMs enable the invocation of standard Python functions (i.e., "tools") by a) "selecting" tools from some larger tool-set and b) generating arguments to these tools. This type of content is not to be confused with ToolResult messages which contain the results of invoking a tool (i.e., the output).

A tool call message has two required fields: tool_name and tool_args. The tool_name field refers to the name of the tool that is being called. The tool_args field contains the arguments that are going to be passed to the tool (represented as a dictionary keyed by parameter names whose entries are the parameter values).

Optional fields for a tool call message include tool_call_id, status, meta, and extra. The tool_call_id field is an optional unique identifier associated with a tool call instance and is used to correlate the call to the result of its execution (i.e., the ToolResult message) by the application. The status field is an optional field that indicates the status of generating the tool call message (e.g., the generated output does not adhere to the function signature). To capture the breadth of LLM-provider metadata, tool call messages may also contain a meta field (used to capture the raw response associated with the tool call message). If extra data is associated with the tool call (e.g., the log-probabilities), it can be stored in the optional extra field.

Tip

If tool_call_id is not specified but your application calls and executes tools sequentially, you can still link the tool call to the corresponding tool result by using the order of the messages in the log. This is the approach taken by the ToolInvocations view (more information can be found here).

Show JSON schema
Config:
frozen: bool = True

use_enum_values: bool = True

Fields:
extra (dict | None)

kind (Literal[agentc_core.activity.models.content.Kind.ToolCall])

meta (dict | None)

status (Literal['success', 'error'] | None)

tool_args (dict[str, Any])

tool_call_id (str)

tool_name (str)

field extra: Optional[dict] = None
Additional data that is associated with the content. This field is optional.

field kind: Literal[Kind.ToolCall] = Kind.ToolCall
field meta: dict | None = None
The raw response associated with the tool call. This must be JSON-serializable.

field status: Literal['success', 'error'] | None = 'success'
field tool_args: dict[str, Any] [Required]
The arguments that are going to be passed to the tool. This field should be JSON-serializable.

field tool_call_id: str [Required]
The unique identifier associated with a tool call instance. This field is (typically) parsed from a LLM response and is used to correlate / JOIN this message with the corresponding ToolResult message.

field tool_name: str [Required]
The name of the tool that is being called. If this tool is indexed with Agent Catalog, this field should refer to the tool's 'name' field.

pydantic model agentc_core.activity.models.content.ToolResultContent[source]
ToolResult refers to messages containing the results of invoking a tool.

Class Description
Tool result messages are used to capture the output of invoking a tool (i.e., the result of the tool call). This type of content is not to be confused with ToolCall messages which contain the arguments for invoking a tool.

A tool result message has a single required field, tool_result, which contains a JSON-serializable object representing the result of executing the tool.

Optional fields for a tool result message include tool_call_id, status, and extra. The tool_call_id field is an optional unique identifier associated with a tool call instance and is used here to correlate the execution of a tool to its call generation. status is an optional field that indicates the status of the tool invocation (i.e., "success", or "error" if the tool itself raised an exception). Finally, if extra data is associated with the tool result (e.g., the error message on unsuccessful tool invocations), it can be stored in the optional extra field.

Show JSON schema
Config:
frozen: bool = True

use_enum_values: bool = True

Fields:
extra (dict | None)

kind (Literal[agentc_core.activity.models.content.Kind.ToolResult])

status (Literal['success', 'error'] | None)

tool_call_id (str | None)

tool_result (Any)

field extra: Optional[dict] = None
Additional data that is associated with the content. This field is optional.

field kind: Literal[Kind.ToolResult] = Kind.ToolResult
field status: Literal['success', 'error'] | None = 'success'
The status of the tool invocation. This field should be one of 'success' or 'error'.

field tool_call_id: str | None = None
The unique identifier of the tool call. This field will be used to correlate / JOIN this message with the corresponding ToolCall message.

field tool_result: Any [Required]
The result of invoking the tool. This field should be JSON-serializable.

pydantic model agentc_core.activity.models.content.ChatCompletionContent[source]
ChatCompletion refers to messages that are generated using a language model.

Class Description
Chat completion messages refer to the output "predicted" text of a language model. In the context of "agentic" applications, these messages are distinct from ToolCallContent messages (even though both are generated using LLMs).

A chat completion message has one required field: output. The output field refers to the unstructured generated text returned by the language model.

To capture the breadth of LLM-provider metadata, chat completion messages may also contain a meta field (used to capture the raw response associated with the chat completion). Finally, any extra data that exists outside the raw response can be stored in the optional extra field.

Show JSON schema
Config:
frozen: bool = True

use_enum_values: bool = True

Fields:
extra (dict | None)

kind (Literal[agentc_core.activity.models.content.Kind.ChatCompletion])

meta (dict | None)

output (str)

field extra: Optional[dict] = None
Additional data that is associated with the content. This field is optional.

field kind: Literal[Kind.ChatCompletion] = Kind.ChatCompletion
field meta: dict | None = None
The raw response associated with the chat completion. This must be JSON-serializable.

field output: str [Required]
The output of the model.

pydantic model agentc_core.activity.models.content.RequestHeaderContent[source]
RequestHeader refers to messages that specifically capture tools and output types used in a request to a language model.

Class Description
Request header messages are used to record "setup" information for subsequent chat-completion and/or tool-call events. These primarily include tools (dictionaries of names, descriptions, and function schemas) and output types.

All fields of a request header message are optional: tools, output, meta, and extra. The tools field is a list of RequestHeaderContent.Tool instances made available to subsequent LLM calls. The output field refers to the output type that subsequent LLM calls must adhere to (most commonly expressed in JSON schema). The meta field refers to a JSON-serializable object containing the request information. Finally, extra is used to capture any other data that does not belong in meta.

Tip

Pydantic enables the specification of their objects using dictionaries. The code snippet below demonstrates two equivalent approaches to specifying the tools attribute: First, users can create RequestHeaderContent.Tool instances by referencing the subclass directly:

from agentc.span import RequestHeaderContent

request_header_content = RequestHeaderContent(
    tools=[
        RequestHeaderContent.Tool(
            name="get_user_by_id",
            description="Lookup a user by their ID field.",
            args_schema={"type": "object", "properties": {"id": {"type": "integer"}}}
        )
    ]
)
Second, users can specify a dictionary:

from agentc.span import RequestHeaderContent

request_header_content = RequestHeaderContent(
    tools=[
        {
            "name": "get_user_by_id",
            "description": "Lookup a user by their ID field.",
            "args_schema": {"type": "object", "properties": {"id": {"type": "integer"}}}
        }
    ]
)
For most cases though, we recommend the former (as this enables most IDEs to catch name errors before runtime).

Show JSON schema
Config:
frozen: bool = True

use_enum_values: bool = True

Fields:
extra (dict | None)

kind (Literal[agentc_core.activity.models.content.Kind.RequestHeader])

meta (dict | None)

output (dict | None)

tools (list[agentc_core.activity.models.content.RequestHeaderContent.Tool] | None)

field extra: Optional[dict] = None
Additional data that is associated with the content. This field is optional.

field kind: Literal[Kind.RequestHeader] = Kind.RequestHeader
field meta: dict | None = None
All request parameters associated with the model input. This must be JSON-serializable.

field output: dict | None = None
The output type of the model (in JSON schema) response. This field is optional.

field tools: list[Tool] | None = <class 'list'>
The tools (name, description, schema) included in the request to a model. For tools indexed by Agent Catalog, this field should refer to the tool's 'name' field. This field is optional.

pydantic model Tool[source]
Show JSON schema
Fields:
args_schema (dict)

description (str)

name (str)

field args_schema: dict [Required]
The (JSON) schema of the tool.

field description: str [Required]
A description of the tool.

field name: str [Required]
The name of the tool.

pydantic model agentc_core.activity.models.content.UserContent[source]
User refers to messages that are directly sent by (none other than) the user.

Class Description
User messages are used to capture a user's input into your application. User messages exclude those generated by a prior LLM call for the purpose of "mocking" an intelligent actor (e.g., multi-agent applications).

A user message has a single required field, value, which contains the direct input given by the user. If extra data is associated with the user message, it can be stored in the optional extra field.

Show JSON schema
Config:
frozen: bool = True

use_enum_values: bool = True

Fields:
extra (dict | None)

kind (Literal[agentc_core.activity.models.content.Kind.User])

user_id (str | None)

value (str)

field extra: Optional[dict] = None
Additional data that is associated with the content. This field is optional.

field kind: Literal[Kind.User] = Kind.User
field user_id: str | None = None
The unique identifier of the user. This field is optional.

field value: str [Required]
The captured user input.

pydantic model agentc_core.activity.models.content.AssistantContent[source]
Assistant refers to messages that are directly served back to the user.

Class Description
Assistant messages are used to capture the direct output of your application (i.e., the messages served back to the user). These messages are not strictly ChatCompletionContent messages, as your application may utilize multiple LLM calls before returning some message back to the user.

An assistant message has a single required field, value, which contains the direct output back to the user. If extra data is associated with the assistant message, it can be stored in the optional extra field.

Show JSON schema
Config:
frozen: bool = True

use_enum_values: bool = True

Fields:
extra (dict | None)

kind (Literal[agentc_core.activity.models.content.Kind.Assistant])

value (str)

field extra: Optional[dict] = None
Additional data that is associated with the content. This field is optional.

field kind: Literal[Kind.Assistant] = Kind.Assistant
field value: str [Required]
The response served back to the user.

pydantic model agentc_core.activity.models.content.BeginContent[source]
Begin refers to marker messages that are used to indicate the start of a span (e.g., a task, agent, state, etc...).

Card Description
Begin messages denote the start of a span and (perhaps just as important) record the entrance state of a span. In certain applications, log analysts are able to use this information to model how the state of an application mutates over time.

Begin messages have two optional fields: i) the state field (used to record the starting state of a span) and ii) extra (used to record any extra information pertaining to the start of a span).

Show JSON schema
Config:
frozen: bool = True

use_enum_values: bool = True

Fields:
extra (dict | None)

kind (Literal[agentc_core.activity.models.content.Kind.Begin])

state (Any | None)

field extra: Optional[dict] = None
Additional data that is associated with the content. This field is optional.

field kind: Literal[Kind.Begin] = Kind.Begin
field state: Any | None = None
The state logged on entering a span.

pydantic model agentc_core.activity.models.content.EndContent[source]
End refers to marker messages that are used to indicate the end of a span (e.g., a task, agent, state, etc...).

Class Description
End messages denote the end of a span and (perhaps just as important) record the exit state of a span. In certain applications, log analysts are able to use this information to model how the state of an application mutates over time.

End messages have two optional fields: i) the state field (used to record the ending state of a span) and ii) extra (used to record any extra information pertaining to the end of a span).

Show JSON schema
Config:
frozen: bool = True

use_enum_values: bool = True

Fields:
extra (dict | None)

kind (Literal[agentc_core.activity.models.content.Kind.End])

state (Any | None)

field extra: Optional[dict] = None
Additional data that is associated with the content. This field is optional.

field kind: Literal[Kind.End] = Kind.End
field state: Any | None = None
The state logged on exiting a span.

pydantic model agentc_core.activity.models.content.EdgeContent[source]
Edge refers to messages used to capture a caller's intent to 'handoff' some state to another span.

Card Description
Edge messages denote the explicit intent to invoke another unit of work. In the case of multi-agent applications, this type of message can be used to capture how one agent might call another agent.

Edge messages have two required fields: i) source, the fully qualified name of the source Span, and ii) dest, the fully qualified name of the destination Span. A payload field can optionally be recorded in an edge message to model spans as a functional unit of work (which they are in most cases). If extra data is associated with the edge message, it can be stored in the optional extra field.

There are two paradigms around span to span "communication": a) horizontal and b) vertical. Horizontal communication refers to (span) graphs that are managed by some orchestrator (e.g., LangGraph, CrewAI, etc...), while vertical communication refers to (span) graphs that are built by directly invoking other spans (ultimately building a call stack).

Note

In the case of our agentc_langgraph.agent.ReActAgent helper class, a previous_node field is used to help build these edges for horizontal communication.

Show JSON schema
Config:
frozen: bool = True

use_enum_values: bool = True

Fields:
dest (list[str])

extra (dict | None)

kind (Literal[agentc_core.activity.models.content.Kind.Edge])

payload (Any | None)

source (list[str])

field dest: list[str] [Required]
Name of the destination span associated with this edge.

field extra: Optional[dict] = None
Additional data that is associated with the content. This field is optional.

field kind: Literal[Kind.Edge] = Kind.Edge
field payload: Any | None = None
A (JSON-serializable) item being sent from the source span to the destination span.

field source: list[str] [Required]
Name of the source span associated with this edge.

pydantic model agentc_core.activity.models.content.KeyValueContent[source]
KeyValue refers to messages that contain user-specified data that are to be logged under some span.

Class Description
Key-value messages serve as the catch-all container for user-defined data that belong to some span. We distinguish key-value messages from log-level annotations, which are used to attach information to a log entry with existing content.

Key-value messages have two required fields: i) the key field (used to record the name of the entry) and ii) value (used to record the value of the entry). Extra data can also be passed in through the optional extra field (as with other messages).

Using the [] syntax with a agentc.span.Span instance generates one key-value entry per call. For example, the following code snippet generates two logs with the same key but different values:

my_span = catalog.Span(name="my_span")
my_span["alpha"] = alpha_value_1
my_span["alpha"] = alpha_value_2
These messages are commonly purposed for recording evaluation data, as seen in our example application here.

Show JSON schema
Config:
frozen: bool = True

use_enum_values: bool = True

Fields:
extra (dict | None)

key (str)

kind (Literal[agentc_core.activity.models.content.Kind.KeyValue])

value (Any)

field extra: Optional[dict] = None
Additional data that is associated with the content. This field is optional.

field key: str [Required]
The name of the key-value pair.

field kind: Literal[Kind.KeyValue] = Kind.KeyValue
field value: Any [Required]
The value of the key-value pair. This value should be JSON-serializable.