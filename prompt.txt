# **Prompt for Intelligent Jupyter Notebook Code Integration**

## **Objective**
You are tasked with intelligently merging Python code from separate script files into a Jupyter notebook's JSON format while maintaining full functionality. Your goal is to create a self-contained notebook that replicates the exact behavior of working Python scripts without fabricating code or introducing errors.

## **Core Principles**

### **1. SOURCE TRUTH FIRST - NEVER FABRICATE**
- **Always read the actual working Python files first** - never guess or assume code structure
- Use working scripts as the **single source of truth** for all code patterns
- Cross-reference with log files to understand expected inputs/outputs
- If you can't find working code for a function, explicitly state what's missing
- **Never write code from memory** - always copy from existing working files

### **2. UNDERSTAND DATA FLOW COMPLETELY**
- Map the complete execution flow: `main.py` ’ `data/` ’ `evals/` ’ final outputs
- Identify all data structures and their types (DataFrame, list, dict, custom objects)
- Understand parameter passing: what goes in, what comes out, how it's accessed
- Note return types and how results are consumed by downstream functions
- **Document the data pipeline** before coding

### **3. PRESERVE FUNCTIONAL CALL CHAINS**
- Maintain the exact same function call sequence as working scripts
- Keep all intermediate data structures and variable names
- Preserve error handling, logging, and exception patterns
- Maintain the same import structure and dependency order
- **Test each integration point** against working outputs

## **Step-by-Step Process**

### **Phase 1: Source Analysis**
1. **Read all relevant Python source files completely**
   - `main.py` - main execution flow and setup functions
   - `data/*.py` - data loading and query functions  
   - `evals/*.py` - evaluation and processing functions
   - Any configuration or utility files

2. **Identify the execution architecture**
   - Map function call hierarchy and dependencies
   - Note all imports, external service calls, and configurations
   - Identify setup requirements (environment variables, connections)
   - Document the complete data flow from input to final output

3. **Review working outputs and logs**
   - Study log files to understand expected behavior at each step
   - Note data structure formats, parameter values, and result patterns
   - Identify any error handling or edge cases
   - Document expected outputs for validation

### **Phase 2: Integration Planning**
1. **Map integration points**
   - Identify where separate modules need to connect in notebook cells
   - Plan the cell structure to match the working script flow
   - Note any shared state or configuration requirements
   - Plan validation points to check intermediate results

2. **Identify potential issues**
   - Parameter name mismatches between modules
   - Data structure access pattern differences  
   - Import statement requirements and ordering
   - JSON escaping requirements for complex code

### **Phase 3: Intelligent Code Integration**

1. **Copy functions with exact fidelity**
   - Use identical function signatures, parameter names, and return types
   - Preserve all error handling, logging, and validation logic
   - Maintain original variable names and data structure creation
   - Keep the same documentation and comments

2. **Handle Jupyter notebook JSON format correctly**
   - Properly escape all strings for JSON embedding: `"` becomes `\"`
   - Maintain correct indentation within JSON string arrays
   - Handle triple-quoted strings and complex multi-line code
   - Preserve Python syntax while fitting JSON structure

3. **Preserve data access patterns**
   - If source uses `results["label"].tolist()`, use exactly that pattern
   - If source uses `data=dataframe`, don't change to `dataframe=dataframe`
   - Maintain the same DataFrame column access methods
   - Keep identical iteration and indexing patterns

4. **Maintain execution order and dependencies**
   - Setup functions before main execution
   - Import statements in correct order
   - Configuration and connection establishment first
   - Data loading before processing, evaluation after results

### **Phase 4: Validation and Testing**

1. **Cross-reference with working logs**
   - Compare expected vs actual data structures at each step
   - Verify parameter values and function call results
   - Check that evaluation outputs match working script format
   - Validate that error handling produces expected messages

2. **Test integration points**
   - Ensure data flows correctly between notebook cells
   - Verify that each cell produces expected intermediate results
   - Check that final outputs match working script behavior
   - Test edge cases and error conditions

## **Critical Rules - NEVER VIOLATE THESE**

### **Parameter and Function Fidelity**
- **NEVER change parameter names** - if source uses `data=df`, use `data=df`, not `dataframe=df`
- **NEVER modify function signatures** - copy exactly as written
- **NEVER change import statements** - use identical import patterns
- **NEVER alter data structure access** - if source uses `.tolist()`, use `.tolist()`

### **Code Source Requirements**  
- **NEVER fabricate code** that doesn't exist in source files
- **NEVER assume library behaviors** - always check source implementation
- **NEVER modify logical flow** - maintain exact execution sequence
- **NEVER change error handling** - preserve original exception patterns

### **JSON Notebook Integration**
- **ALWAYS escape strings properly** for JSON format
- **ALWAYS maintain indentation** within JSON string arrays
- **ALWAYS preserve Python syntax** while fitting JSON structure
- **ALWAYS test that cells execute** in the target notebook environment

## **Expected Deliverable**

A fully functional Jupyter notebook where:
- All code is faithfully copied from working source files
- Function calls work exactly as in the original scripts  
- Data structures and access patterns are identical
- Results match the working script outputs exactly
- No fabricated or modified code - only intelligent integration
- Proper JSON formatting with correct escaping and indentation

## **Validation Criteria**

Your integration is successful when:
- The notebook produces identical outputs to working scripts
- No KeyError, AttributeError, or parameter mismatch errors occur
- Data structures match expected types and formats from logs
- Function call chains execute in the same sequence
- All evaluation results match working script patterns
- Error handling produces the same messages as source files

## **When You Encounter Issues**

If you find missing code or unclear integration points:
1. **State explicitly what source code you need** - don't guess
2. **Ask for clarification** on data structure requirements
3. **Reference specific log files** to understand expected behavior
4. **Identify the exact source files** that contain the missing functionality
5. **Never proceed with fabricated solutions** - always get the working source first

This approach ensures reliable, functional integration that preserves the exact behavior of working Python scripts while adapting to Jupyter notebook format requirements.